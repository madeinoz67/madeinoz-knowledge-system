# Data Model: Gemini Prompt Caching

**Feature**: 006-gemini-prompt-caching
**Date**: 2026-01-27
**Status**: Phase 1 Complete
**Author**: Algorithm Agent (Vera Sterling)

---

## Core Entities

### CacheMetrics

**Purpose**: Represents caching performance data for a single API request, enabling users to understand cache effectiveness and cost savings in real-time.

**Lifecycle**:
- **Created**: When a Gemini API response is received with `usage_metadata`
- **Updated**: Never (immutable per-request snapshot)
- **Deleted**: When response object is garbage collected

**Storage**: Embedded in MCP response metadata; optionally persisted to JSONL metrics log.

**Fields**:
| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| cache_hit | bool | Yes | - | Whether any tokens were served from cache |
| cached_tokens | int | Yes | 0 | Count of tokens retrieved from Gemini cache |
| prompt_tokens | int | Yes | - | Total input tokens in the request |
| completion_tokens | int | Yes | - | Output tokens generated by the model |
| tokens_saved | int | Yes | 0 | Same as cached_tokens (explicit semantic alias) |
| cost_without_cache | float | Yes | - | Hypothetical cost if no caching was used (USD) |
| actual_cost | float | Yes | - | Actual API cost after cache discount (USD) |
| cost_saved | float | Yes | 0.0 | Monetary savings from caching (cost_without_cache - actual_cost) |
| savings_percent | float | Yes | 0.0 | Percentage reduction in cost (0-100) |
| model | str | Yes | - | Gemini model identifier used for pricing lookup |

**Relationships**:
- **Aggregates to**: `SessionMetrics` (session-level cumulative statistics)
- **Derived from**: Gemini API `response.usage_metadata`
- **Used by**: MCP response wrapper, metrics logger

**Validation Rules**:
- `cached_tokens` <= `prompt_tokens` (cannot cache more than input)
- `cost_saved` >= 0 (caching never increases cost)
- `savings_percent` in range [0, 100]
- `cache_hit` = True if and only if `cached_tokens` > 0

**Example Instance**:
```json
{
  "cache_hit": true,
  "cached_tokens": 1500,
  "prompt_tokens": 2000,
  "completion_tokens": 350,
  "tokens_saved": 1500,
  "cost_without_cache": 0.001475,
  "actual_cost": 0.000925,
  "cost_saved": 0.00055,
  "savings_percent": 37.29,
  "model": "gemini-2.5-flash"
}
```

---

### CacheConfiguration

**Purpose**: Represents cache behavior settings that control caching policy, resource limits, and model eligibility.

**Lifecycle**:
- **Created**: On MCP server startup from environment variables
- **Updated**: Only via server restart with new environment values
- **Deleted**: On server shutdown

**Storage**: In-memory configuration object; source of truth is environment variables.

**Fields**:
| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| enabled | bool | Yes | true | Master switch to enable/disable caching |
| ttl_seconds | int | Yes | 600 | Cache entry lifetime in seconds (10 minutes) |
| max_cache_size_mb | int | Yes | 100 | Maximum memory allocation for cache |
| min_prompt_tokens | int | Yes | 1024 | Minimum token count for a prompt to be cacheable |
| eviction_policy | str | Yes | "lru" | Cache eviction strategy (only "lru" supported) |
| enabled_models | list[str] | Yes | ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash"] | Models eligible for caching |

**Relationships**:
- **Configures**: `PromptCache` (LRU cache instance)
- **Validated against**: Gemini API minimum token thresholds (1024 for Flash, 4096 for Pro)

**Validation Rules**:
- `ttl_seconds` >= 60 and <= 3600 (Gemini API limits)
- `max_cache_size_mb` >= 10 (minimum practical size)
- `min_prompt_tokens` >= 1024 (below this, caching overhead exceeds benefit)
- `eviction_policy` must be "lru" (only supported policy)
- `enabled_models` must be non-empty list of valid Gemini model identifiers

**Environment Variable Mapping**:
| Field | Environment Variable |
|-------|---------------------|
| enabled | `MADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED` |
| ttl_seconds | `MADEINOZ_KNOWLEDGE_PROMPT_CACHE_TTL` |
| max_cache_size_mb | `MADEINOZ_KNOWLEDGE_PROMPT_CACHE_SIZE_MB` |
| min_prompt_tokens | `MADEINOZ_KNOWLEDGE_PROMPT_CACHE_MIN_TOKENS` |
| eviction_policy | Not configurable (hardcoded to "lru") |
| enabled_models | Not configurable (uses all supported Gemini models) |

**Example Instance**:
```json
{
  "enabled": true,
  "ttl_seconds": 600,
  "max_cache_size_mb": 100,
  "min_prompt_tokens": 1024,
  "eviction_policy": "lru",
  "enabled_models": ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash"]
}
```

---

### CacheEntry

**Purpose**: Represents a single cached prompt component stored in the local LRU cache, tracking both Gemini's remote cache reference and local access patterns.

**Lifecycle**:
- **Created**: When a prompt meets caching criteria and Gemini cache is created
- **Updated**: On each cache hit (access_count incremented, last_accessed updated)
- **Deleted**: On TTL expiration, LRU eviction, or manual cache clear

**Storage**: In-memory `OrderedDict` keyed by `prompt_hash`.

**Fields**:
| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| prompt_hash | str | Yes | - | SHA-256 hash of the prompt content (unique identifier) |
| cached_content_name | str | Yes | - | Gemini API cache reference (e.g., "caches/abc123") |
| model | str | Yes | - | Gemini model this cache entry is valid for |
| created_at | datetime | Yes | - | When the cache entry was created |
| last_accessed | datetime | Yes | - | Most recent access timestamp |
| expires_at | datetime | Yes | - | When this entry will be evicted (created_at + TTL) |
| access_count | int | Yes | 1 | Number of times this entry was accessed |
| token_count | int | Yes | - | Number of tokens in the cached prompt |
| size_bytes | int | Yes | - | Estimated memory footprint (for LRU size tracking) |

**Relationships**:
- **Managed by**: `PromptCache` (LRU cache manager)
- **References**: Gemini API cached content via `cached_content_name`
- **Produces**: `CacheMetrics` on cache hit

**Validation Rules**:
- `prompt_hash` must be valid 64-character hex string (SHA-256)
- `cached_content_name` must match Gemini cache name pattern `caches/[a-z0-9]+`
- `created_at` <= `last_accessed` <= `now`
- `expires_at` > `created_at`
- `access_count` >= 1
- `token_count` >= `min_prompt_tokens` from configuration
- `size_bytes` > 0

**Computed Properties**:
- `is_expired` = `datetime.now() > expires_at`
- `time_to_live` = `expires_at - datetime.now()` (can be negative if expired)
- `hit_rate` = `access_count / (access_count + 1)` per entry (useful for debugging)

**Example Instance**:
```json
{
  "prompt_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
  "cached_content_name": "caches/abc123def456",
  "model": "gemini-2.5-flash",
  "created_at": "2026-01-27T10:30:00Z",
  "last_accessed": "2026-01-27T10:35:42Z",
  "expires_at": "2026-01-27T10:40:00Z",
  "access_count": 7,
  "token_count": 1523,
  "size_bytes": 6092
}
```

---

### SessionMetrics (Derived Entity)

**Purpose**: Aggregated caching statistics across all requests in a session, enabling session-level cost analysis and hit rate tracking.

**Lifecycle**:
- **Created**: On first Gemini API request in session
- **Updated**: After each Gemini API request completes
- **Deleted**: On session end or server restart

**Storage**: In-memory counter object; optionally exposed via health endpoint.

**Fields**:
| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| total_requests | int | Yes | 0 | Count of all Gemini API requests |
| cache_hits | int | Yes | 0 | Count of requests with cache_hit=true |
| cache_misses | int | Yes | 0 | Count of requests with cache_hit=false |
| total_cached_tokens | int | Yes | 0 | Sum of cached_tokens across all requests |
| total_prompt_tokens | int | Yes | 0 | Sum of prompt_tokens across all requests |
| total_completion_tokens | int | Yes | 0 | Sum of completion_tokens across all requests |
| total_cost_without_cache | float | Yes | 0.0 | Sum of hypothetical costs (USD) |
| total_actual_cost | float | Yes | 0.0 | Sum of actual costs (USD) |
| total_cost_saved | float | Yes | 0.0 | Cumulative savings (USD) |
| session_start | datetime | Yes | - | When the session began |
| last_request | datetime | Yes | - | Most recent request timestamp |

**Computed Properties**:
- `cache_hit_rate` = `cache_hits / total_requests * 100` (percentage)
- `overall_savings_percent` = `total_cost_saved / total_cost_without_cache * 100`
- `average_cached_tokens_per_request` = `total_cached_tokens / total_requests`
- `session_duration` = `last_request - session_start`

**Example Instance**:
```json
{
  "total_requests": 47,
  "cache_hits": 31,
  "cache_misses": 16,
  "total_cached_tokens": 42350,
  "total_prompt_tokens": 68200,
  "total_completion_tokens": 15890,
  "total_cost_without_cache": 0.0512,
  "total_actual_cost": 0.0298,
  "total_cost_saved": 0.0214,
  "session_start": "2026-01-27T09:00:00Z",
  "last_request": "2026-01-27T10:42:15Z",
  "cache_hit_rate": 65.96,
  "overall_savings_percent": 41.80
}
```

---

### PricingTier (Reference Entity)

**Purpose**: Defines pricing rates for a specific Gemini model, used for cost calculations.

**Lifecycle**:
- **Created**: At module load from hardcoded defaults
- **Updated**: Via environment variable overrides (future enhancement)
- **Deleted**: Never (static reference data)

**Storage**: Module-level constant dictionary.

**Fields**:
| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| model | str | Yes | - | Gemini model identifier |
| input_per_million | float | Yes | - | Standard input token cost (USD per 1M tokens) |
| input_per_million_over_200k | float | No | null | Input cost for contexts >200K tokens |
| cached_input_per_million | float | Yes | - | Cached token cost (USD per 1M tokens) |
| cached_input_per_million_over_200k | float | No | null | Cached cost for contexts >200K tokens |
| output_per_million | float | Yes | - | Output token cost (USD per 1M tokens) |
| output_per_million_over_200k | float | No | null | Output cost for contexts >200K tokens |
| storage_per_million_hour | float | Yes | - | Cache storage cost (USD per 1M tokens per hour) |
| min_cache_tokens | int | Yes | 1024 | Minimum tokens for caching eligibility |

**Pricing Data (January 2026)**:
| Model | Input | Cached Input | Output | Cache Discount |
|-------|-------|--------------|--------|----------------|
| gemini-2.5-flash | $0.30 | $0.03 | $2.50 | 90% |
| gemini-2.5-pro | $1.25 | $0.125 | $10.00 | 90% |
| gemini-2.0-flash | $0.10 | $0.01 | $0.40 | 90% |

---

## Data Flow

```
                                    REQUEST FLOW
                                    ============

    +------------------+       +-------------------+       +------------------+
    |  MCP Client      |------>|  MCP Server       |------>|  Gemini API      |
    |  (Claude Code)   |       |  (graphiti_mcp)   |       |  (google.genai)  |
    +------------------+       +--------+----------+       +--------+---------+
                                        |                           |
                                        v                           |
                               +--------+----------+                |
                               | GeminiCachingClient|<---------------+
                               | (wrapper)         |    usage_metadata
                               +--------+----------+
                                        |
                    +-------------------+-------------------+
                    |                   |                   |
                    v                   v                   v
           +--------+-------+  +--------+-------+  +--------+-------+
           | PromptCache    |  | PricingCalculator|  | MetricsLogger  |
           | (LRU+TTL)      |  | (cost math)     |  | (JSONL output) |
           +--------+-------+  +--------+-------+  +----------------+
                    |                   |
                    v                   v
           +--------+------------------+--------+
           |         CacheMetrics               |
           |  (embedded in response metadata)   |
           +------------------------------------+


                                  CACHE LIFECYCLE
                                  ===============

    [Request Received]
           |
           v
    +------+------+
    | Hash Prompt |
    +------+------+
           |
           v
    +------+------+     YES     +-------------------+
    | In Cache?   |------------>| Update last_access|
    +------+------+             | Increment count   |
           | NO                 +--------+----------+
           v                             |
    +------+------+                      |
    | Check Token |                      |
    | Count >= Min|                      v
    +------+------+     NO      +--------+----------+
           |  |---------------->| Skip Caching      |
           | YES                | (direct API call) |
           v                    +-------------------+
    +------+------+
    | Create      |
    | Gemini Cache|
    +------+------+
           |
           v
    +------+------+
    | Store Entry |
    | in LRU      |
    +------+------+
           |
           v
    +------+------+
    | Check Size  |     OVER LIMIT     +-------------------+
    | Limit       |-------------------->| LRU Eviction     |
    +-------------+                     | (remove oldest)  |
                                        +-------------------+
```

---

## Index Definitions

### Local Cache Index
- **Primary Key**: `prompt_hash` (SHA-256, 64 chars)
- **Ordering**: Access recency (for LRU eviction)
- **Constraints**: Unique per model (same prompt can have different caches per model)

### Metrics Log Index
- **Primary Key**: `timestamp` + `request_id`
- **Partitioning**: By date (daily JSONL files)
- **Retention**: Configurable (default: 7 days)

---

## Migration Notes

**No database migrations required** - this feature uses:
1. In-memory cache (`OrderedDict`)
2. Append-only metrics log (JSONL)
3. Environment variable configuration

**Backward Compatibility**:
- Existing MCP responses continue to work without cache metrics if caching disabled
- New `cache_metrics` field is optional in response schema
- Prometheus metrics endpoint is a new addition (no health endpoint changes)

---

## Prometheus Metrics Schema (Added 2026-01-27)

**Note**: Cache statistics are exposed via Prometheus/OpenTelemetry metrics endpoint rather than health endpoint extension. See `contracts/metrics.md` for full specification.

### Counters (Cumulative)
| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `graphiti_cache_hits_total` | counter | model | Total cache hits |
| `graphiti_cache_misses_total` | counter | model | Total cache misses |
| `graphiti_cache_tokens_saved_total` | counter | model | Total tokens saved |
| `graphiti_cache_cost_saved_total` | counter | model | Total cost saved (USD) |
| `graphiti_cache_requests_total` | counter | model | Total API requests |

### Gauges (Current State)
| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `graphiti_cache_hit_rate` | gauge | model | Current hit rate percentage |
| `graphiti_cache_entries` | gauge | - | Current cache entry count |
| `graphiti_cache_size_bytes` | gauge | - | Current cache memory usage |
| `graphiti_cache_enabled` | gauge | - | Caching enabled flag (1/0) |

**Rationale for Prometheus over Health Endpoint**:
- Industry standard observability format
- Native time-series support for trending
- Grafana integration out-of-box
- Standard alerting via Prometheus rules
- User explicitly requested this approach
