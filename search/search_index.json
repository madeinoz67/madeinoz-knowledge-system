{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Madeinoz Knowledge System","text":""},{"location":"index.html#madeinoz-knowledge-system","title":"Madeinoz Knowledge System","text":"<p>Persistent personal knowledge management system powered by Graphiti knowledge graph - automatically extracts entities, relationships, and temporal context from conversations, documents, and ideas.</p>"},{"location":"index.html#what-it-does","title":"What It Does","text":"<p>The Madeinoz Knowledge System transforms your AI conversations into a permanent, searchable knowledge base:</p> <ul> <li>Automatically Learns: Extracts entities and relationships as you work</li> <li>Connects Concepts: Maps how ideas relate over time</li> <li>Semantic Search: Finds relevant knowledge using natural language</li> <li>Builds Context: Compounds knowledge across sessions</li> <li>Never Forgets: Persistent storage with temporal tracking</li> </ul> <p>Core principle: Work normally, knowledge handles itself.</p>"},{"location":"index.html#system-architecture","title":"System Architecture","text":"<p>The knowledge graph sits at the center, automatically organizing your conversations, documents, code, and notes into searchable entities, episodes, facts, and relationships. As memories age, they transition through lifecycle states (ACTIVE \u2192 DORMANT \u2192 ARCHIVED \u2192 EXPIRED) based on importance and stability scores, while Prometheus metrics and Grafana dashboards provide real-time observability.</p>"},{"location":"index.html#quick-start","title":"Quick Start","text":"<p>New to the system? Follow this path:</p> <ol> <li>Overview - What the system does and your first steps (5 min)</li> <li>Installation Guide - Step-by-step setup instructions (15 min)</li> <li>Basic Usage - How to capture and search knowledge (10 min)</li> <li>Quick Reference - Commands at a glance</li> </ol> <p>Total time to get started: 30 minutes</p>"},{"location":"index.html#documentation-sections","title":"Documentation Sections","text":""},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>Start here if you're new:</p> <ul> <li>Overview - What the system does and quick start</li> <li>Quick Reference - Commands and natural language triggers</li> </ul>"},{"location":"index.html#installation","title":"Installation","text":"<p>Set up the system:</p> <ul> <li>Installation Guide - Complete setup instructions</li> <li>Requirements - Prerequisites and dependencies</li> <li>Verification - Confirm everything works</li> </ul>"},{"location":"index.html#usage","title":"Usage","text":"<p>Learn how to use the system:</p> <ul> <li>Basic Usage - Capturing and searching knowledge</li> <li>Advanced Usage - Bulk import, backup, multiple graphs</li> <li>Memory Decay &amp; Lifecycle - Memory prioritization, decay scoring, and lifecycle management</li> <li>Monitoring - Prometheus and Grafana dashboards</li> </ul>"},{"location":"index.html#concepts","title":"Concepts","text":"<p>Understand how it works:</p> <ul> <li>Architecture - System design and components</li> <li>Knowledge Graph - Episodes, entities, facts explained</li> </ul>"},{"location":"index.html#troubleshooting","title":"Troubleshooting","text":"<p>Fix common issues:</p> <ul> <li>Common Issues - Solutions to frequent problems</li> </ul>"},{"location":"index.html#reference","title":"Reference","text":"<p>Detailed specifications:</p> <ul> <li>CLI Reference - Command-line interface</li> <li>Configuration - Environment variables and settings</li> <li>Observability &amp; Metrics - Prometheus metrics, monitoring, caching</li> <li>Model Guide - Ollama and LLM configuration</li> <li>Benchmarks - Model performance comparisons</li> </ul>"},{"location":"index.html#natural-language-commands","title":"Natural Language Commands","text":"<p>The system responds to natural conversation:</p> Say This System Does \"Remember that...\" Captures knowledge with entity extraction \"What do I know about X?\" Searches knowledge base semantically \"How are X and Y related?\" Finds relationships between concepts \"What did I learn today?\" Temporal search - filter by date \"What did I learn recently?\" Shows recent knowledge additions \"Knowledge status\" Displays system health and statistics"},{"location":"index.html#database-backends","title":"Database Backends","text":"<p>Two graph database options:</p> Backend Best For Web UI Neo4j (default) CTI/OSINT data, rich queries localhost:7474 FalkorDB Simple setup, lower resources localhost:3000"},{"location":"index.html#key-features","title":"Key Features","text":""},{"location":"index.html#prompt-caching-gemini-via-openrouter","title":"Prompt Caching (Gemini via OpenRouter)","text":"<p>Reduce LLM costs by up to 15-20% (est) with prompt caching for Gemini models via OpenRouter:</p> <pre><code># Enable in ~/.claude/.env (disabled by default)\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED=true\nMADEINOZ_KNOWLEDGE_MODEL_NAME=google/gemini-2.0-flash-001\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-or-v1-your-key\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\n</code></pre> <p>Note: Prompt caching is disabled by default and must be explicitly enabled. The system uses explicit <code>cache_control</code> markers (similar to Anthropic) - not implicit caching. OpenRouter manages the cache lifecycle automatically when enabled. See Observability &amp; Metrics for details.</p>"},{"location":"index.html#observability-metrics","title":"Observability &amp; Metrics","text":"<p>Monitor API usage, costs, and performance via Prometheus:</p> <pre><code># View metrics\ncurl http://localhost:9091/metrics | grep graphiti_\n\n# Key metrics available:\n# - graphiti_api_cost_total (USD spent)\n# - graphiti_llm_request_duration_seconds (latency)\n# - graphiti_cache_hit_rate (caching effectiveness)\n</code></pre> <p>See Observability &amp; Metrics for full documentation.</p>"},{"location":"index.html#common-commands","title":"Common Commands","text":""},{"location":"index.html#server-management","title":"Server Management","text":"<pre><code># Check status\nbun run server-cli status\n\n# Start server (production mode)\nbun run server-cli start\n\n# Stop server\nbun run server-cli stop\n\n# Restart server\nbun run server-cli restart\n\n# View logs\nbun run server-cli logs\n\n# View logs with options\nbun run server-cli logs --mcp --tail 50\n</code></pre>"},{"location":"index.html#development-mode","title":"Development Mode","text":"<p>For development and testing server code changes:</p> <pre><code># Start in development mode (uses dev ports and env files)\nbun run server-cli start --dev\n\n# Restart in development mode\nbun run server-cli restart --dev\n\n# Dev mode differences:\n# - Neo4j Browser: http://localhost:7475 (instead of 7474)\n# - MCP Server: http://localhost:8001/mcp/ (instead of 8000)\n# - Uses /tmp/madeinoz-knowledge-*-dev.env files\n# - Safe for development without affecting production\n</code></pre>"},{"location":"index.html#memory-sync","title":"Memory Sync","text":"<pre><code># Manual sync (from installed location)\nbun run ~/.claude/hooks/sync-memory-to-knowledge.ts\n\n# Dry run (see what would sync)\nbun run ~/.claude/hooks/sync-memory-to-knowledge.ts --dry-run\n</code></pre>"},{"location":"index.html#need-help","title":"Need Help?","text":"<ol> <li>Check the Troubleshooting Guide</li> <li>Review Key Concepts</li> <li>Look for examples in the Usage Guide</li> </ol>"},{"location":"index.html#quick-reference-card","title":"Quick Reference Card","text":""},{"location":"index.html#mcp-tools","title":"MCP Tools","text":"Tool Purpose Example <code>add_memory</code> Store knowledge <code>{\"name\": \"Note\", \"episode_body\": \"...\", \"group_id\": \"main\"}</code> <code>search_memory_nodes</code> Find entities <code>{\"query\": \"Python frameworks\", \"limit\": 10}</code> <code>search_memory_facts</code> Find relationships <code>{\"query\": \"how X relates to Y\"}</code> <code>get_episodes</code> Temporal retrieval <code>{\"group_id\": \"main\", \"last_n\": 10}</code> <code>get_status</code> Health check <code>{}</code>"},{"location":"index.html#environment-variables-essential","title":"Environment Variables (Essential)","text":"<pre><code># LLM (required)\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=google/gemini-2.5-flash\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-or-v1-...\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\n\n# Embeddings (required)\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=ollama\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://host.docker.internal:11434/v1\n\n# Caching (optional, recommended)\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED=true\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_METRICS_ENABLED=true\n</code></pre>"},{"location":"index.html#ports","title":"Ports","text":"Port Service Environment 8000 MCP Server Production 8001 MCP Server Development 7474 Neo4j Browser Production 7475 Neo4j Browser Development 9090 Prometheus Metrics Production 9091 Prometheus Metrics Development"},{"location":"index.html#key-metrics","title":"Key Metrics","text":"Metric What It Measures <code>graphiti_api_cost_total</code> Total USD spent on LLM API <code>graphiti_total_tokens_total</code> Total tokens consumed <code>graphiti_cache_hit_rate</code> Cache effectiveness (%) <code>graphiti_llm_request_duration_seconds</code> Request latency <code>graphiti_llm_errors_total</code> API error count"},{"location":"index.html#limits-constraints","title":"Limits &amp; Constraints","text":"Limit Default Configurable Notes Rate limit 60 req/60s per IP Yes <code>RATE_LIMIT_MAX_REQUESTS</code>, <code>RATE_LIMIT_WINDOW_SECONDS</code> Concurrent LLM requests 10 Yes <code>SEMAPHORE_LIMIT</code> Search results (nodes) 10 Yes <code>max_nodes</code> parameter Search results (facts) 10 Yes <code>max_facts</code> parameter Search results (episodes) 10 Yes <code>max_episodes</code> parameter Cache minimum tokens 1024 No Requests &lt; 1024 tokens skip caching Episode body size No limit N/A Limited only by LLM context window <p>Note: Episode body size is not explicitly limited. Very large episodes (&gt;100KB) may cause slow processing or LLM context overflow. For bulk imports, consider chunking large documents.</p>"},{"location":"index.html#credits-acknowledgments","title":"Credits &amp; Acknowledgments","text":"<p>This system is built on Graphiti by Zep AI, with graph database support from Neo4j and FalkorDB. It is part of the Personal AI Infrastructure (PAI) ecosystem.</p> <p>See Acknowledgments for full credits to projects, research, and the community that inspired this system.</p>"},{"location":"acknowledgments.html","title":"Acknowledgments","text":""},{"location":"acknowledgments.html#acknowledgments","title":"Acknowledgments","text":"<p>The Madeinoz Knowledge System is built on the shoulders of giants. This page acknowledges the projects, communities, and individuals who made this system possible.</p>"},{"location":"acknowledgments.html#core-dependencies","title":"Core Dependencies","text":""},{"location":"acknowledgments.html#graphiti-by-zep","title":"Graphiti by Zep","text":"<p>Graphiti is the foundational knowledge graph engine that powers this system. It provides:</p> <ul> <li>Temporal Knowledge Graph: Bi-temporal tracking with <code>created_at</code> and <code>last_updated</code> timestamps</li> <li>Entity Extraction: Automatic extraction of entities, relationships, and episodes from text</li> <li>Vector Embeddings: Semantic search capabilities using OpenAI embeddings</li> <li>Neo4j/FalkorDB Backend: Flexible graph database support</li> </ul> <p>Graphiti is an open-source library by Zep for building long-term memory for AI agents.</p>"},{"location":"acknowledgments.html#neo4j","title":"Neo4j","text":"<p>Neo4j is the default graph database backend, providing:</p> <ul> <li>Native graph storage and querying with Cypher</li> <li>Vector similarity search for semantic retrieval</li> <li>ACID transactions for reliable knowledge storage</li> <li>Neo4j Browser for visual graph exploration</li> </ul>"},{"location":"acknowledgments.html#falkordb","title":"FalkorDB","text":"<p>FalkorDB is the alternative graph database backend, providing:</p> <ul> <li>Redis-based graph storage with RediSearch</li> <li>Lucene-based full-text and vector search</li> <li>In-memory performance for fast queries</li> <li>Built-in web UI at port 3000</li> </ul>"},{"location":"acknowledgments.html#inspiration-community","title":"Inspiration &amp; Community","text":""},{"location":"acknowledgments.html#personal-ai-infrastructure-pai","title":"Personal AI Infrastructure (PAI)","text":"<p>PAI is the conceptual foundation for this knowledge system. The PAI project demonstrates how AI assistants can maintain persistent, queryable memory across conversations.</p> <p>Specifically, we'd like to acknowledge:</p> <ul> <li>PAI Discussion #527: Knowledge System Long-term Memory Strategy - This community discussion directly inspired the Memory Decay Scoring &amp; Lifecycle Management feature (Feature 009). The concepts of importance classification, stability scoring, and automated memory lifecycle management emerged from these conversations.</li> </ul> <p>The PAI community's approach to: - Conversational Knowledge Capture: Storing memories without manual organization - Automatic Entity Extraction: Using LLMs to identify people, organizations, and concepts - Relationship Mapping: Tracking how entities connect across episodes - Semantic Search: Finding information by meaning, not just keywords</p> <p>...forms the core philosophy of this system.</p>"},{"location":"acknowledgments.html#memory-systems-research","title":"Memory Systems Research","text":"<p>This system incorporates research and concepts from academic and industry sources:</p> <ul> <li>A-MEM: Agentic Memory for LLMs (NeurIPS 2025) - Memory architecture for agentic AI systems</li> <li>MemOS: An Operating System for LLM Agents - Hierarchical memory with priority-based retrieval</li> <li>Mem0 - Fact extraction and memory management for AI</li> <li>FSRS: Free Spaced Repetition Scheduler - Spaced repetition algorithms for memory retention</li> </ul>"},{"location":"acknowledgments.html#development-tools","title":"Development Tools","text":""},{"location":"acknowledgments.html#bun","title":"Bun","text":"<p>Bun is the TypeScript runtime and package manager used for CLI tools and server orchestration.</p>"},{"location":"acknowledgments.html#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":"<p>MCP provides the standard protocol for communication between the knowledge system and AI assistants like Claude Code.</p>"},{"location":"acknowledgments.html#mkdocs-material","title":"MkDocs Material","text":"<p>Documentation theme and build system for this documentation site.</p>"},{"location":"acknowledgments.html#community-contributors","title":"Community &amp; Contributors","text":"<p>This system exists to serve the PAI community and the broader AI assistant ecosystem. We extend our gratitude to:</p> <ul> <li>Daniel Miessler for creating and maintaining PAI</li> <li>The PAI community for feedback, discussions, and feature ideas</li> <li>Zep for building and open-sourcing Graphiti</li> <li>All contributors who report issues, submit PRs, and improve the system</li> </ul>"},{"location":"acknowledgments.html#license","title":"License","text":"<p>This project is licensed under the MIT License.</p> <p>The Madeinoz Knowledge System is a PAI Pack - a pluggable component for the Personal AI Infrastructure ecosystem.</p> <p>Join the Community:</p> <ul> <li>\ud83c\udf1f Star on GitHub</li> <li>\ud83d\udcac PAI Discussions</li> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udc1b Report Issues</li> </ul>"},{"location":"remote-access.html","title":"Remote Access","text":""},{"location":"remote-access.html#remote-mcp-access","title":"Remote MCP Access","text":"<p>Feature: Remote MCP Access for Knowledge CLI Last Updated: 2026-01-30</p>"},{"location":"remote-access.html#overview","title":"Overview","text":"<p>The Madeinoz Knowledge System supports remote MCP access, enabling you to connect to a centralized knowledge graph from different machines on your network. This capability is useful for:</p> <ul> <li>Team knowledge sharing: Multiple users accessing a shared knowledge graph</li> <li>Remote development: Accessing your knowledge graph from different machines</li> <li>Production deployments: Running the knowledge server on dedicated infrastructure</li> </ul>"},{"location":"remote-access.html#quick-reference","title":"Quick Reference","text":""},{"location":"remote-access.html#environment-variables","title":"Environment Variables","text":"Variable Description Default Example <code>MADEINOZ_KNOWLEDGE_PROFILE</code> Profile name to use <code>default</code> <code>production</code> <code>MADEINOZ_KNOWLEDGE_HOST</code> Server hostname or IP <code>localhost</code> <code>knowledge.example.com</code> <code>MADEINOZ_KNOWLEDGE_PORT</code> Server port <code>8001</code> <code>443</code> <code>MADEINOZ_KNOWLEDGE_PROTOCOL</code> Connection protocol <code>http</code> <code>https</code> <code>MADEINOZ_KNOWLEDGE_TLS_VERIFY</code> Verify TLS certificates <code>true</code> <code>false</code> <code>MADEINOZ_KNOWLEDGE_TLS_CA</code> CA certificate path - <code>/etc/ssl/certs/ca.pem</code> <code>MADEINOZ_KNOWLEDGE_TIMEOUT</code> Connection timeout (ms) <code>30000</code> <code>60000</code> <code>MCP_HOST</code> Server bind address <code>127.0.0.1</code> <code>0.0.0.0</code> <code>MCP_PORT</code> Server listening port <code>8000</code> <code>8001</code> <code>MCP_TLS_ENABLED</code> Enable server TLS <code>false</code> <code>true</code> <p>Configuration Priority (highest to lowest): 1. CLI flags (<code>--host</code>, <code>--port</code>, etc.) 2. Individual environment variables (<code>MADEINOZ_KNOWLEDGE_HOST</code>) 3. Selected profile (<code>MADEINOZ_KNOWLEDGE_PROFILE</code>) 4. Default profile in YAML file 5. Code defaults</p>"},{"location":"remote-access.html#cli-commands","title":"CLI Commands","text":"Command Description Example <code>bun run server start</code> Start MCP server <code>bun run server start</code> <code>bun run server status</code> Check server health <code>bun run server status</code> <code>bun run knowledge-cli --status</code> Show connection status <code>bun run knowledge-cli --status</code> <code>bun run knowledge-cli --list-profiles</code> List available profiles <code>bun run knowledge-cli --list-profiles</code> <code>bun run knowledge-cli --host &lt;host&gt;</code> Connect to specific host <code>bun run knowledge-cli --host 192.168.1.100 search_nodes \"query\"</code>"},{"location":"remote-access.html#default-ports","title":"Default Ports","text":"Port Service Protocol <code>8001</code> MCP HTTP endpoint HTTP/HTTPS <code>8000</code> MCP Server (internal) HTTP <code>7474</code> Neo4j Browser HTTP <code>7687</code> Neo4j Bolt Protocol Bolt"},{"location":"remote-access.html#configuration","title":"Configuration","text":""},{"location":"remote-access.html#profile-configuration","title":"Profile Configuration","text":"<p>Connection profiles are stored in <code>$PAI_DIR/config/knowledge-profiles.yaml</code> (or <code>~/.claude/config/knowledge-profiles.yaml</code>):</p> <pre><code>version: \"1.0\"\ndefault_profile: default\n\nprofiles:\n  default:\n    host: localhost\n    port: 8001\n    protocol: http\n\n  production:\n    host: knowledge.example.com\n    port: 443\n    protocol: https\n    tls:\n      verify: true\n      minVersion: TLSv1.3\n\n  development:\n    host: 192.168.1.100\n    port: 8000\n    protocol: https\n    tls:\n      verify: false  # Self-signed certificate\n</code></pre>"},{"location":"remote-access.html#tls-configuration","title":"TLS Configuration","text":"<p>For production deployments with TLS:</p> <pre><code>profiles:\n  production-tls:\n    host: knowledge.example.com\n    port: 443\n    protocol: https\n    tls:\n      verify: true\n      ca: /path/to/ca.pem\n      minVersion: TLSv1.2\n      maxVersion: TLSv1.3\n</code></pre>"},{"location":"remote-access.html#timeout-configuration","title":"Timeout Configuration","text":"<p>Connection timeout defaults to 30 seconds (30000ms). Configure via:</p> <pre><code># Environment variable\nexport MADEINOZ_KNOWLEDGE_TIMEOUT=60000  # 60 seconds\n\n# In profile\nprofiles:\n  slow-network:\n    host: remote.example.com\n    port: 8001\n    timeout: 60000\n</code></pre>"},{"location":"remote-access.html#usage","title":"Usage","text":""},{"location":"remote-access.html#localhost-default","title":"Localhost (Default)","text":"<p>The knowledge CLI works out of the box with localhost:</p> <pre><code># Start the MCP server\nbun run server start\n\n# Run a knowledge query (uses localhost:8001 by default)\nbun run knowledge-cli search_nodes \"my query\"\n\n# Check connection status\nbun run knowledge-cli --status\n</code></pre> <p>\u26a0\ufe0f Deprecation Notice: Localhost-only access is deprecated for production deployments. Use remote access configuration for team environments.</p>"},{"location":"remote-access.html#remote-connection","title":"Remote Connection","text":"<p>Using environment variables: </p><pre><code># Set the remote host\nexport MADEINOZ_KNOWLEDGE_HOST=192.168.1.100\nexport MADEINOZ_KNOWLEDGE_PORT=8001\n\n# Run knowledge commands\nbun run knowledge-cli search_nodes \"my query\"\n</code></pre><p></p> <p>Using CLI flags: </p><pre><code># Override connection settings per command\nbun run knowledge-cli --host knowledge.example.com --port 8001 search_nodes \"my query\"\n\n# With HTTPS\nbun run knowledge-cli --host knowledge.example.com --protocol https --port 443 search_nodes \"my query\"\n</code></pre><p></p> <p>Using profiles: </p><pre><code># Use specific profile\nexport MADEINOZ_KNOWLEDGE_PROFILE=production\n\n# Run commands (uses production profile)\nbun run knowledge-cli search_nodes \"my query\"\n\n# List available profiles\nbun run knowledge-cli --list-profiles\n</code></pre><p></p>"},{"location":"remote-access.html#server-configuration","title":"Server Configuration","text":"<p>Enable external access: </p><pre><code># Set the MCP host to all interfaces\nexport MCP_HOST=0.0.0.0\n\n# Restart the server\nbun run server restart\n</code></pre><p></p> <p>Enable TLS on server: </p><pre><code># Generate self-signed certificate (development)\nopenssl req -x509 -newkey rsa:4096 -keyout certs/key.pem -out certs/cert.pem -days 365 -nodes -subj \"/CN=knowledge.example.com\"\n\n# Enable TLS in environment\nexport MCP_TLS_ENABLED=true\nexport MCP_TLS_CERTPATH=/certs/cert.pem\nexport MCP_TLS_KEYPATH=/certs/key.pem\n\n# Restart server with TLS\nbun run server restart\n</code></pre><p></p>"},{"location":"remote-access.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"remote-access.html#connection-refused","title":"Connection Refused","text":"<p>Symptom: <code>ECONNREFUSED</code> when connecting</p> <p>Solutions: 1. Check server is running: <code>bun run server status</code> 2. Verify <code>MCP_HOST=0.0.0.0</code> on server 3. Check firewall allows port <code>8001</code> (or custom port) 4. Test with <code>curl http://&lt;host&gt;:&lt;port&gt;/health</code> 5. Verify network connectivity: <code>ping &lt;server-host&gt;</code></p> <p>Diagnostic commands: </p><pre><code># Check if server is listening\nnetstat -an | grep 8001\n\n# Test HTTP endpoint\ncurl -v http://&lt;host&gt;:8001/health\n\n# Check firewall status\nsudo ufw status  # Linux\nsudo pfctl -s rules  # macOS\n</code></pre><p></p>"},{"location":"remote-access.html#tls-certificate-errors","title":"TLS Certificate Errors","text":"<p>Symptom: <code>certificate verify failed</code> or <code>unable to verify</code></p> <p>Solutions: 1. For self-signed certificates (development only):    </p><pre><code>export MADEINOZ_KNOWLEDGE_TLS_VERIFY=false\n</code></pre><p></p> <ol> <li> <p>For custom CA certificates:    </p><pre><code>export MADEINOZ_KNOWLEDGE_TLS_CA=/path/to/ca.pem\n</code></pre><p></p> </li> <li> <p>Check certificate validity:    </p><pre><code># View certificate details\nopenssl x509 -in cert.pem -noout -text\n\n# Check expiration\nopenssl x509 -in cert.pem -noout -dates\n\n# Verify hostname matches\nopenssl x509 -in cert.pem -noout -subject\n</code></pre><p></p> </li> <li> <p>Verify certificate chain:    </p><pre><code>openssl s_client -connect knowledge.example.com:443 -showcerts\n</code></pre><p></p> </li> </ol>"},{"location":"remote-access.html#timeout-errors","title":"Timeout Errors","text":"<p>Symptom: <code>Connection timed out</code> after 30 seconds</p> <p>Solutions: 1. Check network connectivity:    </p><pre><code>ping &lt;server-host&gt;\ntraceroute &lt;server-host&gt;\n</code></pre><p></p> <ol> <li> <p>Increase timeout:    </p><pre><code>export MADEINOZ_KNOWLEDGE_TIMEOUT=60000  # 60 seconds\n</code></pre><p></p> </li> <li> <p>Verify server is listening:    </p><pre><code>netstat -an | grep 8001\nlsof -i :8001\n</code></pre><p></p> </li> <li> <p>Check for network issues:    </p><pre><code># Test port connectivity\nnc -zv &lt;host&gt; 8001\ntelnet &lt;host&gt; 8001\n</code></pre><p></p> </li> </ol>"},{"location":"remote-access.html#profile-not-found","title":"Profile Not Found","text":"<p>Symptom: <code>Profile 'production' not found</code></p> <p>Solutions: 1. Check profile file exists:    </p><pre><code>ls $PAI_DIR/config/knowledge-profiles.yaml\nls ~/.claude/config/knowledge-profiles.yaml\n</code></pre><p></p> <ol> <li> <p>List available profiles:    </p><pre><code>bun run knowledge-cli --list-profiles\n</code></pre><p></p> </li> <li> <p>Verify YAML syntax:    </p><pre><code># Validate YAML\npython -c \"import yaml; yaml.safe_load(open('$PAI_DIR/config/knowledge-profiles.yaml'))\"\n</code></pre><p></p> </li> <li> <p>Check profile name in YAML file:    </p><pre><code>grep -A 5 \"profiles:\" $PAI_DIR/config/knowledge-profiles.yaml\n</code></pre><p></p> </li> </ol>"},{"location":"remote-access.html#concurrent-connection-issues","title":"Concurrent Connection Issues","text":"<p>Symptom: Errors when multiple clients connect simultaneously</p> <p>Solutions: 1. Check server logs for connection errors:    </p><pre><code>bun run server logs | grep -i connection\n</code></pre><p></p> <ol> <li> <p>Verify server configuration:    </p><pre><code># Query server configuration\ncurl http://&lt;host&gt;:8001/config\n\n# Check health endpoint\ncurl http://&lt;host&gt;:8001/health\n</code></pre><p></p> </li> <li> <p>Test concurrent connections:    </p><pre><code># Run multiple queries in parallel\nfor i in {1..10}; do\n  bun run knowledge-cli search_nodes \"test $i\" &amp;\ndone\nwait\n</code></pre><p></p> </li> </ol>"},{"location":"remote-access.html#security-best-practices","title":"Security Best Practices","text":"Practice Why How Use TLS in production Prevents data interception Set <code>MADEINOZ_KNOWLEDGE_PROTOCOL=https</code> Verify certificates Prevents MITM attacks Keep <code>MADEINOZ_KNOWLEDGE_TLS_VERIFY=true</code> Limit firewall access Reduces attack surface Allow only trusted IP ranges Use strong certificates Protects against compromise Use 2048-bit+ RSA or ECDSA Rotate certificates Limits exposure from compromise Rotate annually or quarterly Monitor logs Detects suspicious activity Check server logs regularly Avoid localhost-only Deprecated for production Use remote access configuration"},{"location":"remote-access.html#network-architecture","title":"Network Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Host Machine                            \u2502\n\u2502                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502         Docker Network: madeinoz-knowledge-net           \u2502  \u2502\n\u2502  \u2502                                                           \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502\n\u2502  \u2502  \u2502   graphiti-mcp  \u2502\u25c4\u2500\u2500\u2500\u2500\u25ba\u2502      neo4j              \u2502   \u2502  \u2502\n\u2502  \u2502  \u2502   (0.0.0.0:8001) \u2502      \u2502      (172.18.0.2:7687)  \u2502   \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2502\n\u2502  \u2502           \u2502                                               \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502              \u2502                                                    \u2502\n\u2502              \u25bc                                                   \u2502\n\u2502         Port 8001 (External)                                     \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n            \u2502 Network\n            \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Remote Client Machine                         \u2502\n\u2502                                                                  \u2502\n\u2502  knowledge-cli \u2500\u2500\u25ba https://knowledge.example.com:8001/mcp        \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"remote-access.html#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"remote-access.html#reverse-proxy-nginx","title":"Reverse Proxy (nginx)","text":"<p>For production deployments, use a reverse proxy:</p> <pre><code>upstream knowledge_mcp {\n    server localhost:8001;\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name knowledge.example.com;\n\n    ssl_certificate /etc/ssl/certs/knowledge.crt;\n    ssl_certificate_key /etc/ssl/private/knowledge.key;\n\n    location /mcp {\n        proxy_pass http://knowledge_mcp;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_connect_timeout 60s;\n        proxy_send_timeout 60s;\n        proxy_read_timeout 60s;\n    }\n\n    location /health {\n        proxy_pass http://knowledge_mcp;\n        access_log off;\n    }\n}\n</code></pre>"},{"location":"remote-access.html#docker-compose-override","title":"Docker Compose Override","text":"<p>Create <code>docker-compose.override.yml</code> for custom configuration:</p> <pre><code>services:\n  graphiti-mcp:\n    environment:\n      - MCP_HOST=0.0.0.0\n      - MCP_TLS_ENABLED=true\n    ports:\n      - \"8001:8001\"\n      - \"443:443\"\n    volumes:\n      - ./certs:/certs:ro\n</code></pre>"},{"location":"remote-access.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Installation Guide - Complete installation instructions</li> <li>Troubleshooting - Common issues and solutions</li> <li>Configuration Reference - Full configuration options</li> </ul>"},{"location":"weighted-search.html","title":"Weighted Search","text":""},{"location":"weighted-search.html#weighted-search-guide","title":"Weighted Search Guide","text":""},{"location":"weighted-search.html#overview","title":"Overview","text":"<p>Weighted search is an enhanced search mode that combines semantic similarity with recency and importance to provide more relevant results. Instead of ranking purely by how well text matches your query, weighted search considers:</p> <ul> <li>60% Semantic similarity - How well the content matches your query meaning</li> <li>25% Recency - How recently the item was accessed</li> <li>15% Importance - How significant the item is (rated 1-5)</li> </ul>"},{"location":"weighted-search.html#when-to-use-weighted-search","title":"When to Use Weighted Search","text":""},{"location":"weighted-search.html#use-weighted-search-weighted-when","title":"Use Weighted Search (<code>--weighted</code>) when:","text":"<ul> <li>Finding important knowledge - You want high-importance items to appear first</li> <li>Recent context matters - You need recently accessed information</li> <li>Comprehensive relevance - You want balanced ranking considering multiple factors</li> </ul>"},{"location":"weighted-search.html#use-standard-search-no-flag-when","title":"Use Standard Search (no flag) when:","text":"<ul> <li>Pure semantic match - You only care about text similarity</li> <li>Unbiased results - You want the raw vector embedding match</li> <li>Unfamiliar topics - You're exploring new concepts without established importance</li> </ul>"},{"location":"weighted-search.html#how-to-use","title":"How to Use","text":""},{"location":"weighted-search.html#basic-usage","title":"Basic Usage","text":"<pre><code># Standard search (semantic only)\nbun run src/skills/tools/knowledge-cli.ts search_nodes \"query\" 10\n\n# Weighted search (semantic + recency + importance)\nbun run src/skills/tools/knowledge-cli.ts search_nodes \"query\" 10 --weighted\n</code></pre>"},{"location":"weighted-search.html#with-temporal-filtering","title":"With Temporal Filtering","text":"<pre><code># Weighted search for last 7 days\nbun run src/skills/tools/knowledge-cli.ts search_nodes \"query\" 10 --weighted --since 7d\n\n# Weighted search within date range\nbun run src/skills/tools/knowledge-cli.ts search_nodes \"query\" 10 --weighted --since 2026-01-01 --until 2026-01-31\n</code></pre>"},{"location":"weighted-search.html#raw-output-for-scriptsdebugging","title":"Raw Output (for scripts/debugging)","text":"<pre><code># See full score breakdown\nbun run src/skills/tools/knowledge-cli.ts search_nodes \"query\" 5 --weighted --raw\n</code></pre>"},{"location":"weighted-search.html#understanding-the-results","title":"Understanding the Results","text":""},{"location":"weighted-search.html#standard-search-output","title":"Standard Search Output","text":"<pre><code>{\n  \"message\": \"Nodes retrieved successfully\",\n  \"nodes\": [\n    {\n      \"name\": \"knowledge system\",\n      \"summary\": \"...\",\n      \"created_at\": \"2026-02-02T12:59:36.120429+00:00\",\n      \"attributes\": {\n        \"importance\": 4,\n        \"stability\": 3,\n        \"decay_score\": 0\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"weighted-search.html#weighted-search-output","title":"Weighted Search Output","text":"<pre><code>{\n  \"message\": \"Nodes retrieved with weighted scoring (5 results)\",\n  \"nodes\": [\n    {\n      \"name\": \"knowledge system\",\n      \"summary\": \"...\",\n      \"created_at\": \"2026-02-02T12:59:36.120429+00:00\",\n      \"attributes\": {\n        \"weighted_score\": 0.91,\n        \"score_breakdown\": {\n          \"semantic\": 0.90,\n          \"recency\": 0.98,\n          \"importance\": 0.80\n        },\n        \"importance\": 4,\n        \"stability\": 3,\n        \"decay_score\": 0,\n        \"lifecycle_state\": \"ACTIVE\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"weighted-search.html#score-breakdown","title":"Score Breakdown","text":"Field Range Meaning <code>weighted_score</code> 0-1 Combined relevance score (higher = better) <code>score_breakdown.semantic</code> 0-1 Text similarity to query <code>score_breakdown.recency</code> 0-1 How recently accessed (1 = very recent) <code>score_breakdown.importance</code> 0-1 Importance level normalized (5 \u2192 1.0, 1 \u2192 0.2) <code>importance</code> 1-5 Raw importance rating <code>stability</code> 1-5 How stable/permanent this knowledge is <code>decay_score</code> 0-1 Memory decay score (0 = fresh) <code>lifecycle_state</code> string ACTIVE, DORMANT, ARCHIVED, EXPIRED, or SOFT_DELETED"},{"location":"weighted-search.html#test-results-examples","title":"Test Results &amp; Examples","text":""},{"location":"weighted-search.html#test-1-pai-algorithm-query","title":"Test 1: \"PAI algorithm\" Query","text":"<p>Result: Ranking remained the same, but weighted search added transparency.</p> Rank Item Weighted Score Semantic Recency Importance 1 PAI ALGORITHM 0.94 1.00 1.00 0.60 2 PAI pack logo 0.91 0.95 0.9998 0.60 3 PAI_DIR environment 0.88 0.90 0.9998 0.60 4 fixed-threshold system 0.85 0.85 0.9904 0.60 5 DAIV_PAI speaker 0.82 0.80 0.9885 0.60 <p>Observation: All items had importance=3, so semantic scores determined ranking. Weighted search provided explainability (why items ranked where they did).</p>"},{"location":"weighted-search.html#test-2-knowledge-query-different-importance-levels","title":"Test 2: \"knowledge\" Query (Different Importance Levels)","text":"<p>Result: Weighted search changed rankings to boost high-importance items.</p> Rank Change Unweighted Weighted Importance Why? \u2b06\ufe0f 3\u21922 knowledge system knowledge system 4 High importance boosted it \u2b07\ufe0f 2\u21923 knowledge-mcp container knowledge-mcp container 2 Low importance penalized it \u2b06\ufe0f 4\u21923 knowledge-cli knowledge-cli 4 High importance helped it <p>Full Results:</p> Rank Item (Unweighted) Imp Rank Item (Weighted) Score Imp 1 knowledge sy 3 1 knowledge sy 0.94 3 2 knowledge-mcp container 2 2 knowledge system 0.91 4 3 knowledge system 4 3 knowledge-mcp container 0.88 2 4 knowledge-cli 4 3 knowledge-cli 0.88 4 5 ghcr.io image 3 5 ghcr.io image 0.82 3 <p>Key Insight: When semantic scores are close, the 15% importance weight can change rankings. High-importance items (4) get priority over low-importance items (2).</p>"},{"location":"weighted-search.html#how-the-formula-works","title":"How the Formula Works","text":""},{"location":"weighted-search.html#calculation-example","title":"Calculation Example","text":"<p>For \"knowledge system\" with importance=4:</p> <pre><code>// Component scores\nsemantic = 0.90    // Text similarity to \"knowledge\"\nrecency = 0.98     // Recently accessed\nimportance = 0.80  // importance=4 normalized to 0-1 range (4/5)\n\n// Weighted combination\nweighted_score = (semantic \u00d7 0.60) + (recency \u00d7 0.25) + (importance \u00d7 0.15)\n               = (0.90 \u00d7 0.60) + (0.98 \u00d7 0.25) + (0.80 \u00d7 0.15)\n               = 0.540 + 0.245 + 0.120\n               = 0.905 \u2248 0.91\n</code></pre>"},{"location":"weighted-search.html#importance-normalization","title":"Importance Normalization","text":"Raw Importance Normalized (0-1) Weight in Score 5 (Critical) 1.00 15% \u2192 contributes 0.15 4 (High) 0.80 15% \u2192 contributes 0.12 3 (Medium) 0.60 15% \u2192 contributes 0.09 2 (Low) 0.40 15% \u2192 contributes 0.06 1 (Minimal) 0.20 15% \u2192 contributes 0.03 <p>Note: Importance is a boosting factor, not a filter. Low-importance items can still rank highly if they have strong semantic and recency scores.</p>"},{"location":"weighted-search.html#best-practices","title":"Best Practices","text":""},{"location":"weighted-search.html#1-use-weighted-search-for-discovery","title":"1. Use Weighted Search for Discovery","text":"<pre><code># Find important concepts about a topic\nbun run src/skills/tools/knowledge-cli.ts search_nodes \"architecture\" 10 --weighted\n</code></pre>"},{"location":"weighted-search.html#2-use-standard-search-for-exact-matches","title":"2. Use Standard Search for Exact Matches","text":"<pre><code># Find exact text matches (code, specific terms)\nbun run src/skills/tools/knowledge-cli.ts search_nodes \"graphiti_api_cost\" 10\n</code></pre>"},{"location":"weighted-search.html#3-combine-with-temporal-filters","title":"3. Combine with Temporal Filters","text":"<pre><code># Important knowledge from this week\nbun run src/skills/tools/knowledge-cli.ts search_nodes \"decisions\" 10 --weighted --since 7d\n\n# Recent high-priority items\nbun run src/skills/tools/knowledge-cli.ts search_nodes \"bugs\" 10 --weighted --since today\n</code></pre>"},{"location":"weighted-search.html#4-use-raw-for-analysis","title":"4. Use --raw for Analysis","text":"<pre><code># Export weighted scores for external analysis\nbun run src/skills/tools/knowledge-cli.ts search_nodes \"project\" 20 --weighted --raw &gt; results.json\n</code></pre>"},{"location":"weighted-search.html#faq","title":"FAQ","text":"<p>Q: Does weighted search always return different results?</p> <p>A: Not always. If all items have similar importance and recency, rankings stay similar. Weighted search primarily affects results when: - Importance levels vary (1-5 range) - Some items were accessed recently while others weren't - Semantic scores are close (the 15% importance weight can tip the balance)</p> <p>Q: Should I always use weighted search?</p> <p>A: Use it as a default for knowledge discovery. The 60% semantic weight ensures text relevance is still the primary factor. Only use standard search when you need pure semantic matching.</p> <p>Q: How is importance determined?</p> <p>A: Importance is classified when knowledge is added: - Automatic classification via LLM when adding episodes - Manual classification via <code>classify_memory</code> command - Based on content type, user preferences, and context</p> <p>Q: Can I see why a specific item ranked where it did?</p> <p>A: Yes! Use <code>--weighted --raw</code> to see the full score breakdown: </p><pre><code>bun run src/skills/tools/knowledge-cli.ts search_nodes \"query\" 5 --weighted --raw\n</code></pre><p></p> <p>Q: What's the difference between <code>importance</code> and <code>stability</code>?</p> <p>A: - Importance (1-5): How significant or valuable this knowledge is - Stability (1-5): How permanent or unchanging this knowledge is - Both factors are used by the memory decay system but only importance affects search ranking</p>"},{"location":"weighted-search.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Memory Decay System - How importance and stability are calculated</li> <li>CLI Reference - Complete command documentation</li> <li>Configuration - Setting up the knowledge system</li> </ul>"},{"location":"concepts/architecture.html","title":"Architecture","text":""},{"location":"concepts/architecture.html#architecture","title":"Architecture","text":"<p>The Knowledge System solves the problem of amnesiac AI through automatic knowledge graph construction. Instead of requiring manual note-taking, it extracts and structures knowledge as a natural byproduct of conversation.</p>"},{"location":"concepts/architecture.html#core-architecture","title":"Core Architecture","text":"<p>User conversation and document text flow through the Knowledge System Skill \u2192 Intent Routing \u2192 Graphiti MCP Server (with Memory Decay) \u2192 Graph Database Backend (Neo4j/FalkorDB) \u2192 Graph Storage (Nodes, Edges, Episodes, Indices)</p>"},{"location":"concepts/architecture.html#how-it-works","title":"How It Works","text":""},{"location":"concepts/architecture.html#1-natural-capture","title":"1. Natural Capture","text":"<p>Say \"remember that Podman volumes use host:container syntax\" and the system:</p> <ul> <li>Extracts entities: \"Podman\", \"volume mounting\"</li> <li>Identifies relationship: \"uses\", \"syntax rule\"</li> <li>Classifies importance and stability (Feature 009)</li> <li>Creates episode with full context</li> <li>Stores in graph with timestamp</li> </ul>"},{"location":"concepts/architecture.html#2-semantic-search","title":"2. Semantic Search","text":"<p>Ask \"what do I know about container orchestration?\" and the system:</p> <ul> <li>Searches vector embeddings for related concepts</li> <li>Applies weighted scoring: semantic (60%) + recency (25%) + importance (15%)</li> <li>Returns entities: \"Podman\", \"Kubernetes\", \"Docker Compose\"</li> <li>Shows relationships: \"alternatives to\", \"similar tools\"</li> <li>Displays episodes with full context</li> </ul>"},{"location":"concepts/architecture.html#3-relationship-discovery","title":"3. Relationship Discovery","text":"<p>Ask \"how are FalkorDB and Graphiti connected?\" and the system:</p> <ul> <li>Traverses graph edges between entities</li> <li>Returns: \"FalkorDB is the graph database backend for Graphiti\"</li> <li>Shows temporal context: \"learned on 2025-01-03\"</li> <li>Displays related entities and connections</li> </ul>"},{"location":"concepts/architecture.html#design-principles","title":"Design Principles","text":"<ol> <li>Zero Friction: Capture knowledge through natural conversation</li> <li>Automatic Extraction: LLM-powered entity and relationship detection</li> <li>Semantic Understanding: Vector embeddings enable concept-based search</li> <li>Temporal Tracking: Know when knowledge was added and how it evolves</li> <li>Graph-Based: Explicit relationships show how concepts connect</li> <li>Memory Prioritization: Automatic importance/stability classification with decay scoring (Feature 009)</li> <li>Complete: Every component included - MCP server, PAI skill, workflows</li> </ol>"},{"location":"concepts/architecture.html#multi-layered-architecture","title":"Multi-Layered Architecture","text":"<p>The system uses progressive abstraction across 6 layers:</p> <p>Layer 1 - User Intent: Natural language triggers (\"remember this\", \"what do I know\", \"how are X and Y\")</p> <p>Layer 2 - PAI Skill Routing: SKILL.md frontmatter \u2192 Intent Detection \u2192 Workflow routing</p> <p>Layer 3 - Workflow Execution: CaptureEpisode, SearchKnowledge, SearchFacts, GetRecent</p> <p>Layer 4 - MCP Server Integration: SSE Endpoint (localhost:8000/sse) \u2192 add_memory, search_memory_nodes, search_memory_facts, get_episodes</p> <p>Layer 5 - Graphiti Knowledge Graph: LLM Processing \u2192 Entity Extraction \u2192 Relationship Mapping \u2192 Memory Decay \u2192 Vector Embeddings</p> <p>Layer 6 - Graph Database: Nodes (Entities + Embeddings), Edges (Typed Relationships), Episodes (Full Context), Indices (Vector + Keyword)</p>"},{"location":"concepts/architecture.html#architectural-advantages","title":"Architectural Advantages","text":""},{"location":"concepts/architecture.html#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<p>Each layer has a single responsibility:</p> <ul> <li>Intent Layer: Natural language understanding</li> <li>Routing Layer: Direct user intent to workflow</li> <li>Workflow Layer: Operational procedures</li> <li>Server Layer: API abstraction</li> <li>Graph Layer: Knowledge operations</li> <li>Database Layer: Persistent storage</li> </ul> <p>This is FUNDAMENTALLY DIFFERENT from \"just storing notes\" because:</p> <ul> <li>Progressive abstraction (not everything in one layer)</li> <li>Explicit intent routing (not fuzzy keyword matching)</li> <li>Separation of operations (capture, search, retrieve distinct)</li> <li>Deterministic execution (workflows map intent to MCP calls)</li> </ul>"},{"location":"concepts/architecture.html#2-bidirectional-knowledge-flow","title":"2. Bidirectional Knowledge Flow","text":"<p>Capture path (green): User says \"Remember X\" \u2192 Capture Episode \u2192 Extract Entities \u2192 Create Relationships \u2192 Store in Graph</p> <p>Retrieval path (blue): Graph Query \u2192 Semantic Similarity \u2192 Vector Search \u2192 Search Results \u2192 User asks \"What about X\"</p> <p>Center: Knowledge compounds over time (dotted arrow)</p> <p>Every knowledge addition improves future retrieval. Every search result can trigger new knowledge capture.</p>"},{"location":"concepts/architecture.html#3-multi-dimensional-retrieval","title":"3. Multi-Dimensional Retrieval","text":"<p>Traditional search: Keyword matching in flat text Knowledge graph: Three retrieval dimensions</p> Dimension Mechanism Example Query Result Type Semantic Vector embeddings \"container orchestration\" Podman, Kubernetes, Docker Relational Graph traversal \"how are X and Y related\" \"X uses Y as backend\" Temporal Episode timestamps \"what did I learn about X\" Chronological episodes"},{"location":"concepts/architecture.html#4-automatic-entity-extraction","title":"4. Automatic Entity Extraction","text":"<p>LLM-powered extraction identifies:</p> <ul> <li>Named Entities: People, organizations, locations</li> <li>Abstract Concepts: Technologies, methodologies, patterns</li> <li>Procedural Knowledge: Workflows, SOPs, how-to guides</li> <li>Preferences: Choices, configurations, opinions</li> <li>Requirements: Features, needs, specifications</li> </ul> <p>This happens AUTOMATICALLY - no manual tagging required.</p>"},{"location":"concepts/architecture.html#5-temporal-context-tracking","title":"5. Temporal Context Tracking","text":"<p>Every episode includes:</p> <ul> <li>Timestamp: When knowledge was added</li> <li>Source: Conversation or document</li> <li>Entity State: How understanding evolved</li> <li>Relationship Creation: When connections were made</li> </ul> <p>Example: \"FalkorDB backend for Graphiti (learned 2025-01-03, updated 2025-01-05)\"</p>"},{"location":"concepts/architecture.html#6-lucene-query-sanitization","title":"6. Lucene Query Sanitization","text":"<p>The knowledge system includes automatic query sanitization to handle special characters in search terms, particularly important for CTI/OSINT data with hyphenated identifiers (e.g., <code>apt-28</code>, <code>threat-intel</code>).</p> <p>Full Documentation</p> <p>For detailed information about Lucene query sanitization, including the problem, solution, and sanitization functions, see <code>src/server/lib/lucene.ts</code>.</p>"},{"location":"concepts/architecture.html#component-stack","title":"Component Stack","text":"<p>The architecture includes every component needed for end-to-end operation:</p> <ul> <li>\u2705 MCP Server: <code>bun run server-cli start</code> starts Graphiti + Neo4j/FalkorDB</li> <li>\u2705 PAI Skill: <code>SKILL.md</code> with intent routing</li> <li>\u2705 Workflows: 7 complete operational procedures</li> <li>\u2705 Installation: Step-by-step in <code>tools/Install.md</code></li> <li>\u2705 Configuration: All settings in PAI config (<code>$PAI_DIR/.env</code>)</li> <li>\u2705 Documentation: README, INSTALL, VERIFY</li> <li>\u2705 Query Sanitization: Handles special characters automatically</li> </ul> <p>NOT: \"You need to set up your own vector database\" - FalkorDB is included NOT: \"Implement your own entity extraction\" - Graphiti handles it NOT: \"Configure your own embeddings\" - OpenAI integration built-in NOT: \"Handle special characters manually\" - Lucene sanitization built-in</p>"},{"location":"concepts/architecture.html#knowledge-architecture-innovation","title":"Knowledge Architecture Innovation","text":"<p>The key insight is that knowledge is relational, not transactional. Traditional note-taking treats each piece of information as an isolated transaction. The Madeinoz Knowledge System treats knowledge as a graph of interconnected entities with temporal context.</p> <p>This isn't just \"better search\" - it's a fundamentally different paradigm:</p> <ul> <li>Transaction: \"Note about Podman volumes\" (isolated, static)</li> <li>Relational: \"Podman \u2192 uses \u2192 volume mounting \u2192 syntax \u2192 host:container\" (connected, queryable, temporal)</li> </ul> <p>The graph structure allows queries impossible with flat notes:</p> <ul> <li>\"Show me all technologies related to container orchestration I learned about in the past month\"</li> <li>\"What debugging solutions led to architectural decisions?\"</li> <li>\"How do my preferences for dev tools relate to past troubleshooting sessions?\"</li> <li>\"Find all CTI indicators from group 'apt-28'\"</li> </ul> <p>This architecture makes your AI infrastructure genuinely intelligent, not just a better filing cabinet.</p>"},{"location":"concepts/architecture.html#problems-this-architecture-prevents","title":"Problems This Architecture Prevents","text":"Problem Traditional Approach Knowledge Graph Approach Keyword limits Must know exact terms Semantic similarity finds related concepts Siloed information Notes in separate files Graph connects everything Lost context No temporal tracking Every episode has timestamp No relationships Flat documents Explicit edges between entities Manual organization Tag and categorize yourself Automatic entity extraction Scattered knowledge Multiple tools Single unified graph Hyphenated identifiers Query syntax errors Automatic sanitization"},{"location":"concepts/hooks-integration.html","title":"Hooks & PAI Memory Integration","text":""},{"location":"concepts/hooks-integration.html#hooks-pai-memory-integration","title":"Hooks &amp; PAI Memory Integration","text":""},{"location":"concepts/hooks-integration.html#overview","title":"Overview","text":"<p>The madeinoz-knowledge-system seamlessly integrates with the PAI Memory System through automated session lifecycle hooks. This integration ensures your learning, research, and system improvements are automatically captured in your personal knowledge graph without manual intervention.</p> <p>Hooks are automated scripts that run at specific points in your Claude Code session lifecycle. They enable zero-effort knowledge capture by monitoring PAI Memory directories and syncing new or updated content to your knowledge graph.</p> <p>Zero-Effort Knowledge Capture</p> <p>Once configured, the hooks system automatically:</p> <ul> <li>Syncs PAI Memory files to your knowledge graph at session start</li> <li>Captures the most recent learnings immediately after session end</li> <li>Prevents duplicate entries using content hashing</li> <li>Handles errors gracefully with retry logic</li> </ul>"},{"location":"concepts/hooks-integration.html#component-stack","title":"Component Stack","text":"<pre><code>graph TB\n    subgraph \"Claude Code Session\"\n        CC[Claude Code CLI]\n        SS[SessionStart Event]\n    end\n\n    subgraph \"Hook Layer\"\n        H1[sync-memory-to-knowledge.ts]\n        CFG[sync-sources.json]\n        ALP[Anti-Loop Patterns]\n    end\n\n    subgraph \"PAI Memory System\"\n        M1[LEARNING/ALGORITHM/]\n        M2[LEARNING/SYSTEM/]\n        M3[RESEARCH/]\n    end\n\n    subgraph \"Processing\"\n        FP[Frontmatter Parser]\n        CH[Content Hasher&lt;br/&gt;SHA-256]\n        ST[Sync State Tracker]\n    end\n\n    subgraph \"MCP Server\"\n        MCP[Graphiti MCP Server&lt;br/&gt;localhost:8000/mcp/]\n        API[add_memory Tool&lt;br/&gt;HTTP POST + JSON-RPC 2.0]\n    end\n\n    subgraph \"Knowledge Graph\"\n        DB[(Neo4j / FalkorDB)]\n        ENT[Entities]\n        REL[Relationships]\n        EPI[Episodes]\n    end\n\n    CC --&gt; SS\n    SS --&gt; H1\n    CFG --&gt; H1\n    ALP --&gt; H1\n\n    H1 --&gt; M1\n    H1 --&gt; M2\n    H1 --&gt; M3\n\n    M1 --&gt; FP\n    M2 --&gt; FP\n    M3 --&gt; FP\n\n    FP --&gt; CH\n    FP --&gt; ALP\n    CH --&gt; ST\n    ST --&gt; MCP\n    MCP --&gt; API\n    API --&gt; DB\n\n    DB --&gt; ENT\n    DB --&gt; REL\n    DB --&gt; EPI\n\n    style CC fill:#4a90d9,color:#fff\n    style MCP fill:#28a745,color:#fff\n    style DB fill:#6f42c1,color:#fff\n    style CFG fill:#f0ad4e,color:#000\n    style ALP fill:#d9534f,color:#fff</code></pre>"},{"location":"concepts/hooks-integration.html#available-hooks","title":"Available Hooks","text":"<p>The system provides a single consolidated hook for memory synchronization:</p> Hook Trigger Point Purpose Sync Scope sync-memory-to-knowledge <code>SessionStart</code> Comprehensive sync of all configured PAI Memory files Configurable (LEARNING/ALGORITHM/, LEARNING/SYSTEM/, RESEARCH/)"},{"location":"concepts/hooks-integration.html#sync-memory-to-knowledge-sessionstart-hook","title":"sync-memory-to-knowledge (SessionStart Hook)","text":"<p>This hook performs comprehensive synchronization when you start a new Claude Code session. It scans all configured memory directories and syncs any new or modified files to your knowledge graph.</p> <p>Key Features:</p> <ul> <li>Configurable sync sources - Enable/disable each directory via environment variables or config file</li> <li>Anti-loop detection - Prevents knowledge query results from being re-synced (16 built-in patterns)</li> <li>Custom exclude patterns - Add your own patterns to skip specific content</li> <li>Full directory scanning across all enabled memory sources</li> <li>Batched processing for efficiency</li> <li>YAML frontmatter metadata extraction</li> <li>Content-based deduplication (SHA-256)</li> <li>Exponential backoff retry logic</li> <li>Dry-run mode for testing</li> <li>Status command for visibility</li> </ul>"},{"location":"concepts/hooks-integration.html#pai-memory-integration","title":"PAI Memory Integration","text":"<p>The hooks system integrates with the PAI Memory System v7.0, which organizes AI-assisted learning into structured markdown files with YAML frontmatter.</p>"},{"location":"concepts/hooks-integration.html#memory-directory-structure","title":"Memory Directory Structure","text":"<pre><code>~/.claude/MEMORY/\n\u251c\u2500\u2500 LEARNING/\n\u2502   \u251c\u2500\u2500 ALGORITHM/          # Improvements to The Algorithm\n\u2502   \u2514\u2500\u2500 SYSTEM/             # System-level learnings\n\u2514\u2500\u2500 RESEARCH/               # Research findings and analysis\n</code></pre>"},{"location":"concepts/hooks-integration.html#yaml-frontmatter-parsing","title":"YAML Frontmatter Parsing","text":"<p>Each PAI Memory file contains structured metadata in YAML frontmatter that enriches the knowledge graph:</p> <pre><code>---\ntitle: \"Advanced Error Handling Patterns\"\ntags: [\"error-handling\", \"resilience\", \"typescript\"]\nsource: \"Production debugging session\"\nconfidence: high\ndate: 2026-01-19\n---\n\n# Learning content here...\n</code></pre> <p>The hooks extract this metadata and pass it to the knowledge graph as structured context, enabling:</p> <ul> <li>Rich semantic search using title and tags</li> <li>Source attribution for provenance tracking</li> <li>Confidence filtering for quality assessment</li> <li>Temporal organization using date metadata</li> </ul>"},{"location":"concepts/hooks-integration.html#sync-sources","title":"Sync Sources","text":"<p>The hooks monitor three distinct PAI Memory directories, each serving a specific purpose:</p>"},{"location":"concepts/hooks-integration.html#1-learningalgorithm","title":"1. LEARNING/ALGORITHM/","text":"<p>Purpose: Captures improvements and refinements to \"The Algorithm\" - the core problem-solving methodology used by PAI agents.</p> <p>Content Types:</p> <ul> <li>New reasoning patterns discovered during sessions</li> <li>Refinements to existing problem-solving steps</li> <li>Meta-learnings about effective approaches</li> <li>Algorithm performance optimizations</li> </ul> <p>Why Sync This? Algorithm learnings represent your highest-value knowledge\u2014they're the distilled insights about how to think and solve problems effectively. By syncing these to your knowledge graph:</p> <ul> <li>Compounding Intelligence: Each algorithm improvement builds on previous ones; the graph connects related insights across sessions</li> <li>Pattern Discovery: Semantic search reveals when similar approaches solved different problems</li> <li>Methodology Evolution: Track how your problem-solving methodology improves over time through temporal queries</li> </ul> <p>Example File: <code>~/.claude/MEMORY/LEARNING/ALGORITHM/debugging-methodology-improvement.md</code></p>"},{"location":"concepts/hooks-integration.html#2-learningsystem","title":"2. LEARNING/SYSTEM/","text":"<p>Purpose: Documents system-level knowledge about tools, configurations, and infrastructure.</p> <p>Content Types:</p> <ul> <li>Tool usage patterns and best practices</li> <li>Configuration discoveries</li> <li>Environment-specific learnings</li> <li>Integration insights</li> </ul> <p>Why Sync This? System learnings prevent you from re-solving the same technical problems. This is the knowledge that saves hours of debugging and configuration headaches:</p> <ul> <li>Instant Recall: Ask \"how did I configure MkDocs?\" instead of re-reading documentation</li> <li>Cross-Tool Connections: The graph links related tools (e.g., Podman configurations connect to Docker knowledge)</li> <li>Environment Context: Learnings about your specific setup\u2014your machine, your tools, your workflows</li> <li>Troubleshooting History: When something breaks, search for past solutions to similar issues</li> </ul> <p>Example File: <code>~/.claude/MEMORY/LEARNING/SYSTEM/mkdocs-material-configuration.md</code></p>"},{"location":"concepts/hooks-integration.html#3-research","title":"3. RESEARCH/","text":"<p>Purpose: Stores research findings, competitive analysis, and external knowledge synthesis.</p> <p>Content Types:</p> <ul> <li>Technology research and evaluations</li> <li>Comparative analyses</li> <li>Industry trends and patterns</li> <li>External documentation summaries</li> </ul> <p>Why Sync This? Research represents your curated external knowledge\u2014information you've gathered, evaluated, and found valuable enough to preserve:</p> <ul> <li>Decision Support: When choosing tools or approaches, query past research on similar technologies</li> <li>Knowledge Synthesis: The graph connects research findings to your learnings, showing how external knowledge influenced your decisions</li> <li>Competitive Intelligence: Track how your understanding of the landscape evolves</li> <li>Reference Library: Build a searchable archive of evaluated technologies, patterns, and best practices from the broader ecosystem</li> </ul> <p>Example File: <code>~/.claude/MEMORY/RESEARCH/graphiti-vs-alternatives.md</code></p>"},{"location":"concepts/hooks-integration.html#configurable-sync","title":"Configurable Sync","text":"<p>The memory sync system is fully configurable through an external JSON configuration file and environment variables.</p>"},{"location":"concepts/hooks-integration.html#configuration-file","title":"Configuration File","text":"<p>Sync sources and custom patterns are defined in <code>sync-sources.json</code>:</p> Context Configuration File Location Installed (via <code>bun run install:system</code>) <code>~/.claude/config/sync-sources.json</code> Development (running from source) <code>{project}/config/sync-sources.json</code> <p>Example configuration:</p> <pre><code>{\n  \"version\": \"1.0\",\n  \"sources\": [\n    {\n      \"id\": \"LEARNING_ALGORITHM\",\n      \"path\": \"LEARNING/ALGORITHM\",\n      \"type\": \"LEARNING\",\n      \"description\": \"Task execution learnings\",\n      \"defaultEnabled\": true\n    },\n    {\n      \"id\": \"LEARNING_SYSTEM\",\n      \"path\": \"LEARNING/SYSTEM\",\n      \"type\": \"LEARNING\",\n      \"description\": \"PAI/tooling learnings\",\n      \"defaultEnabled\": true\n    },\n    {\n      \"id\": \"RESEARCH\",\n      \"path\": \"RESEARCH\",\n      \"type\": \"RESEARCH\",\n      \"description\": \"Agent research outputs\",\n      \"defaultEnabled\": true\n    }\n  ],\n  \"customExcludePatterns\": [\n    \"meeting notes\",\n    \"/^draft-/i\"\n  ]\n}\n</code></pre>"},{"location":"concepts/hooks-integration.html#environment-variables","title":"Environment Variables","text":"<p>Environment variables override the config file settings:</p> Variable Default Description <code>MADEINOZ_KNOWLEDGE_SYNC_LEARNING_ALGORITHM</code> <code>true</code> Enable sync for LEARNING/ALGORITHM files <code>MADEINOZ_KNOWLEDGE_SYNC_LEARNING_SYSTEM</code> <code>true</code> Enable sync for LEARNING/SYSTEM files <code>MADEINOZ_KNOWLEDGE_SYNC_RESEARCH</code> <code>true</code> Enable sync for RESEARCH files <code>MADEINOZ_KNOWLEDGE_SYNC_EXCLUDE_PATTERNS</code> - Comma-separated custom exclude patterns (overrides config file) <code>MADEINOZ_KNOWLEDGE_SYNC_MAX_FILES</code> <code>50</code> Max files per sync run (1-1000) <code>MADEINOZ_KNOWLEDGE_SYNC_VERBOSE</code> <code>false</code> Enable verbose logging <p>Example: Disable RESEARCH sync:</p> <pre><code>export MADEINOZ_KNOWLEDGE_SYNC_RESEARCH=false\n</code></pre>"},{"location":"concepts/hooks-integration.html#anti-loop-detection","title":"Anti-Loop Detection","text":"<p>The system automatically excludes knowledge-derived content from being re-synced to prevent feedback loops. This is critical for preventing infinite sync cycles where knowledge query results get synced back to the knowledge graph.</p> <p>16 Built-in Anti-Loop Patterns:</p> Category Pattern Examples What It Detects MCP Tools <code>mcp__madeinoz-knowledge__</code> Tool invocation markers Query Phrases <code>what do I know about</code>, <code>search my knowledge</code> Natural language queries Formatted Output <code>Knowledge Found:</code>, <code>Key Entities:</code> Structured search results Search Results <code>Related Facts:</code>, <code>Source Episodes:</code> Query response sections <p>When a file matches any anti-loop pattern, it is skipped with a logged reason code.</p>"},{"location":"concepts/hooks-integration.html#custom-exclude-patterns","title":"Custom Exclude Patterns","text":"<p>You can add your own patterns to exclude specific content from sync. Patterns can be:</p> <p>1. Substring patterns (case-insensitive match anywhere in content):</p> <pre><code>\"customExcludePatterns\": [\n  \"meeting notes\",\n  \"TODO:\",\n  \"DRAFT\"\n]\n</code></pre> <p>2. Regex patterns (surrounded by <code>/</code>, optional flags):</p> <pre><code>\"customExcludePatterns\": [\n  \"/^draft-/i\",\n  \"/\\\\[WIP\\\\]/\",\n  \"/meeting-\\\\d{4}-\\\\d{2}-\\\\d{2}/\"\n]\n</code></pre> Pattern Type Syntax Example Matches Substring <code>\"text\"</code> <code>\"meeting notes\"</code> Any file containing \"meeting notes\" Regex (case-insensitive) <code>\"/pattern/i\"</code> <code>\"/^draft-/i\"</code> Files starting with \"draft-\" Regex (case-sensitive) <code>\"/pattern/\"</code> <code>\"/TODO:/\"</code> Files containing exactly \"TODO:\" <p>Precedence:</p> <ol> <li>Environment variable <code>MADEINOZ_KNOWLEDGE_SYNC_EXCLUDE_PATTERNS</code> (if set, overrides config file)</li> <li>Config file <code>customExcludePatterns</code> array</li> <li>Built-in anti-loop patterns (always active, cannot be disabled)</li> </ol>"},{"location":"concepts/hooks-integration.html#cli-commands","title":"CLI Commands","text":"<pre><code># Show current sync status and configuration\nbun run src/hooks/sync-memory-to-knowledge.ts --status\n\n# Run sync manually\nbun run src/hooks/sync-memory-to-knowledge.ts\n\n# Sync all files (not just recent)\nbun run src/hooks/sync-memory-to-knowledge.ts --all\n\n# Dry run with verbose output\nbun run src/hooks/sync-memory-to-knowledge.ts --dry-run --verbose\n</code></pre>"},{"location":"concepts/hooks-integration.html#how-it-works","title":"How It Works","text":""},{"location":"concepts/hooks-integration.html#sessionstart-sync-flow","title":"SessionStart Sync Flow","text":"<pre><code>graph TD\n    A[Session Starts] --&gt; B[Load Sync State]\n    B --&gt; C{Sync State Exists?}\n    C --&gt;|No| D[Initialize Empty State]\n    C --&gt;|Yes| E[Load Previous Hashes]\n    D --&gt; F[Scan Memory Directories]\n    E --&gt; F\n    F --&gt; G[For Each File]\n    G --&gt; H[Parse YAML Frontmatter]\n    H --&gt; I[Calculate Content Hash]\n    I --&gt; J{Hash Changed?}\n    J --&gt;|No| G\n    J --&gt;|Yes| K[Add to Batch]\n    K --&gt; L{Batch Full?}\n    L --&gt;|Yes| M[Sync Batch to Knowledge Graph]\n    L --&gt;|No| G\n    M --&gt; N[Update Sync State]\n    N --&gt; G\n    G --&gt; O[Process Remaining Files]\n    O --&gt; P[Save Updated Sync State]\n    P --&gt; Q[Session Ready]</code></pre>"},{"location":"concepts/hooks-integration.html#step-by-step-process","title":"Step-by-Step Process","text":"<ol> <li>Trigger Detection - Hook activates at SessionStart lifecycle point</li> <li>Configuration Loading - Reads <code>sync-sources.json</code> for enabled sources and custom patterns</li> <li>State Loading - Reads <code>~/.claude/.madeinoz-knowledge-sync-state.json</code> for previous sync history</li> <li>File Discovery - Scans enabled memory directories for <code>.md</code> files</li> <li>Anti-Loop Check - Skips files matching built-in or custom exclude patterns</li> <li>Content Hashing - Calculates SHA-256 hash of each file's content</li> <li>Change Detection - Compares current hash against stored state</li> <li>Frontmatter Parsing - Extracts YAML metadata (title, tags, source, etc.)</li> <li>Batch Processing - Groups files into batches (default: 10 files)</li> <li>Knowledge Graph Sync - Calls <code>add_memory</code> tool with content and metadata</li> <li>Retry Logic - Retries failed syncs with exponential backoff (1s, 2s, 4s delays)</li> <li>State Persistence - Updates sync state file with new hashes and timestamps</li> </ol> <p>Anti-Loop Protection</p> <p>The hook automatically detects and skips files containing knowledge query results. This prevents infinite sync cycles where searching the knowledge graph produces output that gets synced back to the graph.</p>"},{"location":"concepts/hooks-integration.html#deduplication","title":"Deduplication","text":"<p>The hooks system uses content-based deduplication to prevent duplicate entries in your knowledge graph, even if you manually re-sync or run multiple sessions.</p>"},{"location":"concepts/hooks-integration.html#how-content-hashing-works","title":"How Content Hashing Works","text":"<ol> <li>Hash Calculation - SHA-256 hash computed from full file content (including frontmatter)</li> <li>State Storage - Hash stored in <code>~/.claude/.madeinoz-knowledge-sync-state.json</code> with file path</li> <li>Change Detection - On subsequent syncs, current hash compared to stored hash</li> <li>Sync Decision - Only files with changed hashes are synced</li> </ol>"},{"location":"concepts/hooks-integration.html#sync-state-file-format","title":"Sync State File Format","text":"<pre><code>{\n  \"lastSync\": \"2026-01-19T10:30:00.000Z\",\n  \"files\": {\n    \"/Users/seaton/.claude/MEMORY/LEARNING/ALGORITHM/debugging-methodology.md\": {\n      \"hash\": \"a1b2c3d4e5f6...\",\n      \"lastModified\": \"2026-01-19T10:25:00.000Z\",\n      \"syncedAt\": \"2026-01-19T10:30:00.000Z\"\n    },\n    \"/Users/seaton/.claude/MEMORY/LEARNING/SYSTEM/mkdocs-config.md\": {\n      \"hash\": \"f6e5d4c3b2a1...\",\n      \"lastModified\": \"2026-01-18T15:20:00.000Z\",\n      \"syncedAt\": \"2026-01-19T10:30:00.000Z\"\n    }\n  }\n}\n</code></pre>"},{"location":"concepts/hooks-integration.html#benefits","title":"Benefits","text":"<ul> <li>No Duplicate Entries - Each unique content version synced exactly once</li> <li>Efficient Syncing - Only modified files processed</li> <li>Safe Re-runs - Can manually trigger hooks without fear of duplication</li> <li>Edit Tracking - Content updates create new knowledge graph entries with fresh context</li> </ul> <p>Hash Stability</p> <p>The hash is calculated from the entire file content, including frontmatter. Even minor edits (like fixing typos) will trigger a re-sync. This is intentional - it ensures your knowledge graph always reflects the latest version of your learnings.</p>"},{"location":"concepts/hooks-integration.html#mcp-protocol-details","title":"MCP Protocol Details","text":"<p>The hooks use a dual protocol architecture that automatically adapts to your database backend type. This ensures compatibility with both Neo4j (default) and FalkorDB backends.</p>"},{"location":"concepts/hooks-integration.html#protocol-selection","title":"Protocol Selection","text":"<p>The <code>MADEINOZ_KNOWLEDGE_DB</code> environment variable determines which MCP protocol the hooks use:</p> Database Type Protocol Endpoint Session Management <code>neo4j</code> (default) HTTP POST + JSON-RPC 2.0 <code>/mcp/</code> <code>Mcp-Session-Id</code> header <code>falkorodb</code> SSE GET + session messages <code>/sse</code> Session endpoint from SSE handshake"},{"location":"concepts/hooks-integration.html#neo4j-protocol-http-post","title":"Neo4j Protocol (HTTP POST)","text":"<p>Flow: 1. Initialize session: POST to <code>/mcp/</code> with <code>initialize</code> method 2. Extract <code>Mcp-Session-Id</code> from response headers 3. Call tools: POST to <code>/mcp/</code> with <code>tools/call</code> method, including <code>Mcp-Session-Id</code> header 4. Parse response body as SSE format (extract <code>data:</code> lines containing JSON)</p> <p>Example Request: </p><pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"add_memory\",\n    \"arguments\": {\n      \"name\": \"LEARNING: My Learning\",\n      \"episode_body\": \"Content here...\",\n      \"group_id\": \"learning\"\n    }\n  }\n}\n</code></pre><p></p>"},{"location":"concepts/hooks-integration.html#falkordb-protocol-sse-get","title":"FalkorDB Protocol (SSE GET)","text":"<p>Flow: 1. Connect to <code>/sse</code> endpoint 2. Receive <code>endpoint</code> event with session-specific messages URL 3. Send initialize request to messages URL 4. Send tool calls via POST to messages URL 5. Receive responses via SSE or as JSON</p>"},{"location":"concepts/hooks-integration.html#query-sanitization","title":"Query Sanitization","text":"<p>The hooks automatically apply conditional query sanitization based on database type:</p> <p>FalkorDB (Lucene/RediSearch): - Special characters are escaped: <code>+ - &amp;&amp; || ! ( ) { } [ ] ^ \" ~ * ? : \\ /</code> - Critical for CTI/OSINT data with hyphenated identifiers (e.g., <code>apt-28</code>, <code>CVE-2024-1234</code>) - Applied automatically by the Docker container at runtime</p> <p>Neo4j (Cypher): - No escaping needed - native Cypher queries handle special characters naturally - Query parameters passed through as-is</p> <p>Protocol Mismatch Prevention</p> <p>The hooks automatically detect your database type and use the correct protocol. If you switch backends, simply update the <code>MADEINOZ_KNOWLEDGE_DB</code> environment variable and restart the MCP server.</p>"},{"location":"concepts/hooks-integration.html#configuration","title":"Configuration","text":""},{"location":"concepts/hooks-integration.html#environment-variables_1","title":"Environment Variables","text":"<p>The hooks system respects the same configuration as the main knowledge graph server:</p> Variable Purpose Default <code>MADEINOZ_KNOWLEDGE_MCP_URL</code> Graphiti MCP server base URL <code>http://localhost:8000</code> <code>MADEINOZ_KNOWLEDGE_DB</code> Database backend type (determines protocol) <code>neo4j</code> <code>MADEINOZ_KNOWLEDGE_TIMEOUT</code> Request timeout in milliseconds <code>15000</code> <code>MADEINOZ_KNOWLEDGE_RETRIES</code> Maximum retry attempts for transient failures <code>3</code> <p>Database Protocol Selection</p> <p>The <code>MADEINOZ_KNOWLEDGE_DB</code> environment variable determines which MCP protocol the hooks use:</p> <ul> <li>neo4j (default): HTTP POST to <code>/mcp/</code> endpoint with JSON-RPC 2.0 protocol</li> <li>falkorodb: SSE GET to <code>/sse</code> endpoint with session-based messaging</li> </ul> <p>This automatic protocol detection ensures the hooks work correctly with both database backends.</p>"},{"location":"concepts/hooks-integration.html#hook-specific-settings","title":"Hook-Specific Settings","text":"<p>These settings are configured in the hook source files:</p> <p>sync-memory-to-knowledge.ts:</p> <pre><code>// Batch size for processing files\nconst BATCH_SIZE = 10;\n\n// Memory directories to scan\nconst MEMORY_DIRS = [\n  `${process.env.HOME}/.claude/MEMORY/LEARNING/ALGORITHM`,\n  `${process.env.HOME}/.claude/MEMORY/LEARNING/SYSTEM`,\n  `${process.env.HOME}/.claude/MEMORY/RESEARCH`,\n];\n\n// Retry configuration\nconst MAX_RETRIES = 3;\nconst RETRY_DELAYS = [1000, 2000, 4000]; // ms\n</code></pre>"},{"location":"concepts/hooks-integration.html#customizing-sync-behavior","title":"Customizing Sync Behavior","text":"<p>Recommended: Use the config file at <code>~/.claude/config/sync-sources.json</code> (installed) or <code>config/sync-sources.json</code> (development).</p> <p>For advanced customization, edit the hook source directly:</p> <pre><code># Hook source\nsrc/hooks/sync-memory-to-knowledge.ts\n\n# Configuration loader\nsrc/hooks/lib/sync-config.ts\n\n# Anti-loop patterns\nsrc/hooks/lib/anti-loop-patterns.ts\n</code></pre> <p>After modifying TypeScript sources, rebuild and reinstall:</p> <pre><code>bun run build\nbun run install:system\n</code></pre>"},{"location":"concepts/hooks-integration.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/hooks-integration.html#hooks-not-running","title":"Hooks Not Running","text":"<p>Symptom: PAI Memory files aren't being synced to knowledge graph.</p> <p>Diagnosis:</p> <ol> <li> <p>Check if hooks are installed in PAI:    </p><pre><code>ls ~/.claude/hooks/\n# Should show: sync-memory-to-knowledge.ts and lib/ directory\n</code></pre><p></p> </li> <li> <p>Check if config file exists:    </p><pre><code>ls ~/.claude/config/sync-sources.json\n</code></pre><p></p> </li> <li> <p>Verify hook registration in PAI settings:    </p><pre><code>cat ~/.claude/settings.json | grep -A 5 hooks\n</code></pre><p></p> </li> <li> <p>Check for hook errors in Claude Code output during session start</p> </li> </ol> <p>Solutions:</p> <ul> <li>Re-run installation: <code>bun run install:system</code></li> <li>Check file permissions: <code>chmod +x ~/.claude/hooks/*.js</code></li> <li>Verify MCP server is running: <code>bun run server-cli status</code></li> </ul>"},{"location":"concepts/hooks-integration.html#duplicate-entries-in-knowledge-graph","title":"Duplicate Entries in Knowledge Graph","text":"<p>Symptom: Same content appears multiple times in search results.</p> <p>Diagnosis:</p> <p>Check sync state file integrity: </p><pre><code>cat ~/.claude/.madeinoz-knowledge-sync-state.json\n</code></pre><p></p> <p>Solutions:</p> <ul> <li>Corrupted state file: Delete and let it regenerate:   <pre><code>rm ~/.claude/.madeinoz-knowledge-sync-state.json\n</code></pre></li> <li>Manual clearing: Use <code>clear_graph</code> tool in Claude Code</li> <li>Verify content hashing is working by checking state file after sync</li> </ul>"},{"location":"concepts/hooks-integration.html#memory-files-not-found","title":"Memory Files Not Found","text":"<p>Symptom: Hook logs show \"No files found\" or \"Directory not found\".</p> <p>Diagnosis:</p> <ol> <li> <p>Verify PAI Memory directory exists:    </p><pre><code>ls -la ~/.claude/MEMORY/\n</code></pre><p></p> </li> <li> <p>Check directory paths in hook configuration match your setup</p> </li> </ol> <p>Solutions:</p> <ul> <li>Create missing directories:   <pre><code>mkdir -p ~/.claude/MEMORY/LEARNING/ALGORITHM\nmkdir -p ~/.claude/MEMORY/LEARNING/SYSTEM\nmkdir -p ~/.claude/MEMORY/RESEARCH\n</code></pre></li> <li>Customize <code>MEMORY_DIRS</code> in hook source if using non-standard paths</li> </ul>"},{"location":"concepts/hooks-integration.html#sync-failures-with-retry-errors","title":"Sync Failures with Retry Errors","text":"<p>Symptom: Hook logs show repeated retry attempts that fail.</p> <p>Diagnosis:</p> <ol> <li> <p>Check MCP server status:    </p><pre><code>bun run server-cli status\n</code></pre><p></p> </li> <li> <p>Verify server logs for errors:    </p><pre><code>bun run server-cli logs\n</code></pre><p></p> </li> <li> <p>Test connection manually:    </p><pre><code>curl http://localhost:8000/health\n</code></pre><p></p> </li> </ol> <p>Solutions:</p> <ul> <li>Server not running: <code>bun run server-cli start</code></li> <li>Port conflict: Change <code>MADEINOZ_KNOWLEDGE_PORT</code> in <code>.env</code></li> <li>Increase timeout: Set <code>MADEINOZ_KNOWLEDGE_TIMEOUT=60000</code> in <code>.env</code></li> <li>Check firewall/network settings</li> </ul>"},{"location":"concepts/hooks-integration.html#frontmatter-parsing-errors","title":"Frontmatter Parsing Errors","text":"<p>Symptom: Files sync but metadata is missing or incorrect.</p> <p>Diagnosis:</p> <p>Check YAML frontmatter format in your memory files: </p><pre><code>head -n 10 ~/.claude/MEMORY/LEARNING/SYSTEM/example.md\n</code></pre><p></p> <p>Solutions:</p> <ul> <li>Ensure frontmatter is valid YAML enclosed in <code>---</code> delimiters</li> <li>Check for indentation errors (use spaces, not tabs)</li> <li>Validate YAML syntax: yamllint.com</li> </ul> <p>Example correct format: </p><pre><code>---\ntitle: \"My Learning\"\ntags: [\"tag1\", \"tag2\"]\nsource: \"Session work\"\n---\n\n# Content here...\n</code></pre><p></p>"},{"location":"concepts/hooks-integration.html#files-being-skipped-anti-loop","title":"Files Being Skipped (Anti-Loop)","text":"<p>Symptom: Files are being skipped with \"anti-loop\" reason in verbose output.</p> <p>Diagnosis:</p> <p>This is expected behavior. The hook automatically skips files containing knowledge query results to prevent feedback loops. Use <code>--verbose</code> to see which patterns matched:</p> <pre><code>bun run src/hooks/sync-memory-to-knowledge.ts --verbose\n</code></pre> <p>Solutions:</p> <ul> <li>This is protective behavior - files containing knowledge search results should not be re-synced</li> <li>If a file is incorrectly matched, review the built-in patterns in <code>src/hooks/lib/anti-loop-patterns.ts</code></li> <li>Use <code>--status</code> to see current configuration including custom exclude patterns</li> </ul> <p>Need More Help?</p> <p>For additional support:</p> <ul> <li>Check server diagnostics: <code>bun run diagnose</code></li> <li>Review server logs: <code>bun run logs</code></li> <li>Visit the GitHub repository</li> <li>Consult the Getting Started guide</li> </ul>"},{"location":"concepts/knowledge-graph.html","title":"Knowledge Graph Concepts","text":""},{"location":"concepts/knowledge-graph.html#key-concepts","title":"Key Concepts","text":"<p>This guide explains how the Madeinoz Knowledge System works under the hood. Understanding these concepts will help you use the system more effectively.</p>"},{"location":"concepts/knowledge-graph.html#the-knowledge-graph","title":"The Knowledge Graph","text":""},{"location":"concepts/knowledge-graph.html#what-is-a-graph","title":"What is a Graph?","text":"<p>Think of a traditional note-taking app like a filing cabinet with folders. You put notes in folders, and later you search for the folder you need.</p> <p>A knowledge graph is different. It's more like a mind map where:</p> <ul> <li>Nodes are the concepts (Podman, Docker, containers)</li> <li>Edges are the relationships (Podman is similar to Docker)</li> <li>Paths connect related concepts across multiple hops</li> </ul> <p>Instead of storing isolated notes, you're building a web of connected knowledge.</p>"},{"location":"concepts/knowledge-graph.html#why-this-matters","title":"Why This Matters","text":"<p>Traditional notes: </p><pre><code>Folder: Docker\n  Note: Docker uses a daemon\n\nFolder: Podman\n  Note: Podman is daemonless\n</code></pre><p></p> <p>You have to remember both notes exist to see the connection.</p> <p>Knowledge graph: </p><pre><code>Docker --&gt; [requires] --&gt; daemon\nPodman --&gt; [is alternative to] --&gt; Docker\nPodman --&gt; [property: daemonless]\n</code></pre><p></p> <p>Ask \"what's different between Docker and Podman?\" and the system can traverse these connections automatically.</p>"},{"location":"concepts/knowledge-graph.html#core-components","title":"Core Components","text":""},{"location":"concepts/knowledge-graph.html#1-episodes","title":"1. Episodes","text":"<p>An episode is like a diary entry in your knowledge system. Every time you say \"remember this,\" the system creates an episode containing:</p> <ul> <li>Name: A brief title</li> <li>Body: The full content you provided</li> <li>Timestamp: When this was captured</li> <li>Source: Where it came from (conversation, document, etc.)</li> </ul> <p>Example Episode: </p><pre><code>name: \"Podman Volume Syntax\"\nbody: \"Podman volume mounting uses host:container syntax.\n       Left side is host path, right side is container path.\"\ntimestamp: 2025-01-08T10:30:00Z\nsource: \"Technical learning\"\n</code></pre><p></p> <p>Episodes preserve the original context while entities and relationships are extracted from them.</p>"},{"location":"concepts/knowledge-graph.html#2-entities","title":"2. Entities","text":"<p>Entities are the \"things\" in your knowledge. The system automatically extracts these from your episodes using AI.</p> <p>Types of Entities:</p> Type Description Examples Person Individual people \"Alice\", \"Bob\", \"Dr. Smith\" Organization Companies, teams, groups \"OpenAI\", \"My Dev Team\", \"PAI Project\" Location Places, servers, repos \"production server\", \"GitHub\", \"Neo4j instance\" Concept Ideas, technologies \"microservices\", \"AI\", \"knowledge graph\" Procedure How-to guides, processes \"deployment process\", \"backup procedure\" Preference Your choices, opinions \"prefer Markdown\", \"use 2-space tabs\" Requirement Needs, specifications \"must support 1000 users\", \"needs auth\" Event Time-bound occurrences \"architecture meeting\", \"bug discovered\" Document Files, articles, books \"API documentation\", \"user manual\" <p>Memory-Derived Entity Types:</p> <p>When integrated with the PAI Memory System, additional entity types are synced:</p> Type Description Examples Learning Knowledge from learning sessions Solved problems, insights gained Research Findings from research Investigated topics, comparisons Decision Architectural and strategic choices Tech selections, design decisions Feature Feature implementations Completed features, improvements <p>These types allow you to filter searches specifically for learnings, research, etc.</p> <p>Automatic Extraction:</p> <p>When you say: </p><pre><code>\"Remember that Graphiti uses OpenAI for entity extraction and Neo4j for storage.\"\n</code></pre><p></p> <p>The system extracts: - Graphiti (Tool/Concept) - OpenAI (Organization) - entity extraction (Procedure) - Neo4j (Tool/Concept) - storage (Concept)</p>"},{"location":"concepts/knowledge-graph.html#3-facts-relationships","title":"3. Facts (Relationships)","text":"<p>Facts connect entities and give your knowledge structure. They're also called \"relationships\" or \"edges.\"</p> <p>Types of Relationships:</p> Type Description Example Causal X causes Y \"Podman is daemonless\" \u2192 causes \u2192 \"better security\" Dependency X requires Y \"Graphiti\" \u2192 requires \u2192 \"OpenAI API key\" Temporal X before Y \"Bug discovered\" \u2192 before \u2192 \"Bug fixed\" Comparison X vs Y \"Podman\" \u2192 alternative to \u2192 \"Docker\" Possession X has Y \"VS Code\" \u2192 has setting \u2192 \"2-space tabs\" Location X in Y \"Neo4j\" \u2192 runs in \u2192 \"container\" Purpose X for Y \"entity extraction\" \u2192 used for \u2192 \"knowledge graph\" <p>Example Facts:</p> <pre><code>Podman \u2192 [is alternative to] \u2192 Docker\nPodman \u2192 [property: daemonless] \u2192 (no daemon required)\nGraphiti \u2192 [uses] \u2192 Neo4j\nNeo4j \u2192 [is backend for] \u2192 Graphiti\n</code></pre>"},{"location":"concepts/knowledge-graph.html#4-groups","title":"4. Groups","text":"<p>Groups let you organize knowledge into separate namespaces, like having multiple notebooks.</p> <p>Default Group:</p> <p>Everything goes into the \"main\" group unless you specify otherwise.</p> <p>Use Cases: - work: Professional knowledge - personal: Life organization, preferences - research: Academic or exploratory learning - projects: Project-specific knowledge</p> <p>Isolation:</p> <p>Groups don't share entities - \"Docker\" in your work group is separate from \"Docker\" in your personal group.</p>"},{"location":"concepts/knowledge-graph.html#5-embeddings","title":"5. Embeddings","text":"<p>This is the \"magic\" that makes semantic search work.</p> <p>What They Are:</p> <p>When you add knowledge, the system converts your text into a vector (a list of numbers). Similar concepts have similar vectors.</p> <p>Example: </p><pre><code>\"container orchestration\" \u2192 [0.23, 0.45, 0.12, ...]\n\"container management\"    \u2192 [0.25, 0.43, 0.15, ...]\n\"baking cookies\"          \u2192 [0.89, 0.02, 0.76, ...]\n</code></pre><p></p> <p>The first two are similar (close vectors), the third is different (distant vector).</p> <p>Why This Matters: You can search for \"container tools\" and find results about \"Docker\" and \"Podman\" even if you never used the word \"tools\" when capturing that knowledge.</p>"},{"location":"concepts/knowledge-graph.html#how-knowledge-flows","title":"How Knowledge Flows","text":""},{"location":"concepts/knowledge-graph.html#capturing-knowledge","title":"Capturing Knowledge","text":"<pre><code>1. You say: \"Remember that Podman is faster than Docker for starting containers\"\n                    \u2193\n2. PAI Skill recognizes the intent (\"remember this\")\n                    \u2193\n3. Content sent to MCP Server\n                    \u2193\n4. Graphiti processes with LLM (GPT-4)\n                    \u2193\n5. Entities extracted:\n   - Podman (Tool)\n   - Docker (Tool)\n   - container startup (Procedure)\n                    \u2193\n6. Relationships identified:\n   - Podman \u2192 faster than \u2192 Docker\n   - Both \u2192 used for \u2192 container startup\n                    \u2193\n7. Embeddings created for semantic search\n                    \u2193\n8. Everything stored in Neo4j graph database\n</code></pre>"},{"location":"concepts/knowledge-graph.html#searching-knowledge","title":"Searching Knowledge","text":"<pre><code>1. You ask: \"What do I know about container tools?\"\n                    \u2193\n2. PAI Skill recognizes search intent\n                    \u2193\n3. Query sent to MCP Server\n                    \u2193\n4. Vector similarity search:\n   \"container tools\" embedding compared to all entities\n                    \u2193\n5. Top matches retrieved:\n   - Podman (similarity: 0.89)\n   - Docker (similarity: 0.87)\n   - Neo4j (similarity: 0.72)\n                    \u2193\n6. Related facts and episodes retrieved\n                    \u2193\n7. Results formatted and returned to you\n</code></pre>"},{"location":"concepts/knowledge-graph.html#finding-relationships","title":"Finding Relationships","text":"<pre><code>1. You ask: \"How are Podman and Docker related?\"\n                    \u2193\n2. PAI Skill recognizes relationship query\n                    \u2193\n3. Graph traversal starts at both entities\n                    \u2193\n4. Neo4j finds paths between them:\n   - Direct: Podman \u2192 [alternative to] \u2192 Docker\n   - Indirect: Podman \u2192 [uses] \u2192 containers \u2190 [uses] \u2190 Docker\n                    \u2193\n5. Temporal context added (when relationships were learned)\n                    \u2193\n6. Full relationship map returned\n</code></pre>"},{"location":"concepts/knowledge-graph.html#technical-architecture","title":"Technical Architecture","text":""},{"location":"concepts/knowledge-graph.html#component-stack","title":"Component Stack","text":"<pre><code>graph TB\n    subgraph User_Interface[User Interface]\n        USER[You - Natural Language Input]\n    end\n\n    subgraph Intent_Layer[Intent Layer]\n        SKILL[PAI Skill]\n        ROUTE[Intent Recognition]\n        FLOW[Conversation Flow Manager]\n    end\n\n    subgraph API_Layer[API Layer]\n        MCP[MCP Server]\n        HTTP[HTTP Endpoint]\n        TOOLS[Tool Definitions]\n        REQ[Request/Response Handler]\n    end\n\n    subgraph Processing_Layer[Processing Layer]\n        GRAPHITI[Graphiti]\n        EXTRACT[LLM Entity Extraction]\n        RELMAP[Relationship Mapping]\n        EMBED[Vector Embedding Creation]\n    end\n\n    subgraph Storage_Layer[Storage Layer]\n        NEO4J[(Neo4j)]\n        NODES[Entities as Nodes]\n        EDGES[Relationships as Edges]\n        VECTOR[Vector Search]\n        INDEX[Index Management]\n    end\n\n    USER --&gt; SKILL\n    SKILL --&gt; ROUTE\n    SKILL --&gt; FLOW\n    ROUTE --&gt; MCP\n    MCP --&gt; HTTP\n    MCP --&gt; TOOLS\n    MCP --&gt; REQ\n    REQ --&gt; GRAPHITI\n    GRAPHITI --&gt; EXTRACT\n    GRAPHITI --&gt; RELMAP\n    GRAPHITI --&gt; EMBED\n    EMBED --&gt; NEO4J\n    NEO4J --&gt; NODES\n    NEO4J --&gt; EDGES\n    NEO4J --&gt; VECTOR\n    NEO4J --&gt; INDEX\n\n    style USER fill:#4a90d9,color:#fff\n    style GRAPHITI fill:#28a745,color:#fff\n    style NEO4J fill:#6f42c1,color:#fff</code></pre>"},{"location":"concepts/knowledge-graph.html#data-flow","title":"Data Flow","text":"<p>On Capture: </p><pre><code>Text Input\n  \u2192 LLM Analysis (GPT-4)\n    \u2192 Entities Identified\n      \u2192 Relationships Mapped\n        \u2192 Embeddings Created\n          \u2192 Graph Updated\n            \u2192 Confirmation Returned\n</code></pre><p></p> <p>On Search: </p><pre><code>Search Query\n  \u2192 Embedding Created\n    \u2192 Vector Similarity Search\n      \u2192 Top Entities Retrieved\n        \u2192 Related Facts Fetched\n          \u2192 Episodes Included\n            \u2192 Results Formatted\n              \u2192 Response Returned\n</code></pre><p></p>"},{"location":"concepts/knowledge-graph.html#the-role-of-llms-language-models","title":"The Role of LLMs (Language Models)","text":"<p>The knowledge system uses LLMs at multiple stages. Understanding this helps you optimize costs and quality.</p>"},{"location":"concepts/knowledge-graph.html#llms-in-knowledge-capture","title":"LLMs in Knowledge Capture","text":"<p>When you say \"remember this,\" an LLM analyzes your text to extract structure:</p> <pre><code>graph TB\n    subgraph Input_Stage[Input]\n        INPUT[Your Natural Language Input]\n    end\n\n    subgraph LLM_Processing[LLM Processing]\n        LLM[GPT-4 / GPT-4o-mini]\n    end\n\n    subgraph Entity_Extraction[Entity Extraction]\n        E1[Podman - Tool]\n        E2[Docker - Tool]\n        E3[daemon process - Concept]\n        E4[container startup - Procedure]\n    end\n\n    subgraph Relationship_Mapping[Relationship Mapping]\n        R1[Podman faster than Docker]\n        R2[Podman does not require daemon]\n        R3[Docker requires daemon]\n        R4[Both used for container startup]\n    end\n\n    subgraph Classification_Stage[Classification]\n        C1[Entity Types]\n        C2[Confidence Scores]\n        C3[Temporal Markers]\n    end\n\n    INPUT --&gt; LLM\n    LLM --&gt; E1\n    LLM --&gt; E2\n    LLM --&gt; E3\n    LLM --&gt; E4\n    E1 --&gt; R1\n    E2 --&gt; R1\n    E1 --&gt; R2\n    E3 --&gt; R2\n    E2 --&gt; R3\n    E3 --&gt; R3\n    E1 --&gt; R4\n    E2 --&gt; R4\n    E4 --&gt; R4\n    R1 --&gt; C1\n    R2 --&gt; C2\n    R3 --&gt; C3\n\n    style INPUT fill:#4a90d9,color:#fff\n    style LLM fill:#28a745,color:#fff\n    style E1 fill:#17a2b8,color:#fff\n    style E2 fill:#17a2b8,color:#fff\n    style E3 fill:#ffc107,color:#000\n    style E4 fill:#fd7e14,color:#fff</code></pre> <p>What the LLM does:</p> <ol> <li>Identifies entities - Recognizes \"things\" in your text (people, tools, concepts)</li> <li>Classifies types - Determines if something is a Procedure, Preference, etc.</li> <li>Extracts relationships - Understands how entities connect</li> <li>Resolves references - Links \"it\" to what \"it\" refers to</li> <li>Handles ambiguity - Makes intelligent choices about unclear text</li> </ol> <p>Why this matters:</p> <ul> <li>Better LLM = better entity extraction</li> <li><code>gpt-4o</code> extracts more nuanced relationships than <code>gpt-4o-mini</code></li> <li>Cost tradeoff: ~$0.01/capture (gpt-4o-mini) vs ~$0.03/capture (gpt-4o)</li> </ul>"},{"location":"concepts/knowledge-graph.html#llms-in-embedding-generation","title":"LLMs in Embedding Generation","text":"<p>Embeddings convert text into vectors for semantic search:</p> <pre><code>Text: \"container orchestration\"\n                 \u2193\n      Embedding Model (text-embedding-3-small)\n                 \u2193\nVector: [0.023, 0.451, 0.122, -0.089, 0.334, ... 1536 dimensions]\n</code></pre> <p>How embeddings enable search: </p><pre><code>Query: \"Docker alternatives\"     \u2192  Vector A: [0.021, 0.448, ...]\nEntity: \"Podman\"                 \u2192  Vector B: [0.025, 0.452, ...]\nEntity: \"Kubernetes\"             \u2192  Vector C: [0.189, 0.201, ...]\nEntity: \"Neo4j\"                  \u2192  Vector D: [0.891, 0.023, ...]\n\nSimilarity scores:\n  Podman:     0.94  \u2190 Very similar (good match!)\n  Kubernetes: 0.72  \u2190 Somewhat related\n  Neo4j:      0.12  \u2190 Not related (different domain)\n</code></pre><p></p> <p>What the embedding model does:</p> <ol> <li>Converts text to vectors - Creates numeric representations</li> <li>Captures meaning - Similar concepts have similar vectors</li> <li>Enables fuzzy matching - Find \"Docker alternatives\" even if you never said \"alternatives\"</li> </ol>"},{"location":"concepts/knowledge-graph.html#llms-in-retrieval-search","title":"LLMs in Retrieval (Search)","text":"<p>When you search, LLMs help in two ways:</p> <p>1. Query Embedding: </p><pre><code>Your Query: \"What container tools do I know about?\"\n                              \u2193\n                   Embedding Model\n                              \u2193\n              Query Vector: [0.034, 0.445, ...]\n                              \u2193\n              Vector Similarity Search in Neo4j\n                              \u2193\n              Top Matches: Podman, Docker, containerd\n</code></pre><p></p> <p>2. Result Synthesis (optional):</p> <p>After retrieval, an LLM can synthesize results into a coherent answer.</p>"},{"location":"concepts/knowledge-graph.html#model-configuration","title":"Model Configuration","text":"<p>The system uses different models for different tasks:</p> Task Model Why Entity Extraction gpt-4o-mini (default) Good balance of cost and quality Embeddings text-embedding-3-small Fast, cheap, excellent for search High-Quality Extraction gpt-4o (optional) Better for complex knowledge <p>In your PAI config (<code>$PAI_DIR/.env</code> or <code>~/.claude/.env</code>): </p><pre><code># Entity extraction model\nMADEINOZ_KNOWLEDGE_MODEL_NAME=gpt-4o-mini\n\n# For better extraction (costs more):\n# MADEINOZ_KNOWLEDGE_MODEL_NAME=gpt-4o\n</code></pre><p></p>"},{"location":"concepts/knowledge-graph.html#cost-breakdown-by-llm-task","title":"Cost Breakdown by LLM Task","text":"Operation Model Used Approximate Cost Capture (entity extraction) gpt-4o-mini ~$0.01 per episode Capture (entity extraction) gpt-4o ~$0.03 per episode Embedding generation text-embedding-3-small ~$0.0001 per text Search query embedding text-embedding-3-small ~$0.0001 per search <p>Monthly estimates:</p> <ul> <li>Light use (50 captures, 200 searches): ~$0.50-1.00</li> <li>Moderate use (200 captures, 500 searches): ~$2.00-5.00</li> <li>Heavy use (500+ captures): ~$5.00-15.00</li> </ul>"},{"location":"concepts/knowledge-graph.html#quality-vs-cost-tradeoffs","title":"Quality vs Cost Tradeoffs","text":"<p>Use <code>gpt-4o-mini</code> (default) when: - Capturing straightforward knowledge - Cost is a concern - High volume of captures - Relationships are explicit in your text</p> <p>Consider <code>gpt-4o</code> when: - Capturing complex technical content - Implicit relationships need to be inferred - Entity types are nuanced - Quality matters more than cost</p> <p>Tips for better extraction with cheaper models: 1. Be explicit about relationships (\"X is faster than Y\") 2. Use clear terminology 3. Provide 50+ words of context 4. State entity types when possible (\"the tool Podman\")</p>"},{"location":"concepts/knowledge-graph.html#why-these-design-choices","title":"Why These Design Choices?","text":""},{"location":"concepts/knowledge-graph.html#llm-powered-extraction","title":"LLM-Powered Extraction","text":"<p>Why not manual tagging?</p> <p>Manual tagging requires you to: 1. Decide what entities exist 2. Tag them consistently 3. Define relationships 4. Maintain the structure</p> <p>LLM extraction does this automatically as you capture knowledge naturally.</p>"},{"location":"concepts/knowledge-graph.html#graph-database","title":"Graph Database","text":"<p>Why not SQL or document database?</p> <p>Relationship queries are the core use case: - \"How are X and Y related?\" - \"What's connected to Z?\" - \"Find paths between A and B\"</p> <p>Graph databases excel at traversing relationships. In SQL, this would require complex joins. In a document database, you'd have to manually maintain references.</p>"},{"location":"concepts/knowledge-graph.html#vector-embeddings","title":"Vector Embeddings","text":"<p>Why not just keyword search?</p> <p>Keyword search breaks on synonyms: - Search for \"container tools\" misses \"Docker runtime\" - Search for \"fast\" misses \"high performance\" - Search for \"fix\" misses \"solution\"</p> <p>Vector search understands meaning, not just words.</p>"},{"location":"concepts/knowledge-graph.html#episode-based-storage","title":"Episode-Based Storage","text":"<p>Why keep the original text?</p> <p>Entities and relationships are extracted interpretations. The original episode preserves: - Full context - Temporal markers - Your exact words - Nuances that entity extraction might miss</p> <p>You can always go back to the source.</p>"},{"location":"concepts/knowledge-graph.html#limitations-and-trade-offs","title":"Limitations and Trade-offs","text":""},{"location":"concepts/knowledge-graph.html#what-this-system-does-well","title":"What This System Does Well","text":"<ol> <li>Semantic search: Find knowledge by meaning, not keywords</li> <li>Relationship discovery: See how concepts connect</li> <li>Automatic organization: No manual tagging or filing</li> <li>Temporal tracking: Know when you learned things</li> <li>Context preservation: Episodes keep the full story</li> </ol>"},{"location":"concepts/knowledge-graph.html#what-this-system-doesnt-do","title":"What This System Doesn't Do","text":"<ol> <li>Real-time collaboration: Not designed for team use (yet)</li> <li>Document editing: Captures knowledge, doesn't edit documents</li> <li>Structured data analysis: Not a replacement for SQL queries</li> <li>Version control: Doesn't track changes to entities over time</li> <li>Access control: No per-entity permissions (yet)</li> </ol>"},{"location":"concepts/knowledge-graph.html#cost-considerations","title":"Cost Considerations","text":"<p>API Costs: - Entity extraction: ~$0.01 per episode (using gpt-4o-mini) - Embeddings: ~$0.0001 per episode - Searches: ~$0.0001 per search</p> <p>Typical monthly cost: $0.50-2.00 for personal use</p> <p>Ways to reduce costs: - Use gpt-4o-mini instead of gpt-4o - Reduce SEMAPHORE_LIMIT to avoid rate charges - Capture only valuable knowledge, not every thought</p>"},{"location":"concepts/knowledge-graph.html#performance-characteristics","title":"Performance Characteristics","text":"<p>Fast: - Searches (&lt;100ms after first embedding) - Single entity retrieval (&lt;50ms) - Recent episodes (&lt;100ms)</p> <p>Slower: - Initial capture (2-5 seconds for entity extraction) - Complex relationship traversal (1-2 seconds) - Bulk imports (depends on volume)</p>"},{"location":"concepts/knowledge-graph.html#quality-factors","title":"Quality Factors","text":"<p>Better extraction with: - Longer, detailed content (50+ words) - Clear relationships stated explicitly - Specific terminology - Structured information - gpt-4o vs gpt-4o-mini</p> <p>Worse extraction with: - Very short snippets (&lt;10 words) - Vague language - Implied relationships - Ambiguous references</p>"},{"location":"concepts/knowledge-graph.html#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"concepts/knowledge-graph.html#entity-deduplication","title":"Entity Deduplication","text":"<p>The system tries to avoid creating duplicate entities:</p> <p>If you capture: 1. \"Docker is a container runtime\" 2. \"Docker requires a daemon process\"</p> <p>It creates ONE \"Docker\" entity with both facts, not two separate entities.</p> <p>How it works: - Embeddings of entity names are compared - Similar entities are merged - Facts accumulate on the same entity</p>"},{"location":"concepts/knowledge-graph.html#temporal-context","title":"Temporal Context","text":"<p>Episodes have timestamps, allowing temporal queries: - \"What did I learn last week?\" - \"Recent knowledge about X\" - \"Show me captures from January\"</p> <p>Facts also track when relationships were established: - \"Learned on 2025-01-08: Podman is faster than Docker\"</p> <p>This helps you see how your understanding evolves.</p>"},{"location":"concepts/knowledge-graph.html#graph-traversal","title":"Graph Traversal","text":"<p>Finding connections uses graph algorithms:</p> <p>Direct connection: </p><pre><code>Podman \u2192 [alternative to] \u2192 Docker\n</code></pre><p></p> <p>Two-hop connection: </p><pre><code>Podman \u2192 [uses] \u2192 containers \u2190 [used by] \u2190 Kubernetes\n</code></pre><p></p> <p>Multi-path: </p><pre><code>Path 1: Graphiti \u2192 [uses] \u2192 Neo4j \u2192 [type] \u2192 graph database\nPath 2: Graphiti \u2192 [requires] \u2192 OpenAI API \u2192 [provides] \u2192 LLM\n</code></pre><p></p>"},{"location":"concepts/knowledge-graph.html#group-isolation","title":"Group Isolation","text":"<p>Groups are completely separate graphs:</p> <pre><code>Group: work\n  - Entities: 50\n  - Facts: 120\n  - Episodes: 30\n\nGroup: personal\n  - Entities: 80\n  - Facts: 200\n  - Episodes: 45\n</code></pre> <p>No cross-group queries (by design - keeps work and personal separate).</p>"},{"location":"concepts/knowledge-graph.html#next-steps","title":"Next Steps","text":"<p>Now that you understand the concepts:</p> <ul> <li>Return to the Usage Guide with deeper understanding</li> <li>Check Troubleshooting if you have issues</li> <li>Explore the technical README for implementation details</li> </ul>"},{"location":"getting-started/overview.html","title":"Getting Started Overview","text":""},{"location":"getting-started/overview.html#knowledge-system-user-guide","title":"Knowledge System - User Guide","text":"<p>Welcome to the Madeinoz Knowledge System! This guide will help you understand and use your personal knowledge management system.</p> <p>PAI Pack</p> <p>This is a PAI (Personal AI Infrastructure) Pack - a modular component that adds persistent memory capabilities to your AI infrastructure. PAI Packs are self-contained modules that can be installed into any PAI-compatible system.</p>"},{"location":"getting-started/overview.html#what-is-the-knowledge-system","title":"What is the Knowledge System?","text":"<p>Think of the Knowledge System as your AI's memory. Instead of forgetting what you discussed yesterday or last week, it remembers everything you tell it to remember. It's like having a smart notebook that:</p> <ul> <li>Automatically organizes information as you add it</li> <li>Finds connections between different topics</li> <li>Lets you search using everyday language</li> <li>Keeps track of when you learned things</li> <li>Never forgets what you taught it</li> </ul>"},{"location":"getting-started/overview.html#what-can-you-do-with-it","title":"What Can You Do With It?","text":""},{"location":"getting-started/overview.html#store-information","title":"Store Information","text":"<p>Just say \"remember this\" and the system captures what you're talking about:</p> <pre><code>You: \"Remember that I prefer using gpt-4o-mini for everyday tasks because it's faster and cheaper.\"\n</code></pre> <p>The system will automatically:</p> <ul> <li>Extract key concepts (gpt-4o-mini, preferences, cost optimization)</li> <li>Note relationships (preference, reason: speed and cost)</li> <li>Store it with today's date</li> </ul>"},{"location":"getting-started/overview.html#find-information","title":"Find Information","text":"<p>Ask questions in plain English:</p> <pre><code>You: \"What do I know about Podman?\"\n</code></pre> <p>The system searches your knowledge and shows you:</p> <ul> <li>Everything you've stored about Podman</li> <li>Related topics (Docker, containers, etc.)</li> <li>When you learned each piece of information</li> </ul>"},{"location":"getting-started/overview.html#discover-connections","title":"Discover Connections","text":"<p>See how different topics relate:</p> <pre><code>You: \"How are Graphiti and FalkorDB connected?\"\n</code></pre> <p>The system traces the relationships in your knowledge graph and explains how these concepts link together.</p>"},{"location":"getting-started/overview.html#quick-start","title":"Quick Start","text":""},{"location":"getting-started/overview.html#before-you-begin","title":"Before You Begin","text":"<p>You'll need:</p> <ol> <li>The Madeinoz Knowledge System installed (see installation guide)</li> <li>The MCP server running in the background</li> <li>An OpenAI API key (or similar service)</li> </ol>"},{"location":"getting-started/overview.html#your-first-knowledge-capture","title":"Your First Knowledge Capture","text":"<p>Try this simple example:</p> <pre><code>You: \"Remember that bun is faster than npm for installing packages.\"\n</code></pre> <p>The system responds with something like:</p> <pre><code>Knowledge Captured\n\nStored episode: Bun Performance Comparison\n\nEntities extracted:\n- Bun (Tool)\n- npm (Tool)\n- package installation (Procedure)\n\nRelationships identified:\n- Bun -&gt; faster than -&gt; npm\n- Bun -&gt; used for -&gt; package installation\n</code></pre>"},{"location":"getting-started/overview.html#your-first-search","title":"Your First Search","text":"<p>Now try finding what you just stored:</p> <pre><code>You: \"What do I know about bun?\"\n</code></pre> <p>The system shows you:</p> <pre><code>Knowledge Found: Bun\n\nBased on your knowledge graph:\n\nKey Entities:\n1. Bun (Tool)\n   - Fast JavaScript runtime and package manager\n   - Alternative to npm and Node.js\n   - Known for faster package installation\n\nRelationships:\n- Bun -&gt; faster than -&gt; npm\n- Bun -&gt; used for -&gt; package installation\n\nEpisodes:\n- \"Bun Performance Comparison\" (today)\n</code></pre>"},{"location":"getting-started/overview.html#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/overview.html#for-developers","title":"For Developers","text":"<p>Capture technical decisions:</p> <pre><code>\"Remember that we chose PostgreSQL over MongoDB because we need strong consistency and complex relationships.\"\n</code></pre> <p>Store configuration snippets:</p> <pre><code>\"Save this: my preferred VS Code settings are 2-space tabs, auto-save on focus change, and Dracula theme.\"\n</code></pre> <p>Document solutions to problems:</p> <pre><code>\"Remember: when Podman containers can't reach the network, check if the firewall is blocking the CNI plugins.\"\n</code></pre>"},{"location":"getting-started/overview.html#for-learning","title":"For Learning","text":"<p>Capture research findings:</p> <pre><code>\"Store this research: Graphiti uses LLMs to automatically extract entities from text, unlike traditional knowledge graphs that require manual annotation.\"\n</code></pre> <p>Track concept connections:</p> <pre><code>\"Remember that FalkorDB is a Redis module that adds graph database capabilities, which is why Graphiti can use it as a backend.\"\n</code></pre>"},{"location":"getting-started/overview.html#for-personal-organization","title":"For Personal Organization","text":"<p>Save preferences:</p> <pre><code>\"Remember that I prefer morning meetings between 9-11 AM and need at least 30 minutes between back-to-back calls.\"\n</code></pre> <p>Track decisions:</p> <pre><code>\"Store this decision: I'm going to use weekly reviews instead of daily standups for my solo projects.\"\n</code></pre>"},{"location":"getting-started/overview.html#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/overview.html#episodes","title":"Episodes","text":"<p>Every time you add knowledge, the system creates an \"episode.\" Think of episodes as diary entries - each one captures:</p> <ul> <li>What you said</li> <li>When you said it</li> <li>What entities and relationships were found</li> </ul>"},{"location":"getting-started/overview.html#entities","title":"Entities","text":"<p>These are the \"things\" in your knowledge - people, places, tools, concepts, preferences, procedures, etc. The system automatically identifies these as you add information.</p> <p>Common entity types:</p> <ul> <li>People: Names of individuals</li> <li>Organizations: Companies, teams, groups</li> <li>Locations: Places, servers, repositories</li> <li>Concepts: Ideas, technologies, methodologies</li> <li>Procedures: How-to guides, workflows</li> <li>Preferences: Your choices and opinions</li> <li>Requirements: Features, needs, specifications</li> </ul>"},{"location":"getting-started/overview.html#relationships","title":"Relationships","text":"<p>Relationships show how entities connect. For example:</p> <ul> <li>\"Bun is faster than npm\" creates a relationship</li> <li>\"PostgreSQL requires strong consistency\" creates another</li> <li>\"I prefer morning meetings\" connects you to a preference</li> </ul>"},{"location":"getting-started/overview.html#groups","title":"Groups","text":"<p>You can organize knowledge into separate groups (like different notebooks). By default, everything goes into the \"main\" group, but you can create separate groups for work, personal, research, etc.</p>"},{"location":"getting-started/overview.html#how-it-works-behind-the-scenes","title":"How It Works Behind the Scenes","text":"<p>When you say \"remember this,\" here's what happens:</p> <ol> <li> <p>Your words go to the system - The PAI skill recognizes you want to store knowledge</p> </li> <li> <p>Content is sent to the MCP server - This is the brain that processes your information</p> </li> <li> <p>An LLM extracts entities - Using AI (like GPT-4), the system identifies important concepts in what you said</p> </li> <li> <p>Relationships are mapped - The system figures out how these concepts relate to each other</p> </li> <li> <p>Embeddings are created - Your knowledge is converted into vector form so it can be searched semantically (by meaning, not just keywords)</p> </li> <li> <p>Everything is stored in FalkorDB - A graph database saves all the entities, relationships, and the original text</p> </li> </ol> <p>When you search, the system uses vector similarity to find relevant knowledge, even if you use different words than you originally used.</p>"},{"location":"getting-started/overview.html#next-steps","title":"Next Steps","text":"<p>Ready to dive deeper? Check out:</p> <ul> <li>Installation Guide - Set up the system step by step</li> <li>Usage Guide - Detailed examples and commands</li> <li>Concepts Guide - Deep dive into how the system works</li> <li>Troubleshooting - Fix common issues</li> </ul>"},{"location":"getting-started/overview.html#getting-help","title":"Getting Help","text":"<p>If something isn't working:</p> <ol> <li>Check if the MCP server is running: <code>bun run server-cli status</code></li> <li>Look at the logs: <code>bun run server-cli logs</code></li> <li>Read the troubleshooting guide</li> <li>Review the Architecture for technical details</li> </ol>"},{"location":"getting-started/overview.html#tips-for-success","title":"Tips for Success","text":"<ol> <li> <p>Be specific: Instead of \"remember Docker,\" say \"remember that Docker requires a daemon process, unlike Podman which is daemonless\"</p> </li> <li> <p>Add context: The more detail you provide, the better the entity extraction works</p> </li> <li> <p>Use it regularly: The more knowledge you add, the more useful the system becomes</p> </li> <li> <p>Review recent additions: Periodically check what you've stored with \"show me recent knowledge\"</p> </li> <li> <p>Don't worry about organization: The system automatically organizes information - you just focus on capturing it</p> </li> </ol>"},{"location":"getting-started/quick-reference.html","title":"Quick Reference Card","text":""},{"location":"getting-started/quick-reference.html#quick-reference-card","title":"Quick Reference Card","text":"<p>One-page reference for the Madeinoz Knowledge System.</p>"},{"location":"getting-started/quick-reference.html#natural-language-commands","title":"Natural Language Commands","text":""},{"location":"getting-started/quick-reference.html#capture-knowledge","title":"Capture Knowledge","text":"<pre><code>\"Remember that [your knowledge]\"\n\"Store this: [information]\"\n\"Add to my knowledge: [details]\"\n\"Save this: [content]\"\n</code></pre>"},{"location":"getting-started/quick-reference.html#search-knowledge","title":"Search Knowledge","text":"<pre><code>\"What do I know about [topic]?\"\n\"Search my knowledge for [subject]\"\n\"Find information about [concept]\"\n\"What have I learned about [theme]?\"\n</code></pre>"},{"location":"getting-started/quick-reference.html#filter-by-entity-type","title":"Filter by Entity Type","text":"<pre><code>\"Find my procedures about [topic]\"\n\"Search for learnings about [subject]\"\n\"Show research about [concept]\"\n\"What decisions have I made about [theme]?\"\n\"Find my preferences for [setting]\"\n</code></pre>"},{"location":"getting-started/quick-reference.html#find-connections","title":"Find Connections","text":"<pre><code>\"How are [X] and [Y] related?\"\n\"What's the connection between [A] and [B]?\"\n\"Show me relationships with [topic]\"\n</code></pre>"},{"location":"getting-started/quick-reference.html#review-recent","title":"Review Recent","text":"<pre><code>\"What did I learn recently?\"\n\"Show me recent knowledge\"\n\"Latest additions about [topic]\"\n</code></pre>"},{"location":"getting-started/quick-reference.html#system-status","title":"System Status","text":"<pre><code>\"Knowledge graph status\"\n\"Show me knowledge stats\"\n\"Is the system healthy?\"\n</code></pre>"},{"location":"getting-started/quick-reference.html#memory-health-lifecycle-feature-009","title":"Memory Health &amp; Lifecycle (Feature 009)","text":""},{"location":"getting-started/quick-reference.html#check-memory-health","title":"Check Memory Health","text":"<pre><code>\"Show me my memory health\"\n\"What's the state of my knowledge graph?\"\n\"How many memories do I have?\"\n</code></pre>"},{"location":"getting-started/quick-reference.html#check-maintenance-status","title":"Check Maintenance Status","text":"<pre><code># Check last maintenance run\ncurl http://localhost:8000/health | jq '.maintenance'\n\n# View memory counts by state\ncurl http://localhost:8000/health | jq '.memory_counts.by_state'\n</code></pre>"},{"location":"getting-started/quick-reference.html#manual-maintenance","title":"Manual Maintenance","text":"<pre><code># Trigger maintenance manually (if MCP tool exposed)\n# Note: Automatic maintenance runs every 24 hours by default\n</code></pre>"},{"location":"getting-started/quick-reference.html#view-lifecycle-breakdown","title":"View Lifecycle Breakdown","text":"<pre><code># Via health endpoint\ncurl http://localhost:8000/health | jq '.memory_counts.by_state'\n\n# Via Grafana Dashboard\n# http://localhost:3002/d/memory-decay-dashboard\n</code></pre> <p>Lifecycle States: - ACTIVE - Recently accessed, full relevance - DORMANT - Not accessed 30+ days - ARCHIVED - Not accessed 90+ days - EXPIRED - Marked for deletion - SOFT_DELETED - Deleted but recoverable (90 days)</p> <p>See Memory Decay &amp; Lifecycle Management for complete guide.</p>"},{"location":"getting-started/quick-reference.html#server-management","title":"Server Management","text":""},{"location":"getting-started/quick-reference.html#status","title":"Status","text":"<pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nbun run server-cli status\n</code></pre>"},{"location":"getting-started/quick-reference.html#start","title":"Start","text":"<pre><code>bun run server-cli start\n</code></pre>"},{"location":"getting-started/quick-reference.html#stop","title":"Stop","text":"<pre><code>bun run server-cli stop\n</code></pre>"},{"location":"getting-started/quick-reference.html#logs","title":"Logs","text":"<pre><code>bun run server-cli logs\n</code></pre>"},{"location":"getting-started/quick-reference.html#restart","title":"Restart","text":"<pre><code>bun run server-cli restart\n</code></pre>"},{"location":"getting-started/quick-reference.html#configuration-file","title":"Configuration File","text":"<p>Location: <code>$PAI_DIR/.env</code> (defaults to <code>~/.claude/.env</code>)</p> <p>Key settings:</p> <pre><code># Required: Your API key\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-your-key-here\n\n# Model selection (cost vs quality)\nMADEINOZ_KNOWLEDGE_MODEL_NAME=gpt-4o-mini\n\n# Concurrency (lower = fewer rate limits)\nMADEINOZ_KNOWLEDGE_SEMAPHORE_LIMIT=10\n\n# Knowledge graph group\nMADEINOZ_KNOWLEDGE_GROUP_ID=main\n</code></pre>"},{"location":"getting-started/quick-reference.html#entity-types","title":"Entity Types","text":"<p>The system automatically extracts:</p> <p>Core Types:</p> <ul> <li>Person - Individual people</li> <li>Organization - Companies, teams</li> <li>Location - Places, servers</li> <li>Concept - Ideas, technologies</li> <li>Procedure - How-to guides</li> <li>Preference - Your choices</li> <li>Requirement - Specifications</li> <li>Event - Occurrences</li> <li>Document - Files, articles</li> </ul> <p>Memory-Derived Types (from PAI Memory sync):</p> <ul> <li>Learning - Knowledge from learning sessions</li> <li>Research - Findings from research</li> <li>Decision - Architectural/strategic choices</li> <li>Feature - Feature implementations</li> </ul> <p>Use these types to filter searches: \"Find my procedures about X\"</p>"},{"location":"getting-started/quick-reference.html#knowledge-flow","title":"Knowledge Flow","text":"<pre><code>1. You say \"remember this\"\n       \u2193\n2. PAI Skill captures intent\n       \u2193\n3. MCP Server receives content\n       \u2193\n4. LLM extracts entities (gpt-4o-mini)\n       \u2193\n5. LLM maps relationships\n       \u2193\n6. Embedding model creates vectors (text-embedding-3-small)\n       \u2193\n7. Stored in Neo4j graph (default) or FalkorDB\n</code></pre>"},{"location":"getting-started/quick-reference.html#llm-roles","title":"LLM Roles","text":"Stage Model Purpose Capture gpt-4o-mini Entity extraction, relationship mapping Embeddings text-embedding-3-small Convert text to searchable vectors Search text-embedding-3-small Convert query to vector for matching <p>Cost per operation:</p> <ul> <li>Capture: ~$0.01 (gpt-4o-mini) or ~$0.03 (gpt-4o)</li> <li>Search: ~$0.0001</li> </ul>"},{"location":"getting-started/quick-reference.html#search-caching","title":"Search Caching","text":"<ul> <li>Search results cached for 5 minutes (speeds up repeated queries)</li> <li>Writes are never cached (always save to database)</li> <li>Cache clears automatically after TTL expires</li> </ul> <p>If new knowledge doesn't appear in search:</p> <ul> <li>Wait 5 minutes for cache refresh, or</li> <li>Ask a slightly different question</li> </ul>"},{"location":"getting-started/quick-reference.html#troubleshooting-checklist","title":"Troubleshooting Checklist","text":""},{"location":"getting-started/quick-reference.html#issue-cant-connect","title":"Issue: Can't connect","text":"<pre><code># Check if running\nbun run server-cli status\n\n# Start if needed\nbun run server-cli start\n\n# Check endpoint\ncurl http://localhost:8000/sse\n</code></pre>"},{"location":"getting-started/quick-reference.html#issue-poor-extraction","title":"Issue: Poor extraction","text":"<ul> <li>Add more detail to your captures</li> <li>Use explicit relationships</li> <li>Consider upgrading to gpt-4o model</li> <li>Provide 50+ words of context</li> </ul>"},{"location":"getting-started/quick-reference.html#issue-no-search-results","title":"Issue: No search results","text":"<ul> <li>Try broader search terms</li> <li>Check if knowledge was captured</li> <li>Verify you're in the right group</li> <li>Review recent additions</li> </ul>"},{"location":"getting-started/quick-reference.html#issue-rate-limits","title":"Issue: Rate limits","text":"<ul> <li>Reduce SEMAPHORE_LIMIT in config</li> <li>Use gpt-4o-mini instead of gpt-4o</li> <li>Check your API tier</li> </ul>"},{"location":"getting-started/quick-reference.html#best-practices","title":"Best Practices","text":"<ol> <li> <p>Be Specific</p> <ul> <li>Bad: \"Remember Docker\"</li> <li>Good: \"Remember that Docker requires a daemon process running as root\"</li> </ul> </li> <li> <p>Add Context</p> <ul> <li>Bad: \"Remember that config\"</li> <li>Good: \"Remember my VS Code config: 2-space tabs, auto-save enabled\"</li> </ul> </li> <li> <p>State Relationships</p> <ul> <li>Bad: \"Remember Podman and Docker\"</li> <li>Good: \"Remember that Podman is an alternative to Docker\"</li> </ul> </li> <li> <p>Review Regularly</p> <ul> <li>Weekly: \"What did I learn this week?\"</li> <li>Monthly: Review knowledge graph status</li> </ul> </li> <li> <p>Capture Immediately</p> <ul> <li>Don't wait to remember details</li> <li>Capture while context is fresh</li> </ul> </li> </ol>"},{"location":"getting-started/quick-reference.html#costs","title":"Costs","text":"<p>Typical monthly costs (gpt-4o-mini):</p> <ul> <li>Light use: $0.50-1.00</li> <li>Moderate use: $1.00-3.00</li> <li>Heavy use: $3.00-10.00</li> </ul> <p>Per operation:</p> <ul> <li>Capture: ~$0.01</li> <li>Search: ~$0.0001</li> <li>Embedding: ~$0.0001</li> </ul>"},{"location":"getting-started/quick-reference.html#urls","title":"URLs","text":"<ul> <li>MCP Server: http://localhost:8000/sse</li> <li>Neo4j Browser: http://localhost:7474 (default backend)</li> <li>FalkorDB UI: http://localhost:3000 (if using FalkorDB)</li> <li>OpenAI Usage: https://platform.openai.com/usage</li> </ul>"},{"location":"getting-started/quick-reference.html#file-locations","title":"File Locations","text":"<p>Configuration:</p> <ul> <li><code>$PAI_DIR/.env</code> (defaults to <code>~/.claude/.env</code>) - All configuration</li> </ul> <p>Installed Skill Directory:</p> <pre><code>~/.claude/skills/Knowledge/\n\u251c\u2500\u2500 SKILL.md                 # Skill definition with routing\n\u251c\u2500\u2500 config/.env.example      # Configuration template (reference only)\n\u251c\u2500\u2500 tools/                   # Server and CLI tools\n\u2502   \u251c\u2500\u2500 server-cli.ts       # Unified server CLI (start, stop, restart, status, logs)\n\u2502   \u251c\u2500\u2500 knowledge-cli.ts    # Knowledge CLI (add, search, status)\n\u2502   \u2514\u2500\u2500 install.ts          # Interactive installer\n\u251c\u2500\u2500 workflows/               # Workflow definitions\n\u2502   \u251c\u2500\u2500 CaptureEpisode.md   # Store knowledge\n\u2502   \u251c\u2500\u2500 SearchKnowledge.md  # Search entities\n\u2502   \u251c\u2500\u2500 SearchFacts.md      # Find relationships\n\u2502   \u251c\u2500\u2500 SearchByDate.md     # Temporal search\n\u2502   \u251c\u2500\u2500 GetRecent.md        # Recent additions\n\u2502   \u251c\u2500\u2500 GetStatus.md        # System health\n\u2502   \u251c\u2500\u2500 ClearGraph.md       # Clear knowledge\n\u2502   \u2514\u2500\u2500 BulkImport.md       # Bulk import\n\u251c\u2500\u2500 docker/                  # Container configuration\n\u2502   \u251c\u2500\u2500 docker-compose-*.yml\n\u2502   \u2514\u2500\u2500 podman-compose-*.yml\n\u2514\u2500\u2500 lib/                     # Shared libraries\n</code></pre>"},{"location":"getting-started/quick-reference.html#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"<p>When editing config:</p> <ul> <li><code>Ctrl+O</code> - Save file</li> <li><code>Enter</code> - Confirm filename</li> <li><code>Ctrl+X</code> - Exit editor</li> </ul>"},{"location":"getting-started/quick-reference.html#docker-vs-podman","title":"Docker vs Podman","text":"<p>The system works with both:</p> <pre><code># Check which you have\npodman --version\n# or\ndocker --version\n</code></pre> <p>Commands are the same, the system auto-detects which to use.</p>"},{"location":"getting-started/quick-reference.html#memory-integration","title":"Memory Integration","text":"<p>PAI Memory System syncs automatically:</p> <p>Auto-sync on session start:</p> <p>Learnings and research automatically sync from <code>~/.claude/MEMORY/</code> to knowledge graph.</p> <p>Manual sync:</p> <pre><code>bun run ~/.claude/hooks/sync-memory-to-knowledge.ts\n</code></pre> <p>Check what will sync:</p> <pre><code>bun run ~/.claude/hooks/sync-memory-to-knowledge.ts --dry-run\n</code></pre>"},{"location":"getting-started/quick-reference.html#backup-and-restore","title":"Backup and Restore","text":""},{"location":"getting-started/quick-reference.html#neo4j-default-backend","title":"Neo4j (Default Backend)","text":"<p>Podman:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nmkdir -p backups\npodman exec madeinoz-knowledge-neo4j neo4j-admin database dump neo4j --to-stdout &gt; ./backups/knowledge-backup.dump\n</code></pre> <p>Docker:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nmkdir -p backups\ndocker exec madeinoz-knowledge-neo4j neo4j-admin database dump neo4j --to-stdout &gt; ./backups/knowledge-backup.dump\n</code></pre> <p>Verify:</p> <pre><code>podman exec madeinoz-knowledge-neo4j cypher-shell -u neo4j -p password \"MATCH (n) RETURN count(n)\"\n</code></pre>"},{"location":"getting-started/quick-reference.html#falkordb-backend","title":"FalkorDB Backend","text":"<p>Podman:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nmkdir -p backups\npodman exec madeinoz-knowledge-falkordb redis-cli BGSAVE\npodman cp madeinoz-knowledge-falkordb:/data/dump.rdb ./backups/knowledge-backup.rdb\n</code></pre> <p>Docker:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nmkdir -p backups\ndocker exec madeinoz-knowledge-falkordb redis-cli BGSAVE\ndocker cp madeinoz-knowledge-falkordb:/data/dump.rdb ./backups/knowledge-backup.rdb\n</code></pre> <p>Verify:</p> <pre><code>podman exec madeinoz-knowledge-falkordb redis-cli DBSIZE\npodman exec madeinoz-knowledge-falkordb redis-cli GRAPH.LIST\n</code></pre> <p>See the Backup &amp; Restore Guide for detailed instructions.</p>"},{"location":"getting-started/quick-reference.html#common-errors","title":"Common Errors","text":"<p>\"Connection refused\" \u2192 Server not running. Run: <code>bun run server-cli start</code></p> <p>\"API key invalid\" \u2192 Check PAI config (<code>$PAI_DIR/.env</code>) has correct key</p> <p>\"Port already in use\" \u2192 Stop other service using port 8000/7687 (Neo4j) or 6379 (FalkorDB)</p> <p>\"No entities extracted\" \u2192 Add more detail to your capture</p> <p>\"Rate limit exceeded\" \u2192 Reduce SEMAPHORE_LIMIT in PAI config</p>"},{"location":"getting-started/quick-reference.html#getting-help","title":"Getting Help","text":"<ol> <li>Check logs: <code>bun run server-cli logs</code></li> <li>Read the Troubleshooting Guide</li> <li>Review the Knowledge Graph Concepts</li> <li>Check the Architecture</li> </ol>"},{"location":"getting-started/quick-reference.html#version-info","title":"Version Info","text":"<p>System: Madeinoz Knowledge System v1.1.0 Components:</p> <ul> <li>Graphiti (MCP server)</li> <li>Neo4j (default graph database) or FalkorDB</li> <li>OpenAI (LLM and embeddings)</li> </ul> <p>Pro Tip: Bookmark this page for quick reference while using the system!</p>"},{"location":"installation/index.html","title":"Installation Guide","text":""},{"location":"installation/index.html#knowledge-system-installation-guide","title":"Knowledge System Installation Guide","text":"<p>Quick Install with AI</p> <p>Just tell Claude Code:</p> <p>\"Install the madeinoz-knowledge-system pack\"</p> <p>Or any of these variations:</p> <ul> <li>\"Help me install the Knowledge skill\"</li> <li>\"Set up the knowledge graph system\"</li> <li>\"Configure madeinoz knowledge\"</li> </ul> <p>Claude will guide you through the interactive installer, help you choose providers, and verify everything works.</p>"},{"location":"installation/index.html#prerequisites","title":"Prerequisites","text":"<p>System Requirements:</p> <ul> <li>Podman 3.0+ or Docker (container runtime)</li> <li>Bun runtime (for PAI skill execution)</li> <li>LLM Provider - One of the following:</li> <li>OpenRouter (recommended) - Access to multiple models including GPT-4o, Gemini 2.0 Flash, Claude</li> <li>OpenAI API key - Direct OpenAI access</li> <li>Anthropic, Google Gemini, or Groq API keys</li> <li>Ollama - For embeddings only (see note below about LLM compatibility)</li> <li>Claude Code or compatible PAI-enabled agent system</li> <li>2GB RAM minimum for container (4GB+ recommended for Ollama)</li> <li>1GB disk space for graph database</li> </ul> <p>Pack Dependencies:</p> <ul> <li>None - The memory sync hook reads from the PAI Memory System (<code>~/.claude/MEMORY/</code>) which is part of the core PAI installation.</li> </ul> <p>LLM Provider Options (Tested &amp; Recommended):</p> <p>Based on comprehensive real-world testing with Graphiti MCP (15 models tested):</p> Provider Model Cost/1K Status Notes OpenRouter (recommended) GPT-4o Mini $0.129 \u2705 MOST STABLE Reliable entity extraction, best balance OpenRouter Gemini 2.0 Flash $0.125 \u26a0\ufe0f BEST VALUE Cheapest but may have occasional validation errors OpenRouter Qwen 2.5 72B $0.126 \u2705 Works Good quality, slower (30s) OpenRouter Claude 3.5 Haiku $0.816 \u2705 Works 6x more expensive OpenRouter GPT-4o $2.155 \u2705 FASTEST Best speed (12s) OpenRouter Grok 3 $2.163 \u2705 Works xAI option, 22s OpenAI Direct gpt-4o-mini ~$0.15 \u2705 Works Proven stable Ollama llama3.2 Free \u274c Fails Pydantic validation errors <p>\u26a0\ufe0f Models that FAIL with Graphiti:</p> <ul> <li>All Llama models (3.1 8B, 3.3 70B) - Pydantic validation errors</li> <li>Mistral 7B - Pydantic validation errors</li> <li>DeepSeek V3 - Pydantic validation errors</li> <li>Grok 4 Fast, 4.1 Fast, 3 Mini, Grok 4 - Validation/timeout issues</li> <li>Claude Sonnet 4 - Processing timeout</li> </ul> <p>Embedding Options:</p> Provider Model Quality Speed Cost Notes Ollama (recommended) mxbai-embed-large 73.9% 87ms FREE Best value OpenRouter text-embedding-3-small 78.2% 824ms $0.02/1M Highest quality Ollama nomic-embed-text 63.5% 93ms FREE Alternative <p>OpenAI-Compatible Providers:</p> <p>These providers use the same API format as OpenAI but with different base URLs:</p> Provider Description Get API Key OpenRouter Access to 200+ models (Claude, GPT-4, Llama, etc.) https://openrouter.ai/keys Together AI Fast inference, good for Llama models https://api.together.xyz/settings/api-keys Fireworks AI Low latency inference https://fireworks.ai/api-keys DeepInfra Serverless GPU inference https://deepinfra.com/dash/api_keys <p>Ollama Setup (if using Ollama or Hybrid mode):</p> <ol> <li>Install Ollama: https://ollama.com/download</li> <li>Pull required models:</li> </ol> <pre><code>ollama pull llama3.2            # LLM model (only needed for full Ollama mode)\nollama pull mxbai-embed-large   # Embedding model (recommended - 77% quality, 156ms)\n</code></pre> <ol> <li>Ensure Ollama is running: <code>ollama serve</code></li> </ol> <p>Note: The <code>mxbai-embed-large</code> model provides the best balance of quality (77%) and speed (156ms) among tested Ollama embedders. See <code>docs/OLLAMA-MODEL-GUIDE.md</code> for detailed comparisons.</p> <p>Madeinoz Patches (Applied at Image Build Time):</p> <p>The custom Docker image includes patches that enable support for:</p> <ul> <li>Ollama (local, no API key required)</li> <li>OpenAI-compatible providers (OpenRouter, Together AI, Fireworks AI, DeepInfra)</li> </ul> <p>The upstream Graphiti MCP server has a bug (GitHub issue #1116) where it ignores custom <code>base_url</code> configuration and uses the wrong OpenAI client:</p> Client Endpoint Compatibility <code>OpenAIClient</code> (upstream default) <code>/v1/responses</code> \u274c OpenAI-only <code>OpenAIGenericClient</code> (patch uses) <code>/v1/chat/completions</code> \u2705 Works everywhere <p>How the patch works:</p> <ol> <li>Local providers (Ollama): When <code>OPENAI_BASE_URL</code> points to localhost/LAN addresses, no API key is required (uses dummy key <code>ollama</code>)</li> <li>Cloud providers (OpenRouter, Together, etc.): When <code>OPENAI_BASE_URL</code> points to a cloud service, requires the provider's API key</li> <li>All requests use <code>OpenAIGenericClient</code> which uses the standard <code>/v1/chat/completions</code> endpoint</li> <li>Embedding requests use the configured <code>EMBEDDER_BASE_URL</code> for the appropriate provider</li> </ol> <p>Supported cloud providers (detected by URL):</p> <ul> <li><code>openrouter.ai</code> \u2192 OpenRouter</li> <li><code>api.together.xyz</code> \u2192 Together AI</li> <li><code>api.fireworks.ai</code> \u2192 Fireworks AI</li> <li><code>api.deepinfra.com</code> \u2192 DeepInfra</li> <li><code>api.perplexity.ai</code> \u2192 Perplexity</li> <li><code>api.mistral.ai</code> \u2192 Mistral AI</li> </ul> <p>The patch is automatically mounted when using any of the docker-compose files:</p> <ul> <li><code>docker-compose-falkordb.yml</code> (FalkorDB backend, Docker)</li> <li><code>docker-compose-neo4j.yml</code> (Neo4j backend, Docker)</li> <li><code>podman-compose-falkordb.yml</code> (FalkorDB backend, Podman)</li> <li><code>podman-compose-neo4j.yml</code> (Neo4j backend, Podman)</li> </ul>"},{"location":"installation/index.html#provider-selection-guide","title":"Provider Selection Guide","text":"<p>The Madeinoz Knowledge System requires two AI components:</p> Component Purpose Selection LLM Provider Entity extraction, relationship detection Must output valid JSON Embedder Provider Vector embeddings for semantic search Generates embedding vectors"},{"location":"installation/index.html#interactive-installation-recommended","title":"Interactive Installation (Recommended)","text":"<p>The easiest way to configure providers is through the interactive installer:</p> <pre><code>cd ~/.claude/skills/Knowledge\nbun run tools/install.ts\n</code></pre> <p>The installer guides you through:</p> <ol> <li>Step 4: LLM Provider Selection - Choose your main LLM</li> <li>Step 5: API Key Configuration - Enter required keys</li> <li>Step 6: Model Selection - Pick specific models</li> </ol>"},{"location":"installation/index.html#provider-combinations","title":"Provider Combinations","text":"<p>Here are the recommended configurations based on real-world MCP testing (15 models tested):</p>"},{"location":"installation/index.html#option-1-gpt-4o-mini-ollama-recommended","title":"Option 1: GPT-4o Mini + Ollama (Recommended) \u2b50","text":"<p>Most stable LLM + free local embeddings - Proven &amp; Reliable</p> Component Provider Model Cost Quality LLM OpenRouter openai/gpt-4o-mini $0.129/1K ops \u2705 Most reliable entity extraction Embedder Ollama mxbai-embed-large Free 73.9% quality, 87ms <pre><code>LLM_PROVIDER=openai\nMODEL_NAME=openai/gpt-4o-mini\nOPENAI_API_KEY=sk-or-v1-your-openrouter-key\nOPENAI_BASE_URL=https://openrouter.ai/api/v1\n\nEMBEDDER_PROVIDER=openai\nEMBEDDER_BASE_URL=http://host.docker.internal:11434/v1\nEMBEDDER_MODEL=mxbai-embed-large\nEMBEDDER_DIMENSIONS=1024\n</code></pre>"},{"location":"installation/index.html#option-2-gemini-20-flash-ollama-budget","title":"Option 2: Gemini 2.0 Flash + Ollama (Budget)","text":"<p>Best value - cheapest working model, but may have occasional validation errors</p> Component Provider Model Cost Quality LLM OpenRouter google/gemini-2.0-flash-001 $0.125/1K ops \u26a0\ufe0f Extracts 8 entities but less stable Embedder Ollama mxbai-embed-large Free 73.9% quality, 87ms <pre><code>LLM_PROVIDER=openai\nMODEL_NAME=google/gemini-2.0-flash-001\nOPENAI_API_KEY=sk-or-v1-your-openrouter-key\nOPENAI_BASE_URL=https://openrouter.ai/api/v1\n\nEMBEDDER_PROVIDER=openai\nEMBEDDER_BASE_URL=http://host.docker.internal:11434/v1\nEMBEDDER_MODEL=mxbai-embed-large\nEMBEDDER_DIMENSIONS=1024\n</code></pre> <p>Note: Gemini 2.0 Flash is ~3% cheaper but may occasionally fail with Pydantic validation errors. If you experience validation errors, switch to Option 1 (GPT-4o Mini).</p>"},{"location":"installation/index.html#option-3-full-cloud-same-provider","title":"Option 3: Full Cloud (Same Provider)","text":"<p>Use Together AI for both LLM and embeddings</p> Component Provider Model Cost LLM Together AI meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo ~$0.88/1M tokens Embedder Together AI BAAI/bge-large-en-v1.5 ~$0.016/1M tokens <pre><code>LLM_PROVIDER=openai\nMODEL_NAME=meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\nOPENAI_API_KEY=your-together-key\nOPENAI_BASE_URL=https://api.together.xyz/v1\n\nEMBEDDER_PROVIDER=openai\nEMBEDDER_BASE_URL=https://api.together.xyz/v1\nEMBEDDER_MODEL=BAAI/bge-large-en-v1.5\nEMBEDDER_DIMENSIONS=1024\n</code></pre>"},{"location":"installation/index.html#option-4-full-ollama-not-recommended-free-but-unreliable","title":"Option 4: Full Ollama (NOT RECOMMENDED - Free but Unreliable)","text":"<p>\u26a0\ufe0f WARNING: Free option, but open-source LLMs fail Graphiti validation</p> Component Provider Model Cost Status LLM Ollama llama3.2 Free \u274c FAILS Pydantic validation Embedder Ollama mxbai-embed-large Free \u2705 Works great <pre><code>LLM_PROVIDER=openai\nMODEL_NAME=llama3.2\nOPENAI_BASE_URL=http://host.docker.internal:11434/v1\n\nEMBEDDER_PROVIDER=openai\nEMBEDDER_BASE_URL=http://host.docker.internal:11434/v1\nEMBEDDER_MODEL=mxbai-embed-large\nEMBEDDER_DIMENSIONS=1024\n</code></pre> <p>\u26a0\ufe0f WARNING: Full Ollama mode DOES NOT WORK with current Graphiti. All open-source models tested (Llama 3.1 8B, Llama 3.3 70B, Mistral 7B, DeepSeek V3) fail with Pydantic validation errors. Use Option 1 (OpenRouter + Ollama) instead - you get Gemini 2.0 Flash for $0.125/1K ops while still using free Ollama embeddings.</p>"},{"location":"installation/index.html#option-5-direct-openai-full-cloud","title":"Option 5: Direct OpenAI (Full Cloud)","text":"<p>Simplest setup, all OpenAI</p> Component Provider Model Cost LLM OpenAI gpt-4o-mini ~$0.15/1M tokens Embedder OpenAI text-embedding-3-small ~$0.02/1M tokens <pre><code>LLM_PROVIDER=openai\nMODEL_NAME=gpt-4o-mini\nOPENAI_API_KEY=sk-your-openai-key\n\nEMBEDDER_PROVIDER=openai\nEMBEDDER_MODEL=text-embedding-3-small\nEMBEDDER_DIMENSIONS=1536\n</code></pre>"},{"location":"installation/index.html#embedder-comparison","title":"Embedder Comparison","text":"Embedder Provider Quality Speed Dimensions Cost mxbai-embed-large Ollama 77% 156ms 1024 Free text-embedding-3-small OpenAI 75% 610ms 1536 $0.02/1M BAAI/bge-large-en-v1.5 Together ~75% ~200ms 1024 $0.016/1M <p>Recommendation: Use Ollama's <code>mxbai-embed-large</code> for embeddings - it's free, fast, and high quality.</p>"},{"location":"installation/index.html#changing-providers-after-installation","title":"Changing Providers After Installation","text":"<p>To change providers after initial setup:</p> <ol> <li>Stop the server:</li> </ol> <pre><code>bun run server-cli stop\n</code></pre> <ol> <li>Edit the configuration:</li> </ol> <pre><code># Edit ~/.claude/.env (or wherever your config is)\nnano ~/.claude/.env\n</code></pre> <ol> <li> <p>Update the relevant variables (see examples above)</p> </li> <li> <p>Restart the server:</p> </li> </ol> <pre><code>bun run server-cli start\n</code></pre>"},{"location":"installation/index.html#pre-installation-system-analysis","title":"Pre-Installation: System Analysis","text":"<p>IMPORTANT: Before installing, analyze the current system state to detect conflicts and ensure dependencies are met.</p> <p>FOR AI AGENTS: Run ALL pre-installation checks before proceeding. If ANY check shows a conflict or missing dependency:</p> <ul> <li>Port in use: Stop the existing service or choose a different port</li> <li>Missing dependency: Install the dependency first (Podman, Bun)</li> <li>Existing installation: Backup existing files, then proceed with update</li> <li>Missing API key: Prompt user to provide API key before continuing</li> </ul>"},{"location":"installation/index.html#step-01-detect-current-configuration","title":"Step 0.1: Detect Current Configuration","text":"<p>Run these commands to understand your current system:</p> <pre><code># 1. Check if PAI_DIR is set\necho \"PAI_DIR: ${PAI_DIR:-'NOT SET - will use ~/.claude'}\"\n\n# 2. Check for existing PAI directory\nPAI_CHECK=\"${PAI_DIR:-$HOME/.claude}\"\nif [ -d \"$PAI_CHECK\" ]; then\n  echo \"\u26a0\ufe0f  PAI directory EXISTS at: $PAI_CHECK\"\n  echo \"Contents:\"\n  ls -la \"$PAI_CHECK\" 2&gt;/dev/null || echo \"  (empty or inaccessible)\"\nelse\n  echo \"\u2713 PAI directory does not exist (clean install)\"\nfi\n\n# 3. Check for existing MCP server\necho \"\"\necho \"Checking for existing MCP server...\"\nif podman ps | grep -q \"madeinoz-knowledge-graph-mcp\"; then\n    echo \"\u26a0\ufe0f  Madeinoz Knowledge MCP server is already running\"\n    podman ps | grep \"madeinoz-knowledge-graph-mcp\"\nelse\n    echo \"\u2713 No Madeinoz Knowledge MCP server running\"\nfi\n\n# 4. Check if port 8000 is available\necho \"\"\necho \"Checking port availability...\"\nif lsof -i :8000 &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u26a0\ufe0f  Port 8000 is already in use\"\n    lsof -i :8000 | head -5\nelse\n    echo \"\u2713 Port 8000 is available\"\nfi\n\n# 5. Check if port 6379 is available (FalkorDB)\necho \"\"\necho \"Checking FalkorDB port 6379...\"\nif lsof -i :6379 &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u26a0\ufe0f  Port 6379 is already in use\"\n    lsof -i :6379 | head -5\nelse\n    echo \"\u2713 Port 6379 is available\"\nfi\n\n# 5b. Check if Neo4j ports are available (for Neo4j backend)\necho \"\"\necho \"Checking Neo4j ports (7474, 7687)...\"\nif lsof -i :7474 &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u26a0\ufe0f  Port 7474 is already in use (Neo4j Browser)\"\n    lsof -i :7474 | head -5\nelse\n    echo \"\u2713 Port 7474 is available (Neo4j Browser)\"\nfi\n\nif lsof -i :7687 &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u26a0\ufe0f  Port 7687 is already in use (Neo4j Bolt)\"\n    lsof -i :7687 | head -5\nelse\n    echo \"\u2713 Port 7687 is available (Neo4j Bolt)\"\nfi\n\n# 6. Check for existing Knowledge skill\necho \"\"\necho \"Checking for existing Knowledge skill...\"\nif [ -d \"$PAI_CHECK/skills/Knowledge\" ]; then\n  echo \"\u26a0\ufe0f  Knowledge skill already exists at: $PAI_CHECK/skills/Knowledge\"\nelse\n  echo \"\u2713 No existing Knowledge skill found\"\nfi\n\n# 7. Check environment variables\necho \"\"\necho \"Environment variables:\"\necho \"  PAI_DIR: ${PAI_DIR:-'NOT SET'}\"\necho \"  MADEINOZ_KNOWLEDGE_OPENAI_API_KEY: ${MADEINOZ_KNOWLEDGE_OPENAI_API_KEY:+SET (value hidden)}\"\n\n# 8. Check Podman installation\necho \"\"\necho \"Container Runtime Check:\"\nif command -v podman &amp;&gt; /dev/null; then\n    echo \"\u2713 Podman is installed: $(podman --version)\"\nelse\n    echo \"\u274c Podman is NOT installed\"\n    echo \"   Install with: brew install podman (macOS)\"\n    echo \"              or: sudo apt install podman (Ubuntu/Debian)\"\nfi\n</code></pre>"},{"location":"installation/index.html#step-02-verify-dependencies","title":"Step 0.2: Verify Dependencies","text":"<pre><code># Check for required dependencies\necho \"Dependency Verification:\"\necho \"========================\"\n\n# Check Bun runtime\nif command -v bun &amp;&gt; /dev/null; then\n    echo \"\u2713 Bun is installed: $(bun --version)\"\nelse\n    echo \"\u274c Bun is NOT installed\"\n    echo \"   Install with: curl -fsSL https://bun.sh/install | bash\"\nfi\n\n# Check for LLM provider configuration\nif [ -n \"$MADEINOZ_KNOWLEDGE_LLM_PROVIDER\" ] &amp;&amp; [ \"$MADEINOZ_KNOWLEDGE_LLM_PROVIDER\" = \"ollama\" ]; then\n    echo \"\u2713 Ollama is configured as LLM provider (no API key needed)\"\n    # Check if Ollama is running\n    if curl -sf http://localhost:11434/api/tags &gt; /dev/null 2&gt;&amp;1; then\n        echo \"\u2713 Ollama is running\"\n    else\n        echo \"\u26a0\ufe0f  Ollama is configured but not running - start with: ollama serve\"\n    fi\nelif [ -n \"$MADEINOZ_KNOWLEDGE_OPENAI_API_KEY\" ] || [ -n \"$MADEINOZ_KNOWLEDGE_ANTHROPIC_API_KEY\" ] || [ -n \"$MADEINOZ_KNOWLEDGE_GOOGLE_API_KEY\" ]; then\n    echo \"\u2713 LLM API key is configured (MADEINOZ_KNOWLEDGE_* prefix)\"\nelif [ -n \"$OPENAI_API_KEY\" ] || [ -n \"$ANTHROPIC_API_KEY\" ] || [ -n \"$GOOGLE_API_KEY\" ]; then\n    echo \"\u2713 LLM API key is configured (legacy - consider using MADEINOZ_KNOWLEDGE_* prefix)\"\nelse\n    echo \"\u2139\ufe0f  No LLM API key found - Ollama will be used by default (free, local)\"\n    echo \"   To use cloud providers, configure API keys during installation\"\nfi\n\n# Check for .env.example file\nif [ -f \"config/.env.example\" ]; then\n    echo \"\u2713 .env.example found in config/ (configuration template)\"\nelse\n    echo \"\u274c .env.example not found in config/\"\n    echo \"   This file should be in the pack at config/.env.example\"\nfi\n</code></pre>"},{"location":"installation/index.html#step-03-conflict-resolution-matrix","title":"Step 0.3: Conflict Resolution Matrix","text":"<p>Based on the detection above, follow the appropriate path:</p> Scenario Existing State Action Clean Install No MCP server, ports available, no existing skill Proceed normally with Step 1 Server Running MCP server already running Decide: keep existing (skip to Step 4) or stop/reinstall Port Conflict (FalkorDB) Ports 8000 or 6379 in use Stop conflicting services or change ports in Docker Compose files Port Conflict (Neo4j) Ports 7474 or 7687 in use Stop conflicting services or use FalkorDB backend Skill Exists Knowledge skill already installed Backup old skill, compare versions, then replace Missing Dependencies Podman or Bun not installed Install dependencies first, then retry"},{"location":"installation/index.html#step-04-version-detection-and-upgrade-check","title":"Step 0.4: Version Detection and Upgrade Check","text":"<p>FOR AI AGENTS: This step determines if an upgrade is needed by comparing the pack version with any existing installation. If versions match, offer to skip unless <code>--force</code> is specified.</p> <p>Step 0.4.1: Extract Pack Version</p> <pre><code># Extract version from pack README.md frontmatter\nPACK_DIR=\"${PACK_DIR:-$(pwd)}\"\nPACK_VERSION=$(grep -E \"^version:\" \"$PACK_DIR/README.md\" | head -1 | sed 's/version:[[:space:]]*//')\n\nif [ -z \"$PACK_VERSION\" ]; then\n    echo \"\u26a0 Warning: Could not extract pack version from README.md\"\n    PACK_VERSION=\"unknown\"\nfi\n\necho \"Pack version: $PACK_VERSION\"\n</code></pre> <p>Step 0.4.2: Detect Existing Installation Version</p> <pre><code>PAI_CHECK=\"${PAI_DIR:-$HOME/.claude}\"\nEXISTING_VERSION=\"none\"\n\n# Primary: extract version from SKILL.md frontmatter\nSKILL_FILE=\"$PAI_CHECK/skills/Knowledge/SKILL.md\"\n\nif [ -f \"$SKILL_FILE\" ]; then\n    EXISTING_VERSION=$(grep -E \"^version:\" \"$SKILL_FILE\" | head -1 | sed 's/version:[[:space:]]*//')\n    if [ -n \"$EXISTING_VERSION\" ]; then\n        echo \"Existing installation version: $EXISTING_VERSION\"\n    else\n        echo \"Existing installation found (version unknown - pre-1.2.0)\"\n        EXISTING_VERSION=\"pre-1.2.0\"\n    fi\nelif [ -d \"$PAI_CHECK/skills/Knowledge\" ]; then\n    echo \"Existing installation found (no SKILL.md - corrupted install)\"\n    EXISTING_VERSION=\"unknown\"\nelse\n    echo \"No existing installation found\"\nfi\n</code></pre> <p>Step 0.4.3: Version Comparison</p> <pre><code># Compare versions\nif [ \"$EXISTING_VERSION\" = \"none\" ]; then\n    echo \"\u2192 Fresh install: proceeding with version $PACK_VERSION\"\n    INSTALL_ACTION=\"install\"\nelif [ \"$EXISTING_VERSION\" = \"$PACK_VERSION\" ]; then\n    echo \"\u2192 Same version ($PACK_VERSION) already installed\"\n    if [ \"${FORCE_REINSTALL:-false}\" = \"true\" ]; then\n        echo \"  --force specified: proceeding with reinstall\"\n        INSTALL_ACTION=\"reinstall\"\n    else\n        echo \"  Use --force to reinstall, or skip to Step 4 (verification)\"\n        INSTALL_ACTION=\"skip\"\n    fi\nelse\n    echo \"\u2192 Upgrade available: $EXISTING_VERSION \u2192 $PACK_VERSION\"\n    INSTALL_ACTION=\"upgrade\"\nfi\n\nexport INSTALL_ACTION EXISTING_VERSION PACK_VERSION\n</code></pre> <p>Step 0.4.4: Backup Before Upgrade (If Needed)</p> <p>Only create backup when upgrading or reinstalling:</p> <pre><code>if [ \"$INSTALL_ACTION\" = \"upgrade\" ] || [ \"$INSTALL_ACTION\" = \"reinstall\" ]; then\n    # Create timestamped backup\n    BACKUP_DIR=\"$HOME/.madeinoz-backup/$(date +%Y%m%d-%H%M%S)\"\n    mkdir -p \"$BACKUP_DIR\"\n\n    echo \"\"\n    echo \"Creating backup at: $BACKUP_DIR\"\n\n    # Backup existing skill if present\n    if [ -d \"$PAI_CHECK/skills/Knowledge\" ]; then\n        cp -r \"$PAI_CHECK/skills/Knowledge\" \"$BACKUP_DIR/Knowledge\"\n        echo \"\u2713 Backed up existing Knowledge skill (v$EXISTING_VERSION)\"\n    fi\n\n    # Backup legacy .env if present (config is now in PAI .env)\n    if [ -f \"config/.env\" ]; then\n        cp config/.env \"$BACKUP_DIR/.env.legacy\"\n        echo \"\u2713 Backed up legacy .env file (migrate to PAI .env)\"\n    fi\n\n    # Backup container if running\n    if podman ps 2&gt;/dev/null | grep -q \"madeinoz-knowledge\"; then\n        podman export madeinoz-knowledge-graph-mcp &gt; \"$BACKUP_DIR/madeinoz-container.tar\" 2&gt;/dev/null || true\n        echo \"\u2713 Backed up running container (if possible)\"\n    fi\n\n    # Save backup manifest\n    cat &gt; \"$BACKUP_DIR/manifest.json\" &lt;&lt; EOF\n{\n    \"backup_date\": \"$(date -Iseconds)\",\n    \"previous_version\": \"$EXISTING_VERSION\",\n    \"upgrading_to\": \"$PACK_VERSION\",\n    \"action\": \"$INSTALL_ACTION\"\n}\nEOF\n    echo \"\u2713 Created backup manifest\"\n\n    echo \"\"\n    echo \"Backup complete! Proceeding with $INSTALL_ACTION...\"\nelif [ \"$INSTALL_ACTION\" = \"skip\" ]; then\n    echo \"\"\n    echo \"Skipping installation (same version). To verify existing install, jump to Step 4.\"\n    echo \"To force reinstall, set FORCE_REINSTALL=true and re-run.\"\nfi\n</code></pre> <p>After completing version check and backup, proceed to Step 1.</p>"},{"location":"installation/index.html#step-1-verify-pack-contents","title":"Step 1: Verify Pack Contents","text":"<p>FOR AI AGENTS: This step verifies the pack is complete. If ANY file is missing, STOP and inform the user - the pack is incomplete and cannot be installed.</p> <p>Ensure you have all required files in the pack directory:</p> <pre><code># Navigate to pack directory\ncd /path/to/madeinoz-knowledge-system\n\n# Verify required files exist\necho \"Checking pack contents...\"\n\nREQUIRED_FILES=(\n    \"README.md\"\n    \"SKILL.md\"\n    \"tools/server-cli.ts\"\n    \"docker/podman-compose-falkordb.yml\"\n    \"docker/podman-compose-neo4j.yml\"\n    \"docker/docker-compose-falkordb.yml\"\n    \"docker/docker-compose-neo4j.yml\"\n    \"config/.env.example\"\n    \"workflows/CaptureEpisode.md\"\n    \"workflows/SearchKnowledge.md\"\n    \"workflows/SearchFacts.md\"\n    \"workflows/SearchByDate.md\"\n    \"workflows/GetRecent.md\"\n    \"workflows/GetStatus.md\"\n    \"workflows/ClearGraph.md\"\n    \"workflows/BulkImport.md\"\n    \"tools/Install.md\"\n)\n\nALL_FOUND=true\nfor file in \"${REQUIRED_FILES[@]}\"; do\n    if [ -f \"$file\" ]; then\n        echo \"\u2713 $file\"\n    else\n        echo \"\u2717 MISSING: $file\"\n        ALL_FOUND=false\n    fi\ndone\n\nif [ \"$ALL_FOUND\" = true ]; then\n    echo \"\"\n    echo \"\u2713 All required files present!\"\nelse\n    echo \"\"\n    echo \"\u274c Some files are missing. Please ensure you have the complete pack.\"\n    exit 1\nfi\n</code></pre>"},{"location":"installation/index.html#step-2-add-configuration-to-pai","title":"Step 2: Add Configuration to PAI","text":"<p>PAI .env is the ONLY source of truth.</p> <p>All MADEINOZ_KNOWLEDGE_* configuration lives in PAI .env (<code>~/.claude/.env</code>). The pack's config/.env is auto-generated at runtime when starting containers.</p> <p>Add Madeinoz Knowledge System settings to your PAI configuration:</p> <p>[Configuration sections continue with PAI .env setup, provider selection, and variable management...]</p>"},{"location":"installation/index.html#step-3-start-mcp-server","title":"Step 3: Start MCP Server","text":"<p>[MCP server startup instructions...]</p>"},{"location":"installation/index.html#step-4-install-full-pack-and-knowledge-skill","title":"Step 4: Install Full Pack and Knowledge Skill","text":"<p>[Pack installation instructions...]</p>"},{"location":"installation/index.html#step-5-configure-mcp-server-in-claude-code","title":"Step 5: Configure MCP Server in Claude Code","text":"<p>[MCP configuration instructions...]</p>"},{"location":"installation/index.html#step-6-verify-installation","title":"Step 6: Verify Installation","text":"<p>[Verification instructions...]</p>"},{"location":"installation/index.html#step-7-test-installation","title":"Step 7: Test Installation","text":"<p>[Testing instructions...]</p>"},{"location":"installation/index.html#step-8-post-installation-configuration","title":"Step 8: Post-Installation Configuration","text":"<p>[Post-installation steps...]</p>"},{"location":"installation/index.html#step-9-install-memory-sync-hook-optional-but-recommended","title":"Step 9: Install Memory Sync Hook (Optional but Recommended)","text":"<p>[Memory sync hook installation...]</p>"},{"location":"installation/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/index.html#server-wont-start","title":"Server Won't Start","text":"<p>Symptoms: <code>bun run server-cli start</code> fails or container exits immediately</p> <p>Solutions:</p> <ol> <li>Port conflict: Stop conflicting service or modify ports in Docker Compose files</li> <li>API key invalid: Verify API key in PAI config has credits/quota</li> <li>Image pull failed: Check internet connection and try again</li> <li>Resource limits: Ensure system has at least 2GB RAM available</li> </ol>"},{"location":"installation/index.html#skill-not-loading","title":"Skill Not Loading","text":"<p>Solutions:</p> <ol> <li>Restart Claude Code - Skills are loaded on startup</li> <li>Check SKILL.md format - Ensure frontmatter is valid YAML</li> <li>Verify file paths - All workflows should be in <code>workflows/</code> and tools in <code>tools/</code></li> </ol>"},{"location":"installation/index.html#knowledge-not-being-captured","title":"Knowledge Not Being Captured","text":"<p>Solutions:</p> <ol> <li>Server not running: Start with <code>bun run server-cli start</code></li> <li>API quota exceeded: Check OpenAI usage dashboard</li> <li>Content too brief: Add more context and detail</li> </ol>"},{"location":"installation/index.html#uninstallation","title":"Uninstallation","text":"<p>To completely remove the Knowledge skill:</p> <pre><code># 1. Stop and remove container\nbun run server-cli stop\npodman rm madeinoz-knowledge-graph-mcp\n\n# 2. Remove Knowledge skill\nrm -rf ~/.claude/skills/Knowledge\n</code></pre>"},{"location":"installation/index.html#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check logs: <code>bun run server-cli logs</code></li> <li>Check status: <code>bun run server-cli status</code></li> <li>Review documentation:</li> <li><code>README.md</code> - Complete pack documentation</li> <li><code>../verification.md</code> - Verification checklist</li> </ol>"},{"location":"installation/remote-deployment.html","title":"Remote Deployment","text":""},{"location":"installation/remote-deployment.html#remote-production-deployment","title":"Remote Production Deployment","text":"<p>Deploy the Knowledge Graph system on remote servers without PAI infrastructure.</p>"},{"location":"installation/remote-deployment.html#overview","title":"Overview","text":"<p>The production Docker Compose configuration provides:</p> <ul> <li>Standalone deployment - No PAI or local infrastructure required</li> <li>Native naming - Clean service names (neo4j, knowledge-mcp)</li> <li>Native environment variables - Standard Neo4j/OpenAI variable names</li> <li>Data persistence - Docker volumes for database storage</li> <li>Auto-restart - Services restart automatically on failure</li> </ul>"},{"location":"installation/remote-deployment.html#quick-start","title":"Quick Start","text":""},{"location":"installation/remote-deployment.html#1-copy-files-to-server","title":"1. Copy Files to Server","text":"<pre><code># Copy the production compose file\nscp src/skills/server/docker-compose-production.yml user@server:/opt/knowledge-graph/\n\n# SSH to server\nssh user@server\ncd /opt/knowledge-graph\n</code></pre>"},{"location":"installation/remote-deployment.html#2-create-environment-file","title":"2. Create Environment File","text":"<pre><code>cat &gt; .env &lt;&lt; 'EOF'\n# Required\nNEO4J_PASSWORD=your-secure-password-here\nOPENAI_API_KEY=sk-your-openai-key\n\n# Optional (defaults shown)\nMODEL_NAME=gpt-4o-mini\nEMBEDDING_MODEL_NAME=text-embedding-3-small\nLOG_LEVEL=INFO\nEOF\n\n# Secure the file\nchmod 600 .env\n</code></pre>"},{"location":"installation/remote-deployment.html#3-start-services","title":"3. Start Services","text":"<pre><code>docker compose -f docker-compose-production.yml up -d\n</code></pre>"},{"location":"installation/remote-deployment.html#4-verify-deployment","title":"4. Verify Deployment","text":"<pre><code># Check service status\ndocker compose -f docker-compose-production.yml ps\n\n# Check health endpoints\ncurl http://localhost:8000/health\ncurl http://localhost:7474\n\n# View logs\ndocker compose -f docker-compose-production.yml logs -f\n</code></pre>"},{"location":"installation/remote-deployment.html#configuration","title":"Configuration","text":""},{"location":"installation/remote-deployment.html#environment-variables","title":"Environment Variables","text":"Variable Required Default Description <code>NEO4J_PASSWORD</code> Yes <code>changeme</code> Neo4j database password <code>OPENAI_API_KEY</code> Yes - OpenAI API key for LLM operations <code>MODEL_NAME</code> No <code>gpt-4o-mini</code> LLM model for entity extraction <code>EMBEDDING_MODEL_NAME</code> No <code>text-embedding-3-small</code> Embedding model <code>LOG_LEVEL</code> No <code>INFO</code> Logging verbosity (DEBUG, INFO, WARNING, ERROR)"},{"location":"installation/remote-deployment.html#alternative-llm-providers","title":"Alternative LLM Providers","text":"<p>For Anthropic Claude:</p> <pre><code>ANTHROPIC_API_KEY=sk-ant-your-key\nMODEL_NAME=claude-3-5-haiku-latest\n</code></pre> <p>For Google Gemini:</p> <pre><code>GOOGLE_API_KEY=your-google-key\nMODEL_NAME=gemini-2.0-flash\n</code></pre>"},{"location":"installation/remote-deployment.html#ports","title":"Ports","text":"Port Service Protocol Description 7474 Neo4j HTTP Neo4j Browser interface 7687 Neo4j Bolt Neo4j database protocol 8000 knowledge-mcp HTTP MCP server endpoint 9090 knowledge-mcp HTTP Prometheus metrics"},{"location":"installation/remote-deployment.html#connecting-from-pai","title":"Connecting from PAI","text":"<p>Configure your PAI MCP client to connect to the remote server:</p> <pre><code>{\n  \"mcpServers\": {\n    \"madeinoz-knowledge\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"http://your-server:8000/mcp\"]\n    }\n  }\n}\n</code></pre> <p>Or using the knowledge-client library:</p> <pre><code>import { KnowledgeClient } from './lib/knowledge-client';\n\nconst client = new KnowledgeClient({\n  endpoint: 'http://your-server:8000',\n});\n</code></pre>"},{"location":"installation/remote-deployment.html#operations","title":"Operations","text":""},{"location":"installation/remote-deployment.html#view-logs","title":"View Logs","text":"<pre><code># All services\ndocker compose -f docker-compose-production.yml logs -f\n\n# Specific service\ndocker compose -f docker-compose-production.yml logs -f knowledge-mcp\ndocker compose -f docker-compose-production.yml logs -f neo4j\n</code></pre>"},{"location":"installation/remote-deployment.html#stop-services","title":"Stop Services","text":"<pre><code>docker compose -f docker-compose-production.yml down\n</code></pre>"},{"location":"installation/remote-deployment.html#restart-services","title":"Restart Services","text":"<pre><code>docker compose -f docker-compose-production.yml restart\n</code></pre>"},{"location":"installation/remote-deployment.html#update-services","title":"Update Services","text":"<pre><code>docker compose -f docker-compose-production.yml pull\ndocker compose -f docker-compose-production.yml up -d\n</code></pre>"},{"location":"installation/remote-deployment.html#backup-data","title":"Backup Data","text":"<pre><code># Stop services first\ndocker compose -f docker-compose-production.yml down\n\n# Backup Neo4j data volume\ndocker run --rm -v knowledge-graph_neo4j-data:/data -v $(pwd):/backup alpine \\\n  tar cvf /backup/neo4j-backup-$(date +%Y%m%d).tar /data\n\n# Restart services\ndocker compose -f docker-compose-production.yml up -d\n</code></pre>"},{"location":"installation/remote-deployment.html#security-considerations","title":"Security Considerations","text":"<ol> <li>Change default password - Never use <code>changeme</code> in production</li> <li>Secure .env file - Set permissions to 600 (owner read/write only)</li> <li>Firewall rules - Restrict port access to trusted IPs</li> <li>TLS/SSL - Consider adding reverse proxy with TLS for production</li> <li>API keys - Rotate API keys regularly</li> </ol>"},{"location":"installation/remote-deployment.html#adding-tls-with-nginx","title":"Adding TLS with Nginx","text":"<p>Example nginx configuration for TLS termination:</p> <pre><code>server {\n    listen 443 ssl;\n    server_name knowledge.yourdomain.com;\n\n    ssl_certificate /etc/letsencrypt/live/knowledge.yourdomain.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/knowledge.yourdomain.com/privkey.pem;\n\n    location / {\n        proxy_pass http://localhost:8000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n</code></pre>"},{"location":"installation/remote-deployment.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/remote-deployment.html#services-wont-start","title":"Services won't start","text":"<pre><code># Check Docker status\ndocker info\n\n# Check available resources\nfree -h\ndf -h\n\n# Check logs for errors\ndocker compose -f docker-compose-production.yml logs\n</code></pre>"},{"location":"installation/remote-deployment.html#neo4j-fails-health-check","title":"Neo4j fails health check","text":"<pre><code># Wait for initialization (can take 60+ seconds on first run)\ndocker logs neo4j\n\n# Check memory settings\ndocker stats neo4j\n</code></pre>"},{"location":"installation/remote-deployment.html#mcp-server-cant-connect-to-neo4j","title":"MCP server can't connect to Neo4j","text":"<pre><code># Verify Neo4j is healthy\ndocker compose -f docker-compose-production.yml ps\n\n# Check network connectivity\ndocker exec knowledge-mcp ping neo4j\n</code></pre>"},{"location":"installation/remote-deployment.html#out-of-memory","title":"Out of memory","text":"<p>Reduce Neo4j memory settings in docker-compose-production.yml:</p> <pre><code>environment:\n  - NEO4J_server_memory_heap_max__size=512m\n  - NEO4J_server_memory_pagecache_size=256m\n</code></pre>"},{"location":"installation/requirements.html","title":"Installation Requirements","text":""},{"location":"installation/requirements.html#installation-guide","title":"Installation Guide","text":"<p>This guide will walk you through installing the Madeinoz Knowledge System step by step. Don't worry if you're not a developer - we'll explain everything clearly.</p>"},{"location":"installation/requirements.html#what-youll-need","title":"What You'll Need","text":"<p>Before starting, make sure you have:</p> <ol> <li> <p>A computer with:</p> </li> <li> <p>macOS, Linux, or Windows with WSL</p> </li> <li>At least 2GB of free RAM</li> <li> <p>At least 1GB of disk space</p> </li> <li> <p>Software installed:</p> </li> <li> <p>Podman or Docker (for running containers)</p> </li> <li>Bun (JavaScript runtime)</li> <li> <p>An OpenAI API key (or compatible service)</p> </li> <li> <p>Dependent systems:</p> </li> <li> <p>None (PAI Memory System is built-in to Claude Code)</p> </li> </ol>"},{"location":"installation/requirements.html#what-is-a-container","title":"What is a Container?","text":"<p>Before we begin, a quick explanation: This system uses \"containers\" to run the knowledge graph software. Think of a container like a self-contained app that includes everything it needs to run. You don't need to install complex database software - the container handles all that for you.</p>"},{"location":"installation/requirements.html#step-by-step-installation","title":"Step-by-Step Installation","text":""},{"location":"installation/requirements.html#step-1-get-your-api-key","title":"Step 1: Get Your API Key","text":"<p>The system uses AI to understand your knowledge, so you need an API key from an AI provider.</p> <p>For OpenAI (Recommended):</p> <ol> <li>Go to https://platform.openai.com/api-keys</li> <li>Sign in or create an account</li> <li>Click \"Create new secret key\"</li> <li>Copy the key (it starts with <code>sk-</code>)</li> <li>Keep it safe - you'll need it soon</li> </ol> <p>Cost: About $0.50-2.00 per month for typical personal use. The system uses the efficient gpt-4o-mini model by default.</p>"},{"location":"installation/requirements.html#step-2-check-if-you-have-podman","title":"Step 2: Check if You Have Podman","text":"<p>Podman is what runs the knowledge system in a container.</p> <p>Open your terminal and type:</p> <pre><code>podman --version\n</code></pre> <p>If you see a version number: Great! Skip to Step 3.</p> <p>If you see \"command not found\":</p> <p>Install Podman:</p> <p>On macOS:</p> <pre><code>brew install podman\npodman machine init\npodman machine start\n</code></pre> <p>On Ubuntu/Debian:</p> <pre><code>sudo apt update\nsudo apt install podman\n</code></pre> <p>On Windows: Use Windows Subsystem for Linux (WSL) and follow Ubuntu instructions.</p>"},{"location":"installation/requirements.html#step-3-check-if-you-have-bun","title":"Step 3: Check if You Have Bun","text":"<p>Bun is the JavaScript runtime that makes everything fast.</p> <pre><code>bun --version\n</code></pre> <p>If you see a version number: Great! Skip to Step 4.</p> <p>If you see \"command not found\":</p> <p>Install Bun:</p> <pre><code>curl -fsSL https://bun.sh/install | bash\n</code></pre> <p>Then close and reopen your terminal.</p>"},{"location":"installation/requirements.html#step-4-navigate-to-the-pack-directory","title":"Step 4: Navigate to the Pack Directory","text":"<p>The Madeinoz Knowledge System is in your PAI packs folder:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n</code></pre> <p>Can't find it? If you installed PAI somewhere else, look for <code>Packs/madeinoz-knowledge-system</code> in your PAI directory.</p>"},{"location":"installation/requirements.html#step-5-configure-your-api-key","title":"Step 5: Configure Your API Key","text":"<p>Add your API key to your PAI configuration file. The config file location is:</p> <ul> <li><code>$PAI_DIR/.env</code> if PAI_DIR is set, OR</li> <li><code>~/.claude/.env</code> (default)</li> </ul> <p>Open your config file:</p> <pre><code>nano \"${PAI_DIR:-$HOME/.claude}/.env\"\n</code></pre> <p>Add these lines (see <code>config/.env.example</code> for all options):</p> <pre><code># Madeinoz Knowledge System\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-your-openai-api-key-here\nMADEINOZ_KNOWLEDGE_MODEL_NAME=gpt-4o-mini\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\n</code></pre> <p>Replace <code>sk-your-openai-api-key-here</code> with your actual API key.</p> <p>Save the file:</p> <ul> <li>Press <code>Ctrl + O</code> to save</li> <li>Press <code>Enter</code> to confirm</li> <li>Press <code>Ctrl + X</code> to exit</li> </ul>"},{"location":"installation/requirements.html#step-6-start-the-knowledge-system","title":"Step 6: Start the Knowledge System","text":"<p>Now start the MCP server (this runs the knowledge graph):</p> <pre><code>bun run server-cli start\n</code></pre> <p>You'll see output like:</p> <pre><code>Starting Madeinoz Knowledge System...\nCreating network: madeinoz-knowledge-net\nStarting Neo4j container...\nStarting Graphiti MCP server...\nServer is running at http://localhost:8000\n</code></pre> <p>This will take 1-2 minutes the first time as it downloads the container images.</p> <p>Keep this terminal window open - the server runs here.</p>"},{"location":"installation/requirements.html#step-7-verify-its-working","title":"Step 7: Verify It's Working","text":"<p>Open a new terminal window and check the status:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nbun run server-cli status\n</code></pre> <p>You should see:</p> <pre><code>Madeinoz Knowledge System Status:\n\nContainers:\n  madeinoz-knowledge-graph-mcp: running\n  madeinoz-knowledge-neo4j: running\n\nMCP Server: http://localhost:8000/sse\n  Status: healthy\n</code></pre>"},{"location":"installation/requirements.html#step-8-install-memory-sync-hook-optional-but-recommended","title":"Step 8: Install Memory Sync Hook (Optional but Recommended)","text":"<p>This hook automatically syncs your learning captures from the PAI Memory System to your knowledge graph:</p> <pre><code>cd ~/.claude/skills/Knowledge\nbun run tools/install.ts\n</code></pre> <p>Follow the prompts to:</p> <ol> <li>Verify your memory directory location (~/.claude/MEMORY)</li> <li>Install the sync hook</li> <li>Configure automatic syncing</li> </ol> <p>What does this do? When you capture \"learnings\" or \"research\" in your AI sessions, they'll automatically be added to your knowledge graph.</p>"},{"location":"installation/requirements.html#step-9-test-the-installation","title":"Step 9: Test the Installation","text":"<p>Time to test everything! In your AI assistant (like Claude Code), try:</p> <pre><code>Remember that the Madeinoz Knowledge System was just installed today.\n</code></pre> <p>The assistant should respond with something like:</p> <pre><code>Knowledge Captured\n\nStored episode: Madeinoz Knowledge System Installation\n\nEntities extracted:\n- Madeinoz Knowledge System (Tool)\n- installation (Event)\n- today (Temporal)\n\nRelationships identified:\n- Madeinoz Knowledge System -&gt; was installed -&gt; today\n</code></pre> <p>Then try searching:</p> <pre><code>What do I know about PAI?\n</code></pre> <p>You should see your newly stored knowledge!</p>"},{"location":"installation/requirements.html#what-just-happened","title":"What Just Happened?","text":"<p>Let's review what you installed:</p> <ol> <li>Neo4j - A graph database (like a smart filing cabinet)</li> <li>Graphiti MCP Server - The AI brain that processes your knowledge</li> <li>PAI Skill - The interface that lets you talk to the system naturally</li> <li>Sync Hook - Automatically captures learnings to your knowledge graph</li> </ol> <p>All of these work together so you can simply say \"remember this\" and have your AI build a knowledge graph automatically.</p>"},{"location":"installation/requirements.html#starting-and-stopping","title":"Starting and Stopping","text":""},{"location":"installation/requirements.html#to-stop-the-server","title":"To Stop the Server","text":"<p>In the terminal where it's running, press <code>Ctrl + C</code>.</p> <p>Or from another terminal:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nbun run server-cli stop\n</code></pre>"},{"location":"installation/requirements.html#to-start-again","title":"To Start Again","text":"<pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nbun run server-cli start\n</code></pre>"},{"location":"installation/requirements.html#check-status-anytime","title":"Check Status Anytime","text":"<pre><code>bun run server-cli status\n</code></pre>"},{"location":"installation/requirements.html#view-logs","title":"View Logs","text":"<p>If something goes wrong:</p> <pre><code>bun run server-cli logs\n</code></pre>"},{"location":"installation/requirements.html#optional-add-to-your-shell-startup","title":"Optional: Add to Your Shell Startup","text":"<p>Want the server to start automatically when you open your terminal?</p> <p>Add this to your shell configuration file (<code>~/.zshrc</code> or <code>~/.bashrc</code>):</p> <pre><code># Auto-start Madeinoz Knowledge System\nif ! podman ps | grep -q \"madeinoz-knowledge-graph-mcp\"; then\n    cd ~/.config/pai/Packs/madeinoz-knowledge-system &amp;&amp; bun run server-cli start\nfi\n</code></pre>"},{"location":"installation/requirements.html#customization-options","title":"Customization Options","text":""},{"location":"installation/requirements.html#using-a-different-ai-model","title":"Using a Different AI Model","text":"<p>The default model is <code>gpt-4o-mini</code> (fast and cheap). You can change it to:</p> <ul> <li><code>gpt-4o</code> - More accurate but costs more</li> <li><code>gpt-3.5-turbo</code> - Cheaper but less accurate</li> </ul> <p>Edit your PAI config (<code>$PAI_DIR/.env</code> or <code>~/.claude/.env</code>):</p> <pre><code>MADEINOZ_KNOWLEDGE_MODEL_NAME=gpt-4o\n</code></pre> <p>Then restart the server.</p>"},{"location":"installation/requirements.html#using-multiple-knowledge-graphs","title":"Using Multiple Knowledge Graphs","text":"<p>Want separate graphs for work and personal? Use group IDs:</p> <p>In your PAI config (<code>$PAI_DIR/.env</code> or <code>~/.claude/.env</code>):</p> <pre><code>MADEINOZ_KNOWLEDGE_GROUP_ID=work\n</code></pre> <p>Or specify when capturing:</p> <pre><code>Remember this in my work knowledge: [your information]\n</code></pre>"},{"location":"installation/requirements.html#adjusting-concurrency","title":"Adjusting Concurrency","text":"<p>If you hit API rate limits, reduce the concurrent requests:</p> <p>In your PAI config (<code>$PAI_DIR/.env</code> or <code>~/.claude/.env</code>):</p> <pre><code>MADEINOZ_KNOWLEDGE_SEMAPHORE_LIMIT=5\n</code></pre> <p>Lower numbers = slower but less likely to hit rate limits.</p>"},{"location":"installation/requirements.html#next-steps","title":"Next Steps","text":"<p>Now that everything is installed:</p> <ol> <li>Read the Usage Guide to learn all the commands</li> <li>Check out Concepts to understand how it works</li> <li>Start using it! The more you capture, the more valuable it becomes</li> </ol>"},{"location":"installation/requirements.html#troubleshooting","title":"Troubleshooting","text":"<p>Problem: \"Port already in use\"</p> <ul> <li>Another service is using port 8000 or 7687 (Neo4j)</li> <li>Stop the other service or change ports in the Docker Compose files</li> </ul> <p>Problem: \"API key invalid\"</p> <ul> <li>Check your API key in PAI config (<code>$PAI_DIR/.env</code> or <code>~/.claude/.env</code>)</li> <li>Verify it has credits at https://platform.openai.com/usage</li> </ul> <p>Problem: \"Container won't start\"</p> <ul> <li>Check Docker/Podman is running: <code>podman ps</code></li> <li>View logs: <code>bun run server-cli logs</code></li> <li>Try restarting: <code>bun run server-cli restart</code></li> </ul> <p>Problem: \"No entities extracted\"</p> <ul> <li>Add more detail to what you're capturing</li> <li>Try a different model (gpt-4o instead of gpt-4o-mini)</li> <li>Make sure your content has clear concepts and relationships</li> </ul> <p>For more help, see the Troubleshooting Guide.</p>"},{"location":"installation/verification.html","title":"Installation Verification","text":""},{"location":"installation/verification.html#madeinoz-knowledge-system-verification-checklist","title":"Madeinoz Knowledge System - Verification Checklist","text":"<p>Mandatory verification checklist for the Knowledge pack installation.</p> <p>Use this checklist to verify that your installation is complete and functional.</p> <p>FOR AI AGENTS: This checklist MUST be completed after installation. Follow these rules:</p> <ol> <li>Run EVERY check - Do not skip any verification step</li> <li>Mark each item PASS or FAIL - Keep track of results</li> <li>Stop on failures - If a section fails, troubleshoot before continuing</li> <li>All sections must pass - Installation is NOT complete until all checks pass</li> <li>Report failures clearly - Tell the user which specific checks failed</li> <li>Detect database backend FIRST - Run Section 0 to determine which backend is configured</li> </ol>"},{"location":"installation/verification.html#verification-overview","title":"Verification Overview","text":"<p>This checklist ensures:</p> <ul> <li>All components are installed</li> <li>System is properly configured</li> <li>All integrations are working</li> <li>End-to-end functionality is operational</li> </ul> <p>Supports two database backends:</p> <ul> <li>Neo4j (default): Native graph database with Cypher queries</li> <li>FalkorDB: Redis-based graph database with RediSearch</li> </ul> <p>Run through each section in order. Mark items as PASS or FAIL.</p>"},{"location":"installation/verification.html#section-0-database-backend-detection","title":"Section 0: Database Backend Detection","text":"<p>FOR AI AGENTS: Run this FIRST to determine which backend is configured. The result affects which checks to run in subsequent sections.</p>"},{"location":"installation/verification.html#01-determine-configured-backend","title":"0.1 Determine Configured Backend","text":"<ul> <li>[ ] Database backend identified</li> </ul> <p>Verification commands:</p> <pre><code># Check PAI config for DATABASE_TYPE\ngrep \"MADEINOZ_KNOWLEDGE_DATABASE_TYPE\" \"${PAI_DIR:-$HOME/.claude}/.env\" 2&gt;/dev/null\n\n# Or check running containers\npodman ps --format \"{{.Names}}\" | grep madeinoz-knowledge\n# For Docker:\ndocker ps --format \"{{.Names}}\" | grep madeinoz-knowledge\n</code></pre> <p>Results:</p> <ul> <li>If <code>DATABASE_TYPE=neo4j</code> OR container <code>madeinoz-knowledge-neo4j</code> is running \u2192 Neo4j Backend</li> <li>If <code>DATABASE_TYPE=falkordb</code> OR container <code>madeinoz-knowledge-falkordb</code> is running \u2192 FalkorDB Backend</li> <li>Default (not set) \u2192 Neo4j Backend</li> </ul> <p>Record your backend: [ ] FalkorDB / [ ] Neo4j</p>"},{"location":"installation/verification.html#section-1-directory-structure-verification","title":"Section 1: Directory Structure Verification","text":"<p>Verify all required files and directories are present.</p>"},{"location":"installation/verification.html#11-pack-root-files","title":"1.1 Pack Root Files","text":"<ul> <li>[ ] README.md exists in pack root</li> <li>[ ] INSTALL.md exists in pack root</li> <li>[ ] VERIFY.md exists in pack root (this file)</li> <li>[ ] package.json exists in pack root</li> </ul> <p>Verification commands:</p> <pre><code>cd /path/to/madeinoz-knowledge-system\nls -la README.md INSTALL.md VERIFY.md package.json\n</code></pre> <p>Expected result: All four files listed</p>"},{"location":"installation/verification.html#section-2-mcp-server-verification","title":"Section 2: MCP Server Verification","text":"<p>Verify the Graphiti MCP server is running and accessible.</p>"},{"location":"installation/verification.html#21-container-status","title":"2.1 Container Status","text":"<ul> <li>[ ] Containers are running</li> </ul> <p>Verification commands:</p> <pre><code># For Podman\npodman ps | grep madeinoz-knowledge\n\n# For Docker\ndocker ps | grep madeinoz-knowledge\n\n# Or use the status script\nbun run server-cli status\n</code></pre> <p>Expected result (FalkorDB backend):</p> <ul> <li>Containers <code>madeinoz-knowledge-graph-mcp</code> and <code>madeinoz-knowledge-falkordb</code> listed with status \"Up\"</li> </ul> <p>Expected result (Neo4j backend):</p> <ul> <li>Containers <code>madeinoz-knowledge-graph-mcp</code> and <code>madeinoz-knowledge-neo4j</code> listed with status \"Up\"</li> </ul>"},{"location":"installation/verification.html#22-mcp-health-endpoint-access","title":"2.2 MCP Health Endpoint Access","text":"<ul> <li>[ ] MCP health endpoint is accessible and returns healthy status</li> </ul> <p>Verification commands:</p> <pre><code>curl -s http://localhost:8000/health --max-time 2\n</code></pre> <p>Expected result: JSON response indicating healthy status:</p> <pre><code>{\"status\":\"healthy\",\"service\":\"graphiti-mcp\"}\n</code></pre>"},{"location":"installation/verification.html#section-3-pai-skill-verification","title":"Section 3: PAI Skill Verification","text":"<p>Verify the PAI skill is properly installed and formatted.</p>"},{"location":"installation/verification.html#31-skill-installation","title":"3.1 Skill Installation","text":"<ul> <li>[ ] Skill directory exists in PAI installation</li> </ul> <p>Verification commands:</p> <pre><code># Check standard location\nls -la ~/.claude/skills/Knowledge/\n\n# Or if using custom PAI_DIR\nls -la ${PAI_DIR:-$HOME/.claude}/skills/Knowledge/\n</code></pre> <p>Expected result: Directory exists with SKILL.md, STANDARDS.md, workflows/, tools/</p>"},{"location":"installation/verification.html#32-skillmd-frontmatter","title":"3.2 SKILL.md Frontmatter","text":"<ul> <li>[ ] SKILL.md has valid YAML frontmatter</li> <li>[ ] Frontmatter contains 'name' field</li> <li>[ ] Frontmatter contains 'description' field</li> <li>[ ] Description includes 'USE WHEN' clause</li> </ul> <p>Verification commands:</p> <pre><code>head -10 ~/.claude/skills/Knowledge/SKILL.md\n</code></pre> <p>Expected result: YAML frontmatter with name and description containing \"USE WHEN\"</p>"},{"location":"installation/verification.html#section-4-configuration-verification","title":"Section 4: Configuration Verification","text":"<p>Verify all configuration is correct.</p> <p>PAI .env is the ONLY source of truth.</p> <p>All MADEINOZ_KNOWLEDGE_* configuration lives in <code>${PAI_DIR}/.env</code>.</p>"},{"location":"installation/verification.html#41-pai-global-configuration","title":"4.1 PAI Global Configuration","text":"<p>Check PAI's global .env for required variables:</p> <ul> <li>[ ] API key is set (OPENAI_API_KEY or MADEINOZ_KNOWLEDGE_OPENAI_API_KEY)</li> <li>[ ] LLM provider is configured</li> </ul> <p>Verification commands:</p> <pre><code>PAI_ENV=\"${PAI_DIR:-$HOME/.claude}/.env\"\nif [ -f \"$PAI_ENV\" ]; then\n    echo \"Checking: $PAI_ENV\"\n    grep -E \"(OPENAI_API_KEY|MADEINOZ_KNOWLEDGE_)\" \"$PAI_ENV\" | grep -v \"^#\" | sed 's/=.*/=&lt;SET&gt;/'\nelse\n    echo \"PAI .env not found at: $PAI_ENV\"\nfi\n</code></pre> <p>Expected result: API key shows as SET (value hidden)</p>"},{"location":"installation/verification.html#42-mcp-server-configuration","title":"4.2 MCP Server Configuration","text":"<ul> <li>[ ] MCP server configured in ~/.claude.json</li> <li>[ ] madeinoz-knowledge server entry exists</li> <li>[ ] HTTP transport configured</li> </ul> <p>Verification commands:</p> <pre><code>if [ -f ~/.claude.json ]; then\n    grep -A 5 \"madeinoz-knowledge\" ~/.claude.json\nelse\n    echo \"~/.claude.json not found\"\nfi\n</code></pre> <p>Expected result:</p> <pre><code>\"madeinoz-knowledge\": {\n  \"type\": \"http\",\n  \"url\": \"http://localhost:8000/mcp\"\n}\n</code></pre>"},{"location":"installation/verification.html#section-5-end-to-end-functionality","title":"Section 5: End-to-End Functionality","text":"<p>FOR AI AGENTS: This is the CRITICAL verification section. It tests actual knowledge operations. ALL tests MUST pass for the installation to be considered complete.</p> <p>Verify the complete system works end-to-end using the actual MCP tools.</p>"},{"location":"installation/verification.html#51-knowledge-capture-add_memory","title":"5.1 Knowledge Capture (add_memory)","text":"<ul> <li>[ ] Can capture knowledge to graph</li> </ul> <p>Verification commands:</p> <pre><code>curl -s -X POST http://localhost:8000/mcp/ \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"jsonrpc\":\"2.0\",\n        \"id\":1,\n        \"method\":\"tools/call\",\n        \"params\":{\n            \"name\":\"add_memory\",\n            \"arguments\":{\n                \"name\":\"PAI Verification Test\",\n                \"episode_body\":\"Madeinoz Knowledge System verification test completed successfully.\",\n                \"source\":\"text\",\n                \"source_description\":\"verification test\"\n            }\n        }\n    }' | head -20\n</code></pre> <p>Expected result: JSON response with success indication, no errors</p>"},{"location":"installation/verification.html#52-knowledge-search-search_memory_nodes","title":"5.2 Knowledge Search (search_memory_nodes)","text":"<ul> <li>[ ] Can search knowledge graph nodes</li> </ul> <p>Verification commands:</p> <pre><code>curl -s -X POST http://localhost:8000/mcp/ \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"jsonrpc\":\"2.0\",\n        \"id\":2,\n        \"method\":\"tools/call\",\n        \"params\":{\n            \"name\":\"search_memory_nodes\",\n            \"arguments\":{\n                \"query\":\"Madeinoz Knowledge System\",\n                \"max_nodes\":5\n            }\n        }\n    }' | head -20\n</code></pre> <p>Expected result: JSON response with search results</p>"},{"location":"installation/verification.html#53-relationship-search-search_memory_facts","title":"5.3 Relationship Search (search_memory_facts)","text":"<ul> <li>[ ] Can search relationships/facts</li> </ul> <p>Verification commands:</p> <pre><code>curl -s -X POST http://localhost:8000/mcp/ \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"jsonrpc\":\"2.0\",\n        \"id\":3,\n        \"method\":\"tools/call\",\n        \"params\":{\n            \"name\":\"search_memory_facts\",\n            \"arguments\":{\n                \"query\":\"PAI\",\n                \"max_facts\":5\n            }\n        }\n    }' | head -20\n</code></pre> <p>Expected result: JSON response with facts/relationships</p>"},{"location":"installation/verification.html#section-6-integration-verification","title":"Section 6: Integration Verification","text":"<p>Verify integration with PAI system and Claude Code.</p>"},{"location":"installation/verification.html#61-pai-skill-recognition","title":"6.1 PAI Skill Recognition","text":"<ul> <li>[ ] Claude Code recognizes the skill</li> </ul> <p>Verification:</p> <ol> <li>Restart Claude Code</li> <li>In Claude Code, type: <code>What is the Madeinoz Knowledge System?</code></li> <li>Check if Claude mentions the skill</li> </ol> <p>Expected result: Claude is aware of the Madeinoz Knowledge System skill</p>"},{"location":"installation/verification.html#62-workflow-invocation","title":"6.2 Workflow Invocation","text":"<ul> <li>[ ] Workflows can be invoked via natural language</li> </ul> <p>Verification: In Claude Code, try each trigger phrase:</p> <ol> <li>\"Show the knowledge graph status\" \u2192 Should invoke GetStatus</li> <li>\"Remember that this is a test\" \u2192 Should invoke CaptureEpisode</li> <li>\"What do I know about PAI?\" \u2192 Should invoke SearchKnowledge</li> </ol> <p>Expected result: Claude Code follows the workflow instructions</p>"},{"location":"installation/verification.html#section-7-memory-sync-hook-verification","title":"Section 7: Memory Sync Hook Verification","text":"<p>Verify the memory sync hook is properly installed for syncing learnings and research to the knowledge graph.</p>"},{"location":"installation/verification.html#71-hook-files-installed","title":"7.1 Hook Files Installed","text":"<ul> <li>[ ] Hook scripts exist in PAI hooks directory</li> <li>[ ] Hook lib files exist</li> </ul> <p>Verification commands:</p> <pre><code>PAI_HOOKS=\"$HOME/.claude/hooks\"\nls -la \"$PAI_HOOKS/\"\nls -la \"$PAI_HOOKS/lib/\"\n</code></pre> <p>Expected result: sync-memory-to-knowledge.ts, sync-learning-realtime.ts, and lib/ directory with support files</p>"},{"location":"installation/verification.html#72-hooks-registered-in-settingsjson","title":"7.2 Hooks Registered in settings.json","text":"<ul> <li>[ ] SessionStart hook registered (sync-memory-to-knowledge.ts)</li> <li>[ ] Stop hook registered (sync-learning-realtime.ts)</li> <li>[ ] SubagentStop hook registered (sync-learning-realtime.ts)</li> </ul> <p>Verification commands:</p> <pre><code>SETTINGS=\"$HOME/.claude/settings.json\"\nif [ -f \"$SETTINGS\" ]; then\n    echo \"Checking hooks in settings.json...\"\n\n    if grep -q \"sync-memory-to-knowledge\" \"$SETTINGS\"; then\n        echo \"\u2713 SessionStart: sync-memory-to-knowledge.ts registered\"\n    else\n        echo \"\u2717 SessionStart: sync-memory-to-knowledge.ts NOT registered\"\n    fi\n\n    if grep -q '\"Stop\"' \"$SETTINGS\" &amp;&amp; grep -q \"sync-learning-realtime\" \"$SETTINGS\"; then\n        echo \"\u2713 Stop: sync-learning-realtime.ts registered\"\n    else\n        echo \"\u2717 Stop: sync-learning-realtime.ts NOT registered\"\n    fi\nfi\n</code></pre> <p>Expected result: All three hooks registered</p>"},{"location":"installation/verification.html#section-8-documentation-verification","title":"Section 8: Documentation Verification","text":"<p>Verify all documentation is complete and accurate.</p>"},{"location":"installation/verification.html#81-readmemd-completeness","title":"8.1 README.md Completeness","text":"<ul> <li>[ ] README.md has all required sections</li> <li>[ ] README.md has proper YAML frontmatter</li> </ul> <p>Verification commands:</p> <pre><code>grep \"^##\" README.md | head -20\nhead -35 README.md | grep \"^---\"\n</code></pre> <p>Expected result: All major sections listed, frontmatter present</p>"},{"location":"installation/verification.html#verification-summary","title":"Verification Summary","text":""},{"location":"installation/verification.html#pass-criteria","title":"Pass Criteria","text":"<p>For a successful installation, you must have:</p> <p>Critical (ALL must pass):</p> <ul> <li>Database backend detected (Section 0)</li> <li>All files in correct locations (Section 1)</li> <li>MCP server running and accessible (Section 2)</li> <li>PAI skill installed (Section 3)</li> <li>Configuration complete with valid API key (Section 4)</li> <li>End-to-end functionality working (Section 5)</li> <li>MCP configured in ~/.claude.json (Section 4.2)</li> </ul> <p>Important (at least 80% pass):</p> <ul> <li>Integration with Claude Code (Section 6)</li> <li>Memory sync hook installed (Section 7)</li> <li>Documentation complete (Section 8)</li> </ul>"},{"location":"installation/verification.html#final-verification","title":"Final Verification","text":"<p>Once all checks pass:</p> <ul> <li>[ ] Create a test episode in Claude Code: \"Remember that I've successfully installed the Madeinoz Knowledge System\"</li> <li>[ ] Search for it: \"What do I know about the Madeinoz Knowledge System installation?\"</li> <li>[ ] Verify it's returned: The search should find your test episode</li> </ul> <p>If all three steps work, your installation is complete and verified!</p> <p>Verification completed: ___</p> <p>Verified by: ___</p> <p>Database backend: [ ] FalkorDB / [ ] Neo4j</p> <p>Result: PASS / FAIL</p> <p>Next Steps:</p> <ul> <li>If PASS: Start using the Madeinoz Knowledge System!</li> <li>If FAIL: Review failed items, re-install as needed, and re-verify</li> </ul>"},{"location":"reference/benchmarks.html","title":"Model Benchmark Results","text":""},{"location":"reference/benchmarks.html#model-benchmark-results-madeinoz-knowledge-system","title":"Model Benchmark Results - MadeInOz Knowledge System","text":"<p>Last Updated: 2026-02-03 Database: Neo4j (neo4j:5.28.0) MCP Server: zepai/knowledge-graph-mcp:standalone Local Ollama Tests: NVIDIA RTX 4090 GPU (24GB VRAM)</p>"},{"location":"reference/benchmarks.html#executive-summary","title":"Executive Summary","text":"<p>Key Finding: Hybrid Architecture is Optimal</p> <p>The best configuration combines cloud LLM for entity extraction with local Ollama embeddings for search. This approach delivers cloud-quality accuracy with local speed and cost savings.</p>"},{"location":"reference/benchmarks.html#real-world-test-results","title":"Real-World Test Results","text":"<p>We tested 15 different models with actual MCP integration via Graphiti. The results were decisive:</p> <ul> <li>\u2705 6 models work with Graphiti's strict Pydantic schemas</li> <li>\u274c 9 models fail with validation errors or timeouts</li> <li>\ud83c\udfc6 Gemini 2.0 Flash is the best value - cheapest working model with best entity extraction</li> </ul> <p>Critical Discovery</p> <p>ALL open-source models (Llama 3.1 8B, Llama 3.3 70B, Mistral 7B, DeepSeek V3) FAIL in real MCP integration despite passing simple JSON tests. They produce Pydantic validation errors with Graphiti's entity/relationship schemas.</p>"},{"location":"reference/benchmarks.html#recommended-configurations","title":"Recommended Configurations","text":"Use Case LLM Embedding Cost/1K Ops Why This? FREE Trinity Large Preview MxBai (Ollama) $0 Completely free, passes all tests Best Value Gemini 2.0 Flash MxBai (Ollama) $0.125 Cheapest paid model, extracts 8 entities, 16.4s Most Reliable GPT-4o Mini MxBai (Ollama) $0.129 Production-proven, 7 entities, 18.4s Fastest GPT-4o MxBai (Ollama) $2.155 12.4s extraction, 6 entities Premium Claude 3.5 Haiku MxBai (Ollama) $0.816 7 entities, 24.7s <p>NEW: Free Trinity Model</p> <p><code>arcee-ai/trinity-large-preview:free</code> via OpenRouter is now available as a completely free option that passes all Graphiti entity extraction tests. See Free Cloud Models below for details.</p> <p>Hybrid Approach = Best Results</p> <p>Use cloud LLM (accurate entity extraction) + local Ollama embeddings (free, 9x faster). This combines the strengths of both approaches.</p>"},{"location":"reference/benchmarks.html#embedding-models-local-vs-cloud","title":"Embedding Models: Local vs Cloud","text":""},{"location":"reference/benchmarks.html#why-embeddings-matter","title":"Why Embeddings Matter","text":"<p>Embeddings power semantic search. Every time you search your knowledge graph, embeddings convert your query into a vector and find similar vectors in the database. Choose wisely - you cannot change models without re-indexing all data.</p>"},{"location":"reference/benchmarks.html#benchmark-results","title":"Benchmark Results","text":"<p>Tested for semantic similarity accuracy using 8 test pairs (5 similar, 3 dissimilar).</p> Rank Model Provider Quality Cost/1M Speed Dimensions 1 Embed 3 Small OpenRouter 78.2% $0.02 824ms 1536 2 Embed 3 Large OpenRouter 77.3% $0.13 863ms 3072 3 MxBai Embed Large \u2b50 Ollama 73.9% FREE 87ms 1024 4 Nomic Embed Text Ollama 63.5% FREE 93ms 768 5 Ada 002 OpenRouter 58.8% $0.10 801ms 1536 <p>\u2b50 Recommended: MxBai Embed Large via Ollama</p>"},{"location":"reference/benchmarks.html#key-insights","title":"Key Insights","text":"<p>MxBai Embed Large Wins</p> <ul> <li>Quality: 73.9% (only 4% lower than best paid model)</li> <li>Speed: 87ms (9x faster than cloud models)</li> <li>Cost: FREE (runs locally via Ollama)</li> <li>Dimensions: 1024 (good balance - not too large, not too small)</li> </ul> <p>When to Use Cloud Embeddings</p> <p>Use Embed 3 Small if you: - Don't have GPU/can't run Ollama locally - Need absolute best quality (78.2% vs 73.9%) - Don't mind 9x slower queries and $0.02/1M cost</p>"},{"location":"reference/benchmarks.html#critical-changing-embedding-models-breaks-everything","title":"\u26a0\ufe0f CRITICAL: Changing Embedding Models Breaks Everything","text":"<p>No Migration Path - Choose Once</p> <p>Switching embedding models requires re-indexing ALL data. Each model produces different vector dimensions:</p> Model Dimensions mxbai-embed-large 1024 nomic-embed-text 768 text-embedding-3-small 1536 text-embedding-3-large 3072 <p>Neo4j's vector search requires all vectors to have identical dimensions. If you index with Model A (768 dims) then switch to Model B (1024 dims), all searches fail with:</p> <pre><code>Invalid input for 'vector.similarity.cosine()':\nThe supplied vectors do not have the same number of dimensions\n</code></pre> <p>To switch models safely:</p> <ol> <li>Export important knowledge (manually note key facts)</li> <li>Clear the graph: Use <code>clear_graph</code> MCP tool</li> <li>Update config:</li> </ol> <pre><code>MADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=your-new-model\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=matching-dimension\n</code></pre> <ol> <li>Restart the server</li> <li>Re-add all knowledge</li> </ol> <p>Best Practice</p> <p>Choose <code>mxbai-embed-large</code> at installation and never change it. Best balance of quality (73.9%), speed (87ms), and cost (FREE).</p>"},{"location":"reference/benchmarks.html#llm-models-what-actually-works","title":"LLM Models: What Actually Works","text":""},{"location":"reference/benchmarks.html#real-life-mcp-integration-test-results","title":"Real-Life MCP Integration Test Results","text":"<p>We tested all 15 models with actual Graphiti integration, not just simple JSON extraction. The test used a complex business scenario requiring extraction of companies, people, locations, and relationships.</p> <p>Test Input:</p> <pre><code>\"During the Q4 planning meeting at TechCorp headquarters in Austin, CEO Sarah\nMartinez announced a strategic partnership with CloudBase Inc, brokered by\nMorgan Stanley. The deal includes cloud infrastructure migration and a\n200-person engineering team based in Seattle.\"\n</code></pre> <p>Expected Entities: TechCorp, Sarah Martinez, CloudBase Inc, Morgan Stanley, Austin, Seattle</p>"},{"location":"reference/benchmarks.html#working-models-815","title":"\u2705 Working Models (8/15)","text":"<p>These models successfully extract entities AND relationships, passing Graphiti's strict Pydantic validation:</p> Rank Model Cost/1K Entities Time Quality Score 0 Trinity Large Preview \ud83c\udd93 $0 6/5 ~25s \u2b50\u2b50\u2b50\u2b50 0a Trinity Mini \ud83c\udd93 $0 6/5 ~16s \u2b50\u2b50\u2b50\u2b50 1 Gemini 2.0 Flash \ud83c\udfc6 $0.125 8/5 16.4s \u2b50\u2b50\u2b50\u2b50\u2b50 2 Qwen 2.5 72B $0.126 8/5 30.8s \u2b50\u2b50\u2b50\u2b50 3 GPT-4o Mini $0.129 7/5 18.4s \u2b50\u2b50\u2b50\u2b50\u2b50 4 Claude 3.5 Haiku $0.816 7/5 24.7s \u2b50\u2b50\u2b50\u2b50 5 GPT-4o $2.155 6/5 12.4s \u2b50\u2b50\u2b50\u2b50\u2b50 6 Grok 3 $2.163 8/5 22.5s \u2b50\u2b50\u2b50 <p>FREE Trinity Models (New 2026-02-03)</p> <p>Both Trinity models are completely free and work correctly:</p> <p>Trinity Large Preview: - \u2705 Passes all Graphiti entity extraction tests - \u2705 No JSON validation errors - \u2705 Reliable entity and relationship extraction - \u2705 Available via OpenRouter free tier - \u26a0\ufe0f Slower processing (~25s) - \u2705 Extracts 6 entities per test</p> <p>Trinity Mini: - \u2705 Passes all Graphiti entity extraction tests - \u2705 No JSON validation errors - \u2705 Reliable entity and relationship extraction - \u2705 Available via OpenRouter free tier - \u26a1 Faster processing (~16s) - \u2705 Extracts 6 entities per test</p> <p>Best for: Cost-conscious users, testing, and development environments.</p> <p>Entity Count Explanation</p> <p>\"8/5\" means: Extracted 8 entities total (including extras beyond the 5 required). This shows the model identified additional relevant entities like \"cloud infrastructure\" or \"Q4 planning meeting\".</p>"},{"location":"reference/benchmarks.html#failed-models-915","title":"\u274c Failed Models (9/15)","text":"<p>These models DO NOT WORK with Graphiti despite passing simple JSON tests:</p> Model Cost/1K Error Type Why It Fails Llama 3.1 8B $0.0145 Pydantic validation Invalid ExtractedEdges schema Llama 3.3 70B $0.114 Processing timeout Cannot complete extraction Mistral 7B $0.0167 Pydantic validation Invalid ExtractedEntities schema DeepSeek V3 $0.0585 Pydantic validation Invalid ExtractedEntities schema Claude Sonnet 4 $4.215 Processing timeout Too slow for Graphiti Grok 4 Fast $0.280 Pydantic validation Invalid ExtractedEntities schema Grok 4.1 Fast $0.434 Processing timeout Cannot complete extraction Grok 3 Mini $0.560 Processing timeout Cannot complete extraction Grok 4 $11.842 Processing timeout Even most expensive Grok fails <p>Why Open-Source Models Fail</p> <p>Llama, Mistral, and DeepSeek models cannot produce JSON that matches Graphiti's strict Pydantic schemas. They work for simple JSON extraction but fail when integrated with the actual knowledge graph system. The \"cheap\" models listed in early benchmarks DO NOT WORK in production.</p>"},{"location":"reference/benchmarks.html#cost-vs-performance-vs-accuracy-matrix","title":"Cost vs Performance vs Accuracy Matrix","text":"Model Cost Speed Entities Best For Gemini 2.0 Flash \ud83d\udcb0 Cheapest \u26a1 Fast (16s) \ud83c\udfaf Most (8) RECOMMENDED GPT-4o Mini \ud83d\udcb0 Cheap \u26a1 Fast (18s) \ud83c\udfaf Good (7) Reliability Qwen 2.5 72B \ud83d\udcb0 Cheap \ud83d\udc0c Slow (31s) \ud83c\udfaf Most (8) Quality over speed Claude 3.5 Haiku \ud83d\udcb0\ud83d\udcb0 Mid \u26a1 Medium (25s) \ud83c\udfaf Good (7) Claude ecosystem GPT-4o \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0 Premium \u26a1\u26a1 Fastest (12s) \ud83c\udfaf Good (6) Speed critical Grok 3 \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0 Premium \u26a1 Medium (23s) \ud83c\udfaf Most (8) xAI ecosystem <p>Model Selection Guide</p> <ul> <li>Default choice: Gemini 2.0 Flash ($0.125/1K, 8 entities, fast)</li> <li>Need reliability: GPT-4o Mini ($0.129/1K, production-proven)</li> <li>Need speed: GPT-4o ($2.155/1K, 12s extraction)</li> <li>Already use Claude: Claude 3.5 Haiku ($0.816/1K)</li> <li>Already use xAI: Grok 3 only (all other Grok variants fail)</li> </ul>"},{"location":"reference/benchmarks.html#configuration-examples","title":"Configuration Examples","text":""},{"location":"reference/benchmarks.html#free-configuration-new-trinity-large-preview","title":"FREE Configuration (New - Trinity Large Preview)","text":"<p>Use Trinity Large Preview + MxBai for completely free knowledge graph operations:</p> <pre><code># LLM: Trinity Large Preview via OpenRouter (FREE)\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openrouter\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-or-v1-...\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_MODEL_NAME=arcee-ai/trinity-large-preview:free\n\n# Embeddings: MxBai via Ollama (local, FREE)\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=ollama\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://localhost:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>Cost: $0/1K operations + FREE embeddings = $0 total Performance: ~25s extraction, 87ms search Quality: 6 entities extracted, 73.9% embedding quality Best for: Development, testing, cost-conscious production</p> <p>Completely Free Knowledge Graph</p> <p>This configuration provides a fully functional knowledge graph system at zero cost. Trinity passes all Graphiti entity extraction tests with no JSON validation errors.</p>"},{"location":"reference/benchmarks.html#best-value-configuration-recommended","title":"Best Value Configuration (Recommended)","text":"<p>Use Gemini 2.0 Flash + MxBai Embed Large for optimal cost/performance:</p> <pre><code># LLM: Gemini 2.0 Flash via OpenRouter\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openrouter\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-or-v1-...\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_MODEL_NAME=google/gemini-2.0-flash-001\n\n# Embeddings: MxBai via Ollama (local)\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=ollama\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://localhost:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>Cost: $0.125/1K operations + FREE embeddings Performance: 16.4s extraction, 87ms search Quality: 8 entities extracted, 73.9% embedding quality</p>"},{"location":"reference/benchmarks.html#production-proven-configuration","title":"Production-Proven Configuration","text":"<p>Use GPT-4o Mini + MxBai for reliability:</p> <pre><code># LLM: GPT-4o Mini via OpenRouter\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openrouter\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-or-v1-...\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_MODEL_NAME=openai/gpt-4o-mini\n\n# Embeddings: MxBai via Ollama (local)\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=ollama\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://localhost:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>Cost: $0.129/1K operations + FREE embeddings Performance: 18.4s extraction, 87ms search Quality: 7 entities extracted, 73.9% embedding quality</p>"},{"location":"reference/benchmarks.html#speed-critical-configuration","title":"Speed-Critical Configuration","text":"<p>Use GPT-4o + MxBai when speed matters more than cost:</p> <pre><code># LLM: GPT-4o via OpenRouter\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openrouter\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-or-v1-...\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_MODEL_NAME=openai/gpt-4o\n\n# Embeddings: MxBai via Ollama (local)\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=ollama\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://localhost:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>Cost: $2.155/1K operations + FREE embeddings Performance: 12.4s extraction (fastest), 87ms search Quality: 6 entities extracted, 73.9% embedding quality</p>"},{"location":"reference/benchmarks.html#cloud-only-configuration","title":"Cloud-Only Configuration","text":"<p>Use GPT-4o Mini + Embed 3 Small if you can't run Ollama locally:</p> <pre><code># LLM: GPT-4o Mini via OpenRouter\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openrouter\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-or-v1-...\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_MODEL_NAME=openai/gpt-4o-mini\n\n# Embeddings: Embed 3 Small via OpenRouter\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=openrouter\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=openai/text-embedding-3-small\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1536\n</code></pre> <p>Cost: $0.129/1K + $0.02/1M embeddings Performance: 18.4s extraction, 824ms search (9x slower) Quality: 7 entities extracted, 78.2% embedding quality (4% better)</p>"},{"location":"reference/benchmarks.html#free-cloud-models-openrouter","title":"Free Cloud Models (OpenRouter)","text":"<p>Several free models on OpenRouter are available for cost-conscious users.</p>"},{"location":"reference/benchmarks.html#trinity-large-preview-recommended-free-option","title":"Trinity Large Preview (Recommended Free Option)","text":"<p>Model: <code>arcee-ai/trinity-large-preview:free</code></p> <p>Test Date: 2026-02-03</p> <p>Results:</p> Test Result Episode processing \u2705 PASS Entity extraction \u2705 PASS JSON validation \u2705 PASS Relationship extraction \u2705 PASS <p>Configuration:</p> <pre><code>MADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=arcee-ai/trinity-large-preview:free\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=your-openrouter-api-key\n</code></pre> <p>Pros: - Completely free - no LLM costs - Reliable entity extraction - No JSON validation errors - Works with OpenRouter's free tier</p> <p>Cons: - Slower than paid models (~25s vs ~16s) - Extracts fewer entities (6 vs 8 with Gemini) - May have rate limits on free tier</p> <p>Best for: Development, testing, and cost-conscious production use.</p>"},{"location":"reference/benchmarks.html#failing-free-models","title":"Failing Free Models","text":"Model Status Error <code>z-ai/glm-4.5-air:free</code> \u274c FAIL ValidationError: Invalid ExtractedEntities schema <p>Avoid GLM Models</p> <p>GLM models (<code>z-ai/glm-*</code>) fail consistently due to incompatible JSON output. Use Trinity or paid models instead.</p>"},{"location":"reference/benchmarks.html#cost-analysis","title":"Cost Analysis","text":""},{"location":"reference/benchmarks.html#monthly-cost-comparison-10000-operations","title":"Monthly Cost Comparison (10,000 operations)","text":"Configuration LLM Cost Embed Cost Total/Month Trinity Large + MxBai (FREE) $0 $0 $0 \ud83c\udd93 Trinity Mini + MxBai (FREE) $0 $0 $0 \ud83c\udd93 Gemini 2.0 Flash + MxBai (Best Value) $1.25 $0 $1.25 GPT-4o Mini + MxBai (Production) $1.29 $0 $1.29 GPT-4o + MxBai (Speed) $21.55 $0 $21.55 GPT-4o Mini + Embed 3 Small (Cloud-only) $1.29 $0.20 $1.49 <p>Zero-Cost Option Available</p> <p>The Trinity Large Preview configuration provides a completely free knowledge graph system. No LLM costs, no embedding costs, fully functional entity extraction.</p> <p>Cost Savings with Hybrid</p> <p>Using local Ollama embeddings saves $0.20/10K operations compared to cloud embeddings, while delivering 9x faster search queries.</p>"},{"location":"reference/benchmarks.html#what-you-actually-pay-for","title":"What You Actually Pay For","text":"<ul> <li>LLM calls: Every <code>add_memory</code> operation (entity/relationship extraction)</li> <li>Embedding calls: Every <code>add_memory</code> (encode episode) + every search query</li> <li>Database: FREE (self-hosted Neo4j or FalkorDB)</li> </ul> <p>Example monthly breakdown (1000 episodes added, 5000 searches):</p> <ul> <li>Gemini 2.0 Flash (1000 extractions): $0.125</li> <li>MxBai embeddings (1000 + 5000 operations): $0.00 (local)</li> <li>Total: $0.125/month</li> </ul>"},{"location":"reference/benchmarks.html#real-life-validation-tests","title":"Real-Life Validation Tests","text":""},{"location":"reference/benchmarks.html#test-1-business-entity-extraction","title":"Test 1: Business Entity Extraction","text":"<p>Input:</p> <pre><code>\"During the Q4 planning meeting, CEO Michael Chen announced that TechVentures\nInc will acquire DataFlow Systems for $500 million. The deal, brokered by\nGoldman Sachs, includes all patents and the 200-person engineering team based\nin Seattle.\"\n</code></pre> <p>Results with GPT-4o Mini:</p> <p>\u2705 Extracted Entities (verified in Neo4j):</p> <ul> <li>DataFlow Systems</li> <li>Goldman Sachs</li> <li>Michael Chen</li> <li>Seattle</li> <li>TechVentures Inc</li> </ul> <p>\u2705 Extracted Facts:</p> <ul> <li>\"The acquisition deal of DataFlow Systems was brokered by Goldman Sachs\"</li> <li>\"TechVentures Inc will acquire DataFlow Systems for $500 million\"</li> <li>\"DataFlow Systems has a 200-person engineering team based in Seattle\"</li> </ul> <p>Validation: 5/5 entities, 3 relationship facts extracted successfully</p>"},{"location":"reference/benchmarks.html#test-2-technical-team-context","title":"Test 2: Technical Team Context","text":"<p>Input:</p> <pre><code>\"Team uses TypeScript with Bun runtime. Sarah, our tech lead, chose Hono for\nthe HTTP framework because it's lightweight and fast.\"\n</code></pre> <p>Results with GPT-4o Mini:</p> <p>\u2705 Extracted Entities:</p> <ul> <li>TypeScript</li> <li>Bun</li> <li>Sarah</li> <li>Hono</li> <li>HTTP framework</li> </ul> <p>\u2705 Extracted Facts:</p> <ul> <li>\"The team uses TypeScript with Bun\"</li> <li>\"Hono is an HTTP framework\"</li> <li>\"Sarah chose Hono as the HTTP framework\"</li> </ul> <p>Validation: All entities and relationships captured correctly</p>"},{"location":"reference/benchmarks.html#test-3-mcp-operation-performance","title":"Test 3: MCP Operation Performance","text":"<p>Tested all MCP operations with real data:</p> Operation Success Rate Avg Time Results add_memory 100% (3/3) ~6ms All episodes queued search_nodes 100% (3/3) ~60ms 10 nodes per query search_memory_facts 100% (3/3) ~50ms 9 facts per query get_episodes 100% (1/1) ~5ms All episodes retrieved <p>Production Ready</p> <p>All MCP operations work reliably with the recommended Gemini 2.0 Flash + MxBai configuration.</p>"},{"location":"reference/benchmarks.html#technical-testing-details","title":"Technical Testing Details","text":""},{"location":"reference/benchmarks.html#test-environment","title":"Test Environment","text":"<ul> <li>Database: Neo4j 5.28.0 (docker)</li> <li>MCP Server: zepai/knowledge-graph-mcp:standalone</li> <li>Ollama: Running on NVIDIA RTX 4090 GPU (24GB VRAM)</li> <li>Network: Local Docker network for Neo4j, separate Ollama instance</li> </ul>"},{"location":"reference/benchmarks.html#test-scripts","title":"Test Scripts","text":"<ul> <li><code>test-all-llms-mcp.ts</code> - Comprehensive MCP test for 10 benchmark models</li> <li><code>test-grok-llms-mcp.ts</code> - Grok models MCP test (5 variants)</li> <li><code>test-search-debug.ts</code> - MCP integration validation script</li> </ul>"},{"location":"reference/benchmarks.html#methodology","title":"Methodology","text":"<ol> <li>Entity Extraction Test: Complex business scenario with 5+ entities</li> <li>Validation: Check Neo4j directly for extracted entities/relationships</li> <li>Schema Compliance: Verify Pydantic validation passes</li> <li>Timeout: 60s limit for extraction (production realistic)</li> <li>Success Criteria: All entities extracted + valid JSON schemas</li> </ol>"},{"location":"reference/benchmarks.html#conclusion","title":"Conclusion","text":""},{"location":"reference/benchmarks.html#key-takeaways","title":"Key Takeaways","text":"<p>What Works</p> <ol> <li>Hybrid architecture (cloud LLM + local embeddings) is optimal</li> <li>Trinity Large Preview is now available as a FREE option that passes all tests</li> <li>Gemini 2.0 Flash is the best paid value at $0.125/1K</li> <li>Gemini 2.0 Flash will be RETIRED March 2026 - switch to Trinity or GPT-4o-mini</li> <li>MxBai Embed Large via Ollama is best embedding choice (free, fast, good quality)</li> <li>9 models now work with Graphiti (including 2 free options)</li> </ol> <p>What Doesn't Work</p> <ol> <li>ALL open-source LLMs fail (Llama, Mistral, DeepSeek) - Pydantic validation errors</li> <li>Most Grok variants fail - Only Grok 3 works ($2.16/1K)</li> <li>\"Fast\" models fail - Speed optimizations break schema compliance</li> <li>Simple JSON tests lie - Real MCP integration is the only valid test</li> </ol> <p>Recommended Setup</p> <p>Start with Trinity Large Preview for free, or Gemini 2.0 Flash + MxBai Embed Large for best value:</p> <p>FREE Option (Trinity): - Costs $0/1K operations (completely free) - Extracts 6 entities (reliable) - ~25s extraction time - FREE, fast local embeddings (87ms searches) - Total cost: $0/month</p> <p>Best Value (Gemini): - Costs $0.125/1K operations (cheapest paid model) - Extracts 8 entities (best performance) - 16.4s extraction time (fast enough) - FREE, fast local embeddings (87ms searches) - Total cost: ~$1.25/month for 10K operations</p>"},{"location":"reference/benchmarks.html#migration-from-other-configs","title":"Migration from Other Configs","text":"<p>If you're currently using:</p> <ul> <li>GPT-4o Mini: Switch to Trinity (FREE) or Gemini 2.0 Flash (3% savings, 1 more entity extracted)</li> <li>Claude Sonnet 4: Switch to Trinity (FREE) or Gemini 2.0 Flash (97% savings, no timeouts)</li> <li>Llama/Mistral/DeepSeek: Switch to Trinity (FREE) or Gemini 2.0 Flash (these don't actually work)</li> <li>Any cloud embeddings: Switch to MxBai via Ollama (saves $0.02-$0.13/1M, 9x faster)</li> </ul> <p>Gemini Retirement Notice</p> <p>Gemini 2.0 Flash will be RETIRED in March 2026. Use Trinity (free) or GPT-4o-mini as alternatives.</p>"},{"location":"reference/benchmarks.html#future-considerations","title":"Future Considerations","text":"<ul> <li>Watch for Graphiti updates that might support open-source models</li> <li>Monitor Ollama for new embedding models with better quality</li> <li>Test new cloud models as they're released (especially cheaper options)</li> </ul> <p>The bottom line: Don't trust simple JSON benchmarks. Real MCP integration with Graphiti is the only valid test. Use this guide to choose models that actually work in production.</p>"},{"location":"reference/cache-implementation.html","title":"Cache Implementation Guide","text":""},{"location":"reference/cache-implementation.html#cache-implementation-guide","title":"Cache Implementation Guide","text":"<p>This document describes how the Gemini prompt caching system works internally. For configuration and usage, see Observability &amp; Metrics.</p>"},{"location":"reference/cache-implementation.html#architecture-overview","title":"Architecture Overview","text":""},{"location":"reference/cache-implementation.html#visual-architecture-diagrams","title":"Visual Architecture Diagrams","text":""},{"location":"reference/cache-implementation.html#request-flow-diagram","title":"Request Flow Diagram","text":"<p>LLM requests flow through the CachingLLMClient wrapper, which checks three conditions before applying cache_control markers and forwarding to OpenRouter.</p>"},{"location":"reference/cache-implementation.html#component-architecture","title":"Component Architecture","text":"<p>Core modules work together: environment configuration controls behavior, core components power the caching wrapper, which wraps the LLM client and exports metrics to Prometheus.</p>"},{"location":"reference/cache-implementation.html#decision-tree-when-is-caching-applied","title":"Decision Tree: When is Caching Applied?","text":"<p>Three checks must pass: caching enabled, Gemini model, and sufficient token count. If any check fails, the request bypasses caching.</p>"},{"location":"reference/cache-implementation.html#metrics-flow","title":"Metrics Flow","text":"<p>OpenRouter responses are parsed for cache metrics, calculated for savings and hit rates, recorded to SessionMetrics, and exported to Prometheus.</p>"},{"location":"reference/cache-implementation.html#complete-request-flow","title":"Complete Request Flow","text":"<p>The complete flow from LLM request through client routing, caching wrapper, preprocessing, API call, and postprocessing with metrics extraction.</p>"},{"location":"reference/cache-implementation.html#file-architecture","title":"File Architecture","text":"<p>All cache implementation files live in <code>docker/patches/</code> following Constitution Principle VII (Language Separation): Python code in <code>docker/</code>, TypeScript code in <code>src/</code>.</p> File Purpose Key Exports <code>factories.py</code> Client routing logic <code>LLMClientFactory.create()</code> <code>caching_wrapper.py</code> Patches OpenAI client <code>wrap_openai_client_for_caching()</code> <code>message_formatter.py</code> Adds cache_control markers <code>format_messages_for_caching()</code> <code>cache_metrics.py</code> Per-request metrics <code>CacheMetrics</code>, <code>PricingTier</code> <code>session_metrics.py</code> Session aggregation <code>SessionMetrics</code> <code>metrics_exporter.py</code> Prometheus export <code>CacheMetricsExporter</code> <code>caching_llm_client.py</code> Wrapper class <code>CachingLLMClient</code>"},{"location":"reference/cache-implementation.html#client-routing","title":"Client Routing","text":"<p>The critical design decision in this implementation: Gemini models on OpenRouter must use OpenAIGenericClient to route requests through <code>/chat/completions</code> instead of <code>/responses</code>.</p> <p>Location: <code>docker/patches/factories.py</code> (lines 305-343)</p> <pre><code># Simplified routing logic\nif _is_gemini_model(config.model) and 'openrouter' in provider_name.lower():\n    # Use OpenAIGenericClient (routes to /chat/completions)\n    client = OpenAIGenericClient(config=llm_config)\n    if _caching_wrapper_available:\n        client = wrap_openai_client_for_caching(client, config.model)\n    return client\n</code></pre>"},{"location":"reference/cache-implementation.html#why-this-matters","title":"Why This Matters","text":"Endpoint Multipart Format Caching Works Structured Output <code>/responses</code> Rejected No Yes <code>/chat/completions</code> Accepted Yes Yes (json_schema) <p>The <code>/responses</code> endpoint returns <code>\"expected string, received array\"</code> when it receives multipart messages with <code>cache_control</code> markers. The <code>/chat/completions</code> endpoint accepts both multipart format AND <code>json_schema</code> for structured outputs.</p>"},{"location":"reference/cache-implementation.html#message-formatting","title":"Message Formatting","text":"<p>Location: <code>docker/patches/message_formatter.py</code></p> <p>The formatter converts messages to multipart format and adds <code>cache_control</code> markers to the last message only.</p>"},{"location":"reference/cache-implementation.html#transformation-example","title":"Transformation Example","text":"<p>Input (standard format):</p> <pre><code>{\"role\": \"system\", \"content\": \"You are an AI assistant...\"}\n</code></pre> <p>Output (multipart with cache_control):</p> <pre><code>{\n    \"role\": \"system\",\n    \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"You are an AI assistant...\",\n            \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"}\n        }\n    ]\n}\n</code></pre>"},{"location":"reference/cache-implementation.html#why-mark-only-the-last-message","title":"Why Mark Only the Last Message?","text":"<p>The cache_control marker tells Gemini \"everything up to and including this point should be cached.\" By marking the last message:</p> <ol> <li>System prompts and context (earlier messages) become part of the cached prefix</li> <li>First request creates the cache (pays full price)</li> <li>Subsequent requests with the same prefix get 90% discount on cached tokens</li> </ol>"},{"location":"reference/cache-implementation.html#processing-functions","title":"Processing Functions","text":"Function Purpose <code>is_caching_enabled()</code> Checks <code>MADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED</code> <code>is_gemini_model(model)</code> Pattern matches \"google/gemini\" <code>is_cacheable_request(messages)</code> Heuristic: minimum ~1024 tokens <code>convert_to_multipart(message)</code> Converts string content to array format <code>add_cache_control_marker(message)</code> Adds <code>{\"type\": \"ephemeral\", \"ttl\": \"...\"}</code> <code>format_messages_for_caching(messages, model)</code> Main entry point"},{"location":"reference/cache-implementation.html#metrics-data-models","title":"Metrics Data Models","text":""},{"location":"reference/cache-implementation.html#cachemetrics-per-request","title":"CacheMetrics (Per-Request)","text":"<p>Location: <code>docker/patches/cache_metrics.py</code></p> <pre><code>@dataclass\nclass CacheMetrics:\n    cache_hit: bool           # Was cache used?\n    cached_tokens: int        # Tokens served from cache\n    prompt_tokens: int        # Total input tokens\n    completion_tokens: int    # Output tokens\n    tokens_saved: int         # Alias for cached_tokens\n    cost_without_cache: float # Hypothetical cost (USD)\n    actual_cost: float        # Real cost after discount\n    cost_saved: float         # Dollar savings\n    savings_percent: float    # Reduction percentage (0-100)\n    model: str                # Model identifier\n</code></pre>"},{"location":"reference/cache-implementation.html#cost-calculation","title":"Cost Calculation","text":"<pre><code># Cost without cache (all tokens at full rate)\ncost_without_cache = (prompt_tokens * input_rate) + (completion_tokens * output_rate)\n\n# Actual cost (cached tokens at 90% discount)\nuncached_tokens = prompt_tokens - cached_tokens\nactual_cost = (uncached_tokens * input_rate) + \\\n              (cached_tokens * cached_input_rate) + \\\n              (completion_tokens * output_rate)\n\n# Savings\ncost_saved = cost_without_cache - actual_cost\n</code></pre>"},{"location":"reference/cache-implementation.html#pricing-tiers","title":"Pricing Tiers","text":"Model Input (per 1M) Cached (90% off) Output (per 1M) gemini-2.0-flash-001 $0.10 $0.01 $0.40 gemini-2.5-flash $0.30 $0.03 $2.50 gemini-2.5-pro $1.25 $0.125 $10.00"},{"location":"reference/cache-implementation.html#sessionmetrics-aggregated","title":"SessionMetrics (Aggregated)","text":"<p>Location: <code>docker/patches/session_metrics.py</code></p> <p>Aggregates metrics across all requests in a session:</p> Property Type Description <code>total_requests</code> int Total API calls <code>cache_hits</code> int Requests with cache_hit=true <code>cache_misses</code> int Requests with cache_hit=false <code>total_cached_tokens</code> int Sum of all cached_tokens <code>total_cost_without_cache</code> float Cumulative hypothetical cost <code>total_actual_cost</code> float Cumulative real cost <code>total_cost_saved</code> float Total savings <code>cache_hit_rate</code> float Calculated percentage (0-100) <code>overall_savings_percent</code> float Overall cost reduction %"},{"location":"reference/cache-implementation.html#the-caching-wrapper","title":"The Caching Wrapper","text":"<p>Location: <code>docker/patches/caching_wrapper.py</code></p> <p>The wrapper monkey-patches <code>chat.completions.create()</code> to intercept requests and responses.</p>"},{"location":"reference/cache-implementation.html#request-processing","title":"Request Processing","text":"<pre><code>async def create_with_caching(*args, **kwargs):\n    # 1. PREPROCESSING: Format messages with cache_control\n    if 'messages' in kwargs:\n        kwargs['messages'] = format_messages_for_caching(\n            kwargs['messages'], model\n        )\n\n    # 2. TIMING: Measure request duration\n    start_time = time.monotonic()\n    response = await original_create(*args, **kwargs)\n    duration = time.monotonic() - start_time\n\n    # 3. POSTPROCESSING: Extract metrics\n    extract_and_record_metrics(response)\n\n    return response\n</code></pre>"},{"location":"reference/cache-implementation.html#response-metric-extraction","title":"Response Metric Extraction","text":"<p>The wrapper handles multiple response formats:</p> Format Source Fields OpenRouter (root-level) <code>response.native_tokens_cached</code> <code>native_tokens_cached</code>, <code>tokens_prompt</code>, <code>tokens_completion</code> OpenAI (usage object) <code>response.usage.cached_tokens</code> <code>cached_tokens</code>, <code>prompt_tokens</code>, <code>completion_tokens</code>"},{"location":"reference/cache-implementation.html#known-limitation-responsesparse","title":"Known Limitation: responses.parse()","text":"<p>The <code>responses.parse()</code> endpoint does not support multipart format:</p> <pre><code>Error: expected string, received array\n</code></pre> <p>Solution: Caching is automatically skipped for <code>responses.parse()</code> calls. Only <code>chat.completions.create()</code> supports caching.</p>"},{"location":"reference/cache-implementation.html#prometheus-integration","title":"Prometheus Integration","text":"<p>Location: <code>docker/patches/metrics_exporter.py</code></p>"},{"location":"reference/cache-implementation.html#initialization","title":"Initialization","text":"<p>The metrics exporter is initialized in <code>graphiti_mcp_server.py</code>:</p> <pre><code>if _metrics_exporter_available:\n    metrics_port = int(os.getenv(\"MADEINOZ_KNOWLEDGE_METRICS_PORT\", \"9090\"))\n    metrics_enabled = os.getenv(\"MADEINOZ_KNOWLEDGE_PROMPT_CACHE_METRICS_ENABLED\", \"true\")\n    initialize_metrics_exporter(enabled=metrics_enabled, port=metrics_port)\n</code></pre>"},{"location":"reference/cache-implementation.html#exposed-metrics","title":"Exposed Metrics","text":""},{"location":"reference/cache-implementation.html#counters-cumulative","title":"Counters (Cumulative)","text":"Metric Labels Description <code>graphiti_cache_hits_total</code> model Cache hit count <code>graphiti_cache_misses_total</code> model Cache miss count <code>graphiti_cache_tokens_saved_total</code> model Total cached tokens <code>graphiti_cache_cost_saved_total</code> model Total $ saved <code>graphiti_prompt_tokens_total</code> model Input tokens used <code>graphiti_completion_tokens_total</code> model Output tokens used <code>graphiti_api_cost_total</code> model Total API cost"},{"location":"reference/cache-implementation.html#gauges-current-value","title":"Gauges (Current Value)","text":"Metric Labels Description <code>graphiti_cache_hit_rate</code> model Current hit rate % <code>graphiti_cache_enabled</code> none 1=enabled, 0=disabled"},{"location":"reference/cache-implementation.html#histograms-distributions","title":"Histograms (Distributions)","text":"Metric Description <code>graphiti_prompt_tokens_per_request</code> Input token distribution <code>graphiti_api_cost_per_request</code> Cost per request <code>graphiti_llm_request_duration_seconds</code> Response latency"},{"location":"reference/cache-implementation.html#example-output","title":"Example Output","text":"<pre><code># Cache hit rate gauge\ngraphiti_cache_hit_rate{model=\"google/gemini-2.0-flash-001\"} 75.77\n\n# Cache hits counter\ngraphiti_cache_hits_total{model=\"google/gemini-2.0-flash-001\"} 1523\n\n# Cost savings counter\ngraphiti_cache_cost_saved_total{model=\"google/gemini-2.0-flash-001\"} 0.7584\n</code></pre>"},{"location":"reference/cache-implementation.html#configuration-reference","title":"Configuration Reference","text":"Variable Default Description <code>MADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED</code> <code>false</code> Enable/disable caching <code>MADEINOZ_KNOWLEDGE_PROMPT_CACHE_TTL</code> <code>1h</code> Cache TTL (<code>5m</code> or <code>1h</code>) <code>MADEINOZ_KNOWLEDGE_METRICS_PORT</code> <code>9090</code> Prometheus metrics port <code>MADEINOZ_KNOWLEDGE_PROMPT_CACHE_METRICS_ENABLED</code> <code>true</code> Enable metrics export <code>MADEINOZ_KNOWLEDGE_PROMPT_CACHE_LOG_REQUESTS</code> <code>false</code> Debug logging"},{"location":"reference/cache-implementation.html#testing","title":"Testing","text":"<p>Test files follow Constitution Principle VII (Language Separation):</p> Test File Location Purpose <code>test_cache_metrics.py</code> <code>docker/tests/unit/</code> Cache metrics extraction <code>test_session_metrics.py</code> <code>docker/tests/unit/</code> Session aggregation <code>test_caching_e2e.py</code> <code>docker/tests/integration/</code> End-to-end workflow <code>test_metrics_endpoint.py</code> <code>docker/tests/integration/</code> Prometheus endpoint"},{"location":"reference/cache-implementation.html#success-criteria","title":"Success Criteria","text":"Metric Target Token reduction from cache 40%+ Cache hit rate (after warmup) 60%+ Response time improvement 25%+ faster"},{"location":"reference/cache-implementation.html#quick-reference","title":"Quick Reference","text":""},{"location":"reference/cache-implementation.html#dependency-graph","title":"Dependency Graph","text":"<pre><code>factories.py\n    \u2192 wrap_openai_client_for_caching() [caching_wrapper.py]\n        \u2192 format_messages_for_caching() [message_formatter.py]\n        \u2192 extract_and_record_metrics()\n            \u2192 CacheMetrics.from_openrouter_response() [cache_metrics.py]\n            \u2192 SessionMetrics.record_request() [session_metrics.py]\n            \u2192 metrics_exporter.record_*() [metrics_exporter.py]\n</code></pre>"},{"location":"reference/cache-implementation.html#key-design-decisions","title":"Key Design Decisions","text":"Decision Rationale OpenAIGenericClient routing <code>/responses</code> rejects multipart; <code>/chat/completions</code> supports both Mark only last message System prompts become cached prefix Session-level metrics Track cumulative effectiveness Prometheus + OpenTelemetry Standard observability format 90% cached token discount Reflects Gemini's actual pricing 1-hour default TTL Balance hit rate vs freshness"},{"location":"reference/cli.html","title":"CLI Reference","text":""},{"location":"reference/cli.html#cli-reference","title":"CLI Reference","text":""},{"location":"reference/cli.html#overview","title":"Overview","text":"<p>The Madeinoz Knowledge System provides command-line tools for managing the knowledge graph server and performing knowledge operations.</p>"},{"location":"reference/cli.html#quick-reference","title":"Quick Reference","text":"Command Purpose Example <code>bun run server-cli start</code> Start MCP server containers <code>bun run server-cli start --dev</code> <code>bun run knowledge-cli.ts status</code> Show connection status <code>bun run knowledge-cli.ts status --profile production</code> <code>bun run knowledge-cli.ts list_profiles</code> List available profiles <code>bun run knowledge-cli.ts list_profiles</code> <code>bun run knowledge-cli.ts search_nodes</code> Search knowledge graph <code>bun run knowledge-cli.ts search_nodes \"query\" --profile remote</code>"},{"location":"reference/cli.html#server-management-commands","title":"Server Management Commands","text":""},{"location":"reference/cli.html#start-server","title":"Start Server","text":"<pre><code># Production mode\nbun run server-cli start\n\n# Development mode\nbun run server-cli start --dev\n</code></pre> <p>Launches the Graphiti MCP server with the configured database backend (Neo4j or FalkorDB).</p> <p>Options:</p> <ul> <li><code>--dev</code> or <code>-d</code>: Enable development mode (uses different ports and env files)</li> <li>Detects container runtime (Podman or Docker) automatically</li> <li>Loads configuration from PAI .env file</li> <li>Generates container environment files</li> <li>Waits for server initialization before returning</li> </ul> <p>Development Mode Differences:</p> Feature Production Development Neo4j Browser http://localhost:7474 http://localhost:7475 MCP Server http://localhost:8000/mcp/ http://localhost:8001/mcp/ Env Files <code>/tmp/madeinoz-knowledge-*.env</code> <code>/tmp/madeinoz-knowledge-*-dev.env</code> Use Case Production usage Code development/testing <p>Expected Output:</p> <pre><code>\u2713 Server is running and healthy!\n</code></pre>"},{"location":"reference/cli.html#restart-server","title":"Restart Server","text":"<pre><code># Production mode\nbun run server-cli restart\n\n# Development mode\nbun run server-cli restart --dev\n</code></pre> <p>Restarts the server containers while preserving data. Regenerates environment files before restarting to ensure configuration changes take effect.</p> <p>When to use: - After configuration changes - After code updates (development mode) - To refresh the server environment</p>"},{"location":"reference/cli.html#stop-server","title":"Stop Server","text":"<pre><code>bun run server-cli stop\n</code></pre> <p>Stops and removes the running containers.</p> <p>Note: Database data is persisted in Docker/Podman volumes and will be available when you restart.</p>"},{"location":"reference/cli.html#check-status","title":"Check Status","text":"<pre><code>bun run server-cli status\n</code></pre> <p>Displays current container status and server health.</p> <p>Output includes:</p> <ul> <li>Container runtime type (Podman/Docker)</li> <li>Database backend (Neo4j/FalkorDB)</li> <li>Container status (running/stopped)</li> <li>MCP server health check result</li> <li>Port availability and access URLs</li> </ul>"},{"location":"reference/cli.html#view-logs","title":"View Logs","text":"<pre><code>bun run server-cli logs\n</code></pre> <p>Streams logs from the running MCP server container.</p> <p>Options:</p> <ul> <li><code>--mcp</code>: Show only MCP server logs (not database)</li> <li><code>--db</code>: Show only database logs (not MCP server)</li> <li><code>--tail N</code>: Number of lines to show (default: 100)</li> <li><code>--no-follow</code>: Don't follow log output (show current logs and exit)</li> </ul> <p>Usage:</p> <pre><code># Follow all logs in real-time\nbun run server-cli logs\n\n# Show only MCP server logs\nbun run server-cli logs --mcp\n\n# Show last 50 lines and exit\nbun run server-cli logs --tail 50 --no-follow\n\n# Show only database logs\nbun run server-cli logs --db\n</code></pre>"},{"location":"reference/cli.html#knowledge-operations","title":"Knowledge Operations","text":""},{"location":"reference/cli.html#add-knowledge-capture","title":"Add Knowledge (Capture)","text":"<p>Via Claude Code skill:</p> <pre><code>Remember that [your knowledge here]\n</code></pre> <p>This triggers the CaptureEpisode workflow which:</p> <ol> <li>Sends content to MCP server</li> <li>Extracts entities and relationships</li> <li>Creates vector embeddings</li> <li>Stores in graph database with timestamp</li> </ol>"},{"location":"reference/cli.html#search-knowledge","title":"Search Knowledge","text":"<p>Via Claude Code skill:</p> <pre><code>What do I know about [topic]?\n</code></pre> <p>This triggers the SearchKnowledge workflow which:</p> <ol> <li>Converts query to vector embedding</li> <li>Searches graph for semantically similar entities</li> <li>Returns entities with summaries and facts</li> </ol>"},{"location":"reference/cli.html#find-relationships","title":"Find Relationships","text":"<p>Via Claude Code skill:</p> <pre><code>How are [entity A] and [entity B] related?\n</code></pre> <p>This triggers the SearchFacts workflow which:</p> <ol> <li>Traverses edges between entities in graph</li> <li>Returns direct relationships</li> <li>Shows temporal context of relationships</li> </ol>"},{"location":"reference/cli.html#view-recent-knowledge","title":"View Recent Knowledge","text":"<p>Via Claude Code skill:</p> <pre><code>What did I learn recently?\n</code></pre> <p>This triggers the GetRecent workflow which:</p> <ol> <li>Queries graph for recent episodes</li> <li>Returns chronological list</li> <li>Shows timestamps and summaries</li> </ol>"},{"location":"reference/cli.html#check-system-status","title":"Check System Status","text":"<p>Via Claude Code skill:</p> <pre><code>Show the knowledge graph status\n</code></pre> <p>This triggers the GetStatus workflow which:</p> <ol> <li>Connects to MCP server</li> <li>Returns entity count, episode count, last update</li> <li>Shows database health</li> </ol>"},{"location":"reference/cli.html#clear-knowledge","title":"Clear Knowledge","text":"<p>Via Claude Code skill:</p> <pre><code>Clear my knowledge graph\n</code></pre> <p>This triggers the ClearGraph workflow which:</p> <ol> <li>Confirms destructive action</li> <li>Deletes all entities and relationships</li> <li>Rebuilds indices</li> </ol>"},{"location":"reference/cli.html#knowledge-cli-token-efficient-wrapper","title":"Knowledge CLI (Token-Efficient Wrapper)","text":"<p>The <code>knowledge</code> CLI provides a token-efficient wrapper around MCP operations with compact output formatting and metrics tracking. Achieves 25-35% token savings through intelligent formatting.</p>"},{"location":"reference/cli.html#syntax","title":"Syntax","text":"<p>Run from the project directory:</p> <pre><code>bun run tools/knowledge-cli.ts &lt;command&gt; [args...] [options]\n</code></pre> <p>Note: All CLI commands should be run from the installed skill directory (<code>~/.claude/skills/Knowledge/</code>) where tools are available at <code>tools/knowledge-cli.ts</code>.</p>"},{"location":"reference/cli.html#commands","title":"Commands","text":""},{"location":"reference/cli.html#add_episode","title":"add_episode","text":"<p>Add knowledge to the graph:</p> <pre><code>bun run tools/knowledge-cli.ts add_episode \"Episode title\" \"Episode body content\"\n</code></pre> <p>With optional source description:</p> <pre><code>bun run tools/knowledge-cli.ts add_episode \"CTI Research\" \"Analysis of threat actor\" \"osint-recon\"\n</code></pre> <p>Arguments:</p> <ul> <li><code>&lt;title&gt;</code> (required): Episode title</li> <li><code>&lt;body&gt;</code> (required): Episode content</li> <li><code>[source_description]</code> (optional): Source identifier (e.g., 'user-input', 'api-import', 'osint-recon')</li> </ul>"},{"location":"reference/cli.html#search_nodes","title":"search_nodes","text":"<p>Search for entities in the knowledge graph:</p> <pre><code>bun run tools/knowledge-cli.ts search_nodes \"container orchestration\"\n</code></pre> <p>Limit results:</p> <pre><code>bun run tools/knowledge-cli.ts search_nodes \"container orchestration\" 10\n</code></pre> <p>Arguments:</p> <ul> <li><code>&lt;query&gt;</code> (required): Search query</li> <li><code>[limit]</code> (optional): Max results (default: 5)</li> </ul> <p>Output format:</p> <ul> <li>Compact format showing entity name, type, and summary</li> <li>~30% token savings vs raw MCP output</li> </ul>"},{"location":"reference/cli.html#search_facts","title":"search_facts","text":"<p>Find relationships between entities:</p> <pre><code>bun run tools/knowledge-cli.ts search_facts \"Podman\"\n</code></pre> <p>Limit facts returned:</p> <pre><code>bun run tools/knowledge-cli.ts search_facts \"Podman\" 10\n</code></pre> <p>Arguments:</p> <ul> <li><code>&lt;query&gt;</code> (required): Entity name or search query</li> <li><code>[limit]</code> (optional): Max facts (default: 5)</li> </ul> <p>Output format:</p> <ul> <li>Shows relationship text and type</li> <li>~30% token savings vs raw MCP output</li> </ul>"},{"location":"reference/cli.html#get_episodes","title":"get_episodes","text":"<p>Retrieve recent episodes from the knowledge graph:</p> <pre><code>bun run tools/knowledge-cli.ts get_episodes\n</code></pre> <p>Limit number of episodes:</p> <pre><code>bun run tools/knowledge-cli.ts get_episodes 10\n</code></pre> <p>Arguments:</p> <ul> <li><code>[limit]</code> (optional): Max episodes to retrieve (default: 5)</li> </ul> <p>Output format:</p> <ul> <li>Shows episode content and timestamp</li> <li>~30% token savings vs raw MCP output</li> </ul>"},{"location":"reference/cli.html#get_status","title":"get_status","text":"<p>Get knowledge graph status and health:</p> <pre><code>bun run tools/knowledge-cli.ts get_status\n</code></pre> <p>Output includes:</p> <ul> <li>Entity count</li> <li>Episode count</li> <li>Last update timestamp</li> <li>Database health status</li> </ul>"},{"location":"reference/cli.html#clear_graph","title":"clear_graph","text":"<p>Delete all knowledge from the graph (destructive operation):</p> <pre><code>bun run tools/knowledge-cli.ts clear_graph --force\n</code></pre> <p>Safety:</p> <ul> <li>Requires <code>--force</code> flag to confirm</li> <li>Deletes ALL entities, relationships, and episodes</li> <li>Cannot be undone</li> </ul>"},{"location":"reference/cli.html#health","title":"health","text":"<p>Check MCP server health:</p> <pre><code>bun run tools/knowledge-cli.ts health\n</code></pre> <p>Output:</p> <ul> <li>Server status</li> <li>Connection state</li> <li>Response time</li> </ul>"},{"location":"reference/cli.html#options","title":"Options","text":"<p>All commands support the following flags:</p> <pre><code>--raw              # Output raw JSON instead of compact format\n--metrics          # Display token metrics after operation\n--metrics-file &lt;p&gt; # Write metrics to JSONL file\n--since &lt;date&gt;     # Filter results created after this date\n--until &lt;date&gt;     # Filter results created before this date\n-h, --help         # Show help message\n</code></pre> <p>Temporal Filter Options:</p> <p>The <code>--since</code> and <code>--until</code> flags enable date-based filtering for <code>search_nodes</code> and <code>search_facts</code> commands.</p> <p>Supported date formats:</p> Format Example Description <code>today</code> <code>--since today</code> Start of current day (UTC) <code>yesterday</code> <code>--since yesterday</code> Start of previous day (UTC) <code>Nd</code> <code>--since 7d</code> N days ago <code>Nw</code> <code>--since 2w</code> N weeks ago <code>Nm</code> <code>--since 1m</code> N months ago (30 days per month) ISO 8601 <code>--since 2026-01-15</code> Specific date ISO 8601 <code>--since 2026-01-15T14:30:00Z</code> Specific datetime"},{"location":"reference/cli.html#examples","title":"Examples","text":"<p>Add knowledge with metrics:</p> <pre><code>bun run tools/knowledge-cli.ts add_episode \\\n  \"Test Episode\" \\\n  \"This is a test episode\" \\\n  --metrics\n</code></pre> <p>Search with raw JSON output:</p> <pre><code>bun run tools/knowledge-cli.ts search_nodes \"PAI\" --raw\n</code></pre> <p>Search with metrics logging:</p> <pre><code>bun run tools/knowledge-cli.ts search_nodes \"PAI\" 10 \\\n  --metrics-file ~/.madeinoz-knowledge/metrics.jsonl\n</code></pre> <p>Get status and track metrics:</p> <pre><code>bun run tools/knowledge-cli.ts get_status --metrics\n</code></pre>"},{"location":"reference/cli.html#temporal-search-examples","title":"Temporal Search Examples","text":"<p>Search for today's knowledge:</p> <pre><code>bun run tools/knowledge-cli.ts search_nodes \"PAI\" --since today\n</code></pre> <p>Search from the last 7 days:</p> <pre><code>bun run tools/knowledge-cli.ts search_facts \"decisions\" --since 7d\n</code></pre> <p>Search within a specific date range:</p> <pre><code>bun run tools/knowledge-cli.ts search_nodes \"project\" --since 2026-01-01 --until 2026-01-15\n</code></pre> <p>Search yesterday's knowledge:</p> <pre><code>bun run tools/knowledge-cli.ts search_nodes \"learning\" --since yesterday --until today\n</code></pre> <p>Search from last month:</p> <pre><code>bun run tools/knowledge-cli.ts search_facts \"architecture\" --since 1m\n</code></pre> <p>Combine temporal filters with other options:</p> <pre><code>bun run tools/knowledge-cli.ts search_nodes \"AI\" 20 --since 7d --metrics\n</code></pre>"},{"location":"reference/cli.html#environment-variables","title":"Environment Variables","text":"<p>The Knowledge CLI respects the following environment variables for customization:</p> <pre><code># Disable compact output (use raw JSON by default)\nexport MADEINOZ_WRAPPER_COMPACT=false\n\n# Enable metrics collection by default\nexport MADEINOZ_WRAPPER_METRICS=true\n\n# Default metrics file path\nexport MADEINOZ_WRAPPER_METRICS_FILE=~/.madeinoz-knowledge/metrics.jsonl\n\n# Error log path for transformation issues\nexport MADEINOZ_WRAPPER_LOG_FILE=~/.madeinoz-knowledge/errors.log\n\n# Slow processing threshold in milliseconds (default: 50)\nexport MADEINOZ_WRAPPER_SLOW_THRESHOLD=50\n\n# Processing timeout in milliseconds (default: 100)\nexport MADEINOZ_WRAPPER_TIMEOUT=100\n</code></pre>"},{"location":"reference/cli.html#metrics-tracking","title":"Metrics Tracking","text":"<p>When <code>--metrics</code> flag is enabled or <code>MADEINOZ_WRAPPER_METRICS=true</code>, the CLI displays token usage statistics:</p> <pre><code>bun run tools/knowledge-cli.ts search_nodes \"AI models\" --metrics\n</code></pre> <p>Metrics output includes:</p> <ul> <li>Operation name</li> <li>Raw size (bytes and estimated tokens)</li> <li>Compact size (bytes and estimated tokens)</li> <li>Savings percentage</li> <li>Tokens saved</li> <li>Processing time (milliseconds)</li> </ul> <p>Example output:</p> <pre><code>--- Token Metrics ---\nOperation: search_nodes\nRaw size: 12,345 bytes (3,086 est. tokens)\nCompact size: 8,234 bytes (2,059 est. tokens)\nSavings: 33.3% (1,027 tokens saved)\nProcessing time: 42ms\n</code></pre>"},{"location":"reference/cli.html#metrics-file-format","title":"Metrics File Format","text":"<p>When using <code>--metrics-file</code>, metrics are written as JSONL (one JSON object per line):</p> <pre><code>{\"operation\":\"search_nodes\",\"timestamp\":\"2025-01-19T12:34:56.789Z\",\"rawBytes\":12345,\"compactBytes\":8234,\"savingsPercent\":33.3,\"estimatedTokensBefore\":3086,\"estimatedTokensAfter\":2059,\"processingTimeMs\":42}\n{\"operation\":\"get_status\",\"timestamp\":\"2025-01-19T12:35:12.345Z\",\"rawBytes\":567,\"compactBytes\":234,\"savingsPercent\":58.7,\"estimatedTokensBefore\":142,\"estimatedTokensAfter\":59,\"processingTimeMs\":15}\n</code></pre> <p>This format is ideal for:</p> <ul> <li>Time-series analysis</li> <li>Performance monitoring</li> <li>Cost tracking</li> <li>Optimization validation</li> </ul>"},{"location":"reference/cli.html#remote-access-cli-options","title":"Remote Access CLI Options","text":"<p>The Knowledge CLI supports remote MCP access via connection profiles and CLI flags. See Remote Access Documentation for complete details.</p>"},{"location":"reference/cli.html#connection-profiles","title":"Connection Profiles","text":"<p>Use predefined connection profiles for different environments:</p> <pre><code># List all available profiles\nbun run knowledge-cli.ts list_profiles\n\n# Use specific profile\nbun run knowledge-cli.ts search_nodes \"query\" --profile production\n\n# Show current connection status\nbun run knowledge-cli.ts status\n</code></pre> <p>Profile Configuration Priority:</p> <ol> <li>CLI flags (<code>--host</code>, <code>--port</code>, <code>--protocol</code>)</li> <li>Individual environment variables (<code>MADEINOZ_KNOWLEDGE_HOST</code>)</li> <li>Selected profile (<code>--profile</code> or <code>MADEINOZ_KNOWLEDGE_PROFILE</code>)</li> <li>Default profile in YAML file</li> <li>Code defaults (localhost:8001, http)</li> </ol>"},{"location":"reference/cli.html#cli-flags","title":"CLI Flags","text":"<p>All commands support these remote access flags:</p> Flag Description Example <code>--profile &lt;name&gt;</code> Use specific connection profile <code>--profile production</code> <code>--host &lt;hostname&gt;</code> Override profile host <code>--host knowledge.example.com</code> <code>--port &lt;port&gt;</code> Override profile port <code>--port 443</code> <code>--protocol &lt;proto&gt;</code> Override protocol (http/https) <code>--protocol https</code>"},{"location":"reference/cli.html#environment-variable-overrides","title":"Environment Variable Overrides","text":"<p>Override any profile setting via environment variables:</p> Variable Description Default Example <code>MADEINOZ_KNOWLEDGE_PROFILE</code> Profile name to use <code>default</code> <code>production</code> <code>MADEINOZ_KNOWLEDGE_HOST</code> Server hostname or IP <code>localhost</code> <code>knowledge.example.com</code> <code>MADEINOZ_KNOWLEDGE_PORT</code> Server port <code>8001</code> <code>443</code> <code>MADEINOZ_KNOWLEDGE_PROTOCOL</code> Connection protocol <code>http</code> <code>https</code> <code>MADEINOZ_KNOWLEDGE_TLS_VERIFY</code> Verify TLS certificates <code>true</code> <code>false</code> <code>MADEINOZ_KNOWLEDGE_TLS_CA</code> CA certificate path - <code>/etc/ssl/certs/ca.pem</code> <code>MADEINOZ_KNOWLEDGE_TIMEOUT</code> Connection timeout (ms) <code>30000</code> <code>60000</code> <p>Example usage:</p> <pre><code># Use environment variables for remote connection\nexport MADEINOZ_KNOWLEDGE_HOST=192.168.1.100\nexport MADEINOZ_KNOWLEDGE_PORT=8001\nbun run knowledge-cli.ts search_nodes \"my query\"\n\n# Combine profile with host override\nbun run knowledge-cli.ts get_status --profile production --host backup.example.com\n</code></pre>"},{"location":"reference/cli.html#profile-configuration-file","title":"Profile Configuration File","text":"<p>Connection profiles are stored in:</p> <pre><code># Priority location\n$PAI_DIR/config/knowledge-profiles.yaml\n\n# Fallback location\n~/.claude/config/knowledge-profiles.yaml\n</code></pre> <p>Example profile configuration:</p> <pre><code>version: \"1.0\"\ndefault_profile: default\n\nprofiles:\n  default:\n    host: localhost\n    port: 8001\n    protocol: http\n\n  production:\n    host: knowledge.example.com\n    port: 443\n    protocol: https\n    tls:\n      verify: true\n      minVersion: TLSv1.3\n\n  development:\n    host: 192.168.1.100\n    port: 8001\n    protocol: http\n</code></pre>"},{"location":"reference/cli.html#new-commands-feature-010","title":"New Commands (Feature 010)","text":""},{"location":"reference/cli.html#status","title":"status","text":"<p>Show connection status and active profile:</p> <pre><code>bun run knowledge-cli.ts status\n</code></pre> <p>Output includes:</p> <ul> <li>Active profile name</li> <li>Connection status (connected/disconnected/error)</li> <li>Connected host and port</li> <li>Protocol (http/https)</li> <li>Server version (if available)</li> <li>Last connection time</li> </ul> <pre><code>$ bun run knowledge-cli.ts status --profile production\n\nKnowledge CLI Connection Status\n================================\nProfile: production\nStatus: connected\nHost: knowledge.example.com\nPort: 443\nProtocol: https\nServer Version: 1.7.0\nLast Connected: 2026-01-30T12:34:56Z\n</code></pre>"},{"location":"reference/cli.html#list_profiles","title":"list_profiles","text":"<p>List all available connection profiles:</p> <pre><code>bun run knowledge-cli.ts list_profiles\n</code></pre> <p>Output includes:</p> <ul> <li>All profile names from configuration</li> <li>Default profile indicator</li> <li>Configuration file path</li> </ul> <pre><code>$ bun run knowledge-cli.ts list_profiles\n\nAvailable Profiles\n==================\nConfiguration: ~/.claude/config/knowledge-profiles.yaml\nDefault Profile: default\n\nProfiles:\n  - default (localhost:8001, http)\n  - production (knowledge.example.com:443, https)\n  - development (192.168.1.100:8001, http)\n</code></pre>"},{"location":"reference/cli.html#interactive-installation","title":"Interactive Installation","text":"<pre><code>cd ~/.claude/skills/Knowledge\nbun run tools/install.ts\n</code></pre> <p>Guides through:</p> <ol> <li>System analysis and conflict detection</li> <li>LLM provider selection</li> <li>API key configuration</li> <li>Database backend selection</li> <li>Service startup</li> </ol> <p>Options:</p> <ul> <li><code>--yes</code> / <code>-y</code>: Non-interactive mode with defaults</li> <li><code>--update</code> / <code>-u</code>: Update existing installation</li> </ul>"},{"location":"reference/cli.html#utilities","title":"Utilities","text":""},{"location":"reference/cli.html#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:8000/health\n</code></pre> <p>Returns server health status as JSON:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"service\": \"graphiti-mcp\",\n  \"patch\": \"madeinoz-all-groups-enabled\"\n}\n</code></pre>"},{"location":"reference/cli.html#mcp-endpoint","title":"MCP Endpoint","text":"<pre><code>curl http://localhost:8000/mcp\n</code></pre> <p>SSE endpoint for MCP protocol communication.</p>"},{"location":"reference/cli.html#environment-variables_1","title":"Environment Variables","text":"<p>All CLI commands read from PAI configuration:</p> <pre><code># Set PAI directory (defaults to ~/.claude)\nexport PAI_DIR=/path/to/pai\n\n# Set database type\nexport MADEINOZ_KNOWLEDGE_DATABASE_TYPE=neo4j\n\n# Set logging level\nexport LOG_LEVEL=debug\n</code></pre>"},{"location":"reference/cli.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/cli.html#port-already-in-use","title":"\"Port already in use\"","text":"<pre><code># Find process using port 8000\nlsof -i :8000\n\n# Kill process if needed\nkill -9 [PID]\n\n# Or use different port by modifying the Docker Compose files\n</code></pre>"},{"location":"reference/cli.html#container-not-found","title":"\"Container not found\"","text":"<pre><code># List all containers\npodman ps -a\n# or: docker ps -a\n\n# Start services\nbun run server-cli start\n</code></pre>"},{"location":"reference/cli.html#health-check-failed","title":"\"Health check failed\"","text":"<pre><code># Check logs\nbun run server-cli logs\n\n# Verify server is responding\ncurl --max-time 5 http://localhost:8000/health\n</code></pre>"},{"location":"reference/cli.html#connection-refused","title":"\"Connection refused\"","text":"<pre><code># Verify MCP server endpoint\ncurl http://localhost:8000/health\n\n# If unavailable, start server\nbun run server-cli start\n</code></pre>"},{"location":"reference/cli.html#configuration-files","title":"Configuration Files","text":""},{"location":"reference/cli.html#pai-environment-file","title":"PAI Environment File","text":"<p>Location: <code>~/.claude/.env</code> (or <code>$PAI_DIR/.env</code>)</p> <p>Contains all Madeinoz Knowledge System configuration:</p> <ul> <li>API keys</li> <li>LLM provider settings</li> <li>Database backend configuration</li> <li>Performance tuning options</li> </ul>"},{"location":"reference/cli.html#docker-compose-files","title":"Docker Compose Files","text":"<ul> <li><code>docker/docker-compose-falkordb.yml</code> - FalkorDB backend (Docker)</li> <li><code>docker/docker-compose-neo4j.yml</code> - Neo4j backend (Docker)</li> <li><code>docker/podman-compose-falkordb.yml</code> - FalkorDB backend (Podman)</li> <li><code>docker/podman-compose-neo4j.yml</code> - Neo4j backend (Podman)</li> </ul>"},{"location":"reference/cli.html#configuration-template","title":"Configuration Template","text":"<p>Reference file: <code>config/.env.example</code></p> <p>Complete example of all available configuration options.</p>"},{"location":"reference/cli.html#related-commands","title":"Related Commands","text":"<pre><code># View all available skills (from installed location)\nls -la ~/.claude/skills/Knowledge/workflows/\n\n# Test LLM connectivity\nbun run tools/install.ts\n\n# Export knowledge data\n# (Via MCP: get_episodes with limit=999999)\n\n# Monitor memory sync\nbun run ~/.claude/hooks/sync-memory-to-knowledge.ts --verbose\n</code></pre>"},{"location":"reference/configuration.html","title":"Configuration Reference","text":""},{"location":"reference/configuration.html#configuration-reference","title":"Configuration Reference","text":""},{"location":"reference/configuration.html#overview","title":"Overview","text":"<p>All Knowledge System configuration is managed through your PAI environment file. This document describes every available configuration option.</p>"},{"location":"reference/configuration.html#configuration-location","title":"Configuration Location","text":"<p>Primary: <code>~/.claude/.env</code> (or <code>$PAI_DIR/.env</code>)</p> <p>This is the single source of truth for all Madeinoz Knowledge System settings. The <code>config/.env.example</code> file in the pack is a reference template only.</p>"},{"location":"reference/configuration.html#configuration-file-format","title":"Configuration File Format","text":"<p>The configuration file uses standard shell environment variable syntax:</p> <pre><code># Comments start with #\nVARIABLE_NAME=value\nMULTI_LINE_VALUE=\"can span lines with quotes\"\nNUMERIC_VALUE=123\nBOOLEAN_VALUE=true\n</code></pre>"},{"location":"reference/configuration.html#llm-provider-configuration","title":"LLM Provider Configuration","text":""},{"location":"reference/configuration.html#provider-selection","title":"Provider Selection","text":"<pre><code>MADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\n</code></pre> <p>Valid values:</p> <ul> <li><code>openai</code> - OpenAI API or OpenAI-compatible providers (OpenRouter, Together, etc.)</li> <li><code>anthropic</code> - Anthropic Claude API</li> <li><code>gemini</code> - Google Gemini API</li> <li><code>groq</code> - Groq API</li> <li><code>ollama</code> - Local Ollama server or Ollama-compatible provider (apikey required)</li> </ul>"},{"location":"reference/configuration.html#model-selection","title":"Model Selection","text":"<pre><code>MADEINOZ_KNOWLEDGE_MODEL_NAME=gpt-4o-mini\n</code></pre> <p>Recommended models:</p> <ul> <li><code>arcee-ai/trinity-large-preview:free</code> (OpenRouter) - FREE model that passes all tests</li> <li><code>arcee-ai/trinity-mini:free</code> (OpenRouter) - FREE alternative that passes all tests</li> <li><code>gpt-4o-mini</code> (OpenAI) - Best balance of cost and quality</li> <li><code>google/gemini-2.0-flash-001</code> (OpenRouter) - Fast, reliable, NOT being retired</li> <li><code>openai/gpt-4o</code> (OpenRouter) - Fastest extraction</li> <li><code>meta-llama/llama-3.1-8b-instruct</code> (OpenRouter) - Lowest cost (may have validation issues)</li> </ul> <p>Gemini Retirement Notice</p> <p><code>google/gemini-2.0-flash-001</code> will be retired in March 2026. Switch to either Trinity model as free alternatives: - <code>arcee-ai/trinity-large-preview:free</code> (recommended) - <code>arcee-ai/trinity-mini:free</code> (faster)</p> <p>Free Trinity Models</p> <p>Both Trinity models are free via OpenRouter and successfully pass all Graphiti entity extraction tests: - <code>arcee-ai/trinity-large-preview:free</code> - Larger model, more detailed extraction - <code>arcee-ai/trinity-mini:free</code> - Smaller model, faster processing (~16s)</p> <p>These are excellent options for cost-conscious users, especially with Gemini retiring in March 2026.</p>"},{"location":"reference/configuration.html#api-keys","title":"API Keys","text":"<pre><code>MADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-your-openai-key-here\nMADEINOZ_KNOWLEDGE_ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here\nMADEINOZ_KNOWLEDGE_GOOGLE_API_KEY=your-google-api-key\nMADEINOZ_KNOWLEDGE_GROQ_API_KEY=gsk-your-groq-key\n</code></pre> <p>Important: Only include the API key for your chosen provider.</p>"},{"location":"reference/configuration.html#custom-base-url-openai-compatible-providers","title":"Custom Base URL (OpenAI-Compatible Providers)","text":"<pre><code>MADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\n</code></pre> <p>Common values:</p> <ul> <li><code>https://openrouter.ai/api/v1</code> - OpenRouter</li> <li><code>https://api.together.xyz/v1</code> - Together AI</li> <li><code>https://api.fireworks.ai/inference/v1</code> - Fireworks AI</li> <li><code>https://api.deepinfra.com/v1/openai</code> - DeepInfra</li> <li><code>http://localhost:11434/v1</code> - Local Ollama</li> </ul> <p>For Ollama, use:</p> <pre><code>MADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=http://host.docker.internal:11434/v1\n# On Linux, replace host.docker.internal with your Ollama server IP\n</code></pre>"},{"location":"reference/configuration.html#embedder-configuration","title":"Embedder Configuration","text":""},{"location":"reference/configuration.html#embedder-provider","title":"Embedder Provider","text":"<pre><code>MADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=openai\n</code></pre> <p>Valid values:</p> <ul> <li><code>openai</code> - OpenAI API or OpenAI-compatible providers</li> <li><code>anthropic</code> - Anthropic (limited embedding support)</li> <li><code>ollama</code> - Local Ollama embeddings (recommended for cost)</li> </ul>"},{"location":"reference/configuration.html#embedding-model","title":"Embedding Model","text":"<pre><code>MADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\n</code></pre> <p>Recommended models:</p> <ul> <li><code>mxbai-embed-large</code> (Ollama) - FREE, 87ms, 73.9% quality</li> <li><code>text-embedding-3-small</code> (OpenAI) - $0.02/1M, 78.2% quality</li> <li><code>BAAI/bge-large-en-v1.5</code> (Together AI) - Fast, high quality</li> </ul>"},{"location":"reference/configuration.html#embedding-dimensions","title":"Embedding Dimensions","text":"<pre><code>MADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>Important: Must match the embedding model's output dimensions:</p> <ul> <li><code>mxbai-embed-large</code> \u2192 <code>1024</code></li> <li><code>text-embedding-3-small</code> \u2192 <code>1536</code></li> <li><code>text-embedding-3-large</code> \u2192 <code>3072</code></li> <li><code>nomic-embed-text</code> \u2192 <code>768</code></li> </ul> <p>\u26a0\ufe0f CRITICAL: Changing embedding models breaks existing data. All vectors must have identical dimensions for Neo4j vector search to work. If you change embedding models, you must clear the graph and re-add all knowledge.</p>"},{"location":"reference/configuration.html#embedder-base-url","title":"Embedder Base URL","text":"<pre><code>MADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://host.docker.internal:11434/v1\n</code></pre> <p>Set only if using Ollama or another OpenAI-compatible embedder provider.</p>"},{"location":"reference/configuration.html#database-backend-configuration","title":"Database Backend Configuration","text":""},{"location":"reference/configuration.html#database-type","title":"Database Type","text":"<pre><code>MADEINOZ_KNOWLEDGE_DATABASE_TYPE=neo4j\n</code></pre> <p>Valid values:</p> <ul> <li><code>neo4j</code> (default) - Native graph database, better special character handling</li> <li><code>falkordb</code> - Redis-based, simpler setup, lower resources</li> </ul>"},{"location":"reference/configuration.html#neo4j-configuration","title":"Neo4j Configuration","text":"<pre><code>MADEINOZ_KNOWLEDGE_NEO4J_URI=bolt://neo4j:7687\nMADEINOZ_KNOWLEDGE_NEO4J_USER=neo4j\nMADEINOZ_KNOWLEDGE_NEO4J_PASSWORD=madeinozknowledge\nMADEINOZ_KNOWLEDGE_NEO4J_DATABASE=neo4j\n</code></pre> <p>Default values work with docker-compose setup.</p> <p>For external Neo4j:</p> <pre><code>MADEINOZ_KNOWLEDGE_NEO4J_URI=bolt://neo4j:7687\nMADEINOZ_KNOWLEDGE_NEO4J_USER=neo4j\nMADEINOZ_KNOWLEDGE_NEO4J_PASSWORD=madeinozknowledge\n</code></pre>"},{"location":"reference/configuration.html#changing-neo4j-password","title":"Changing Neo4j Password","text":""},{"location":"reference/configuration.html#why-this-process-is-required","title":"Why This Process Is Required","text":"<p>Neo4j's security architecture stores credentials inside the persistent data volume, not in environment variables. Understanding this is critical:</p> <ol> <li> <p>First Startup Behavior: The <code>NEO4J_AUTH</code> environment variable is only read on first startup when no user data exists. Neo4j uses it to create the initial <code>neo4j</code> user with the specified password.</p> </li> <li> <p>Password Persistence: After initialization, the password is stored encrypted in the <code>/data</code> volume as part of the <code>system</code> database. The <code>NEO4J_AUTH</code> environment variable is ignored on subsequent startups.</p> </li> <li> <p>Why Environment Variables Don't Work: Many users expect changing <code>NEO4J_AUTH</code> to update the password\u2014this is a common misconception. Since the password lives in the data volume, changing environment variables has no effect on existing databases.</p> </li> <li> <p>The Correct Approach: You must use Cypher commands (<code>ALTER CURRENT USER</code> or <code>ALTER USER</code>) to modify passwords in a running database. This ensures the change is written to the <code>system</code> database where credentials are actually stored.</p> </li> </ol> <p>This design ensures that your credentials remain consistent with your data, and that accidentally changing an environment variable cannot lock you out of your database.</p> <p>Reference: Neo4j Operations Manual - Manage Users</p>"},{"location":"reference/configuration.html#method-1-using-neo4j-browser-recommended","title":"Method 1: Using Neo4j Browser (Recommended)","text":"<p>The Neo4j Browser provides a web interface for database administration.</p> <ol> <li>Open Neo4j Browser:</li> </ol> <pre><code>http://localhost:7474\n</code></pre> <ol> <li>Login with current credentials:</li> <li>Username: <code>neo4j</code></li> <li> <p>Password: (your current password, default: <code>madeinozknowledge</code>)</p> </li> <li> <p>Run the official password change command:</p> </li> </ol> <pre><code>ALTER CURRENT USER SET PASSWORD FROM 'current-password' TO 'new-password'\n</code></pre> <p>This command requires you to know your current password and changes it atomically in the <code>system</code> database.</p> <ol> <li>Update your <code>.env</code> file to match:</li> </ol> <pre><code># Edit ~/.claude/.env\nMADEINOZ_KNOWLEDGE_NEO4J_PASSWORD=new-password\n</code></pre> <p>Important: The <code>.env</code> file must match the database password for the MCP server to connect.</p> <ol> <li>Restart the MCP server to apply the new password:</li> </ol> <pre><code>bun run server-cli stop\nbun run server-cli start\n</code></pre>"},{"location":"reference/configuration.html#method-2-using-cypher-shell-command-line","title":"Method 2: Using Cypher Shell (Command Line)","text":"<p>For users who prefer command-line administration.</p> <ol> <li>Connect to the Neo4j container:</li> </ol> <pre><code>docker exec -it madeinoz-knowledge-neo4j cypher-shell -u neo4j -p 'current-password'\n</code></pre> <ol> <li>Run the official password change command:</li> </ol> <pre><code>ALTER CURRENT USER SET PASSWORD FROM 'current-password' TO 'new-password';\n</code></pre> <p>Then exit the shell:</p> <pre><code>:exit\n</code></pre> <ol> <li>Update <code>.env</code> and restart (same as Method 1, steps 4-5)</li> </ol>"},{"location":"reference/configuration.html#method-3-admin-changing-another-users-password-enterprise","title":"Method 3: Admin Changing Another User's Password (Enterprise)","text":"<p>If you have admin privileges, you can change any user's password:</p> <pre><code>ALTER USER username SET PASSWORD 'new-password'\n</code></pre> <p>Optionally force a password change on next login:</p> <pre><code>ALTER USER username SET PASSWORD 'new-password' CHANGE REQUIRED\n</code></pre>"},{"location":"reference/configuration.html#password-requirements","title":"Password Requirements","text":"<ul> <li>Minimum 8 characters (Neo4j default)</li> <li>Avoid special characters that may cause shell escaping issues</li> <li>Store securely - this password protects your knowledge graph</li> </ul>"},{"location":"reference/configuration.html#critical-never-delete-data-volumes","title":"\u26a0\ufe0f CRITICAL: Never Delete Data Volumes","text":"<p>NEVER use <code>docker compose down -v</code> or <code>podman compose down -v</code> to \"reset\" a forgotten password. The <code>-v</code> flag deletes all data volumes, permanently destroying your knowledge graph. This data is irreplaceable.</p> <p>If you've forgotten your password:</p> <ol> <li>Try the default password: <code>madeinozknowledge</code></li> <li>Check your <code>~/.claude/.env</code> for <code>MADEINOZ_KNOWLEDGE_NEO4J_PASSWORD</code></li> <li>Use Method 1 or Method 2 above with a password you remember having set</li> <li>If all else fails, contact the community for recovery assistance before considering any destructive action</li> </ol>"},{"location":"reference/configuration.html#troubleshooting-password-issues","title":"Troubleshooting Password Issues","text":"<p>\"AuthenticationRateLimit\" error: Neo4j blocks connections after too many failed attempts. Wait 30 seconds or restart the Neo4j container:</p> <pre><code>docker restart madeinoz-knowledge-neo4j\n</code></pre> <p>\"Authentication failed\" after password change: The MCP server is using the old password. Ensure:</p> <ol> <li><code>.env</code> file has the new password</li> <li>MCP server was restarted after the change</li> <li>No shell environment variable is overriding the <code>.env</code> value</li> </ol>"},{"location":"reference/configuration.html#falkordb-configuration","title":"FalkorDB Configuration","text":"<pre><code>MADEINOZ_KNOWLEDGE_FALKORDB_HOST=madeinoz-knowledge-falkordb\nMADEINOZ_KNOWLEDGE_FALKORDB_PORT=6379\n</code></pre> <p>For external FalkorDB:</p> <pre><code>MADEINOZ_KNOWLEDGE_FALKORDB_HOST=your-redis-server\nMADEINOZ_KNOWLEDGE_FALKORDB_PORT=6379\n</code></pre>"},{"location":"reference/configuration.html#knowledge-graph-configuration","title":"Knowledge Graph Configuration","text":""},{"location":"reference/configuration.html#group-id","title":"Group ID","text":"<pre><code>MADEINOZ_KNOWLEDGE_GROUP_ID=main\n</code></pre> <p>Organizes knowledge into logical groups. Enables multiple isolated knowledge graphs:</p> <pre><code># Create separate groups for different domains\nMADEINOZ_KNOWLEDGE_GROUP_ID=main          # Default personal knowledge\nMADEINOZ_KNOWLEDGE_GROUP_ID=research      # Research findings\nMADEINOZ_KNOWLEDGE_GROUP_ID=osint-intel   # OSINT/CTI data\n</code></pre> <p>Multiple groups can be searched together using the search workflows.</p>"},{"location":"reference/configuration.html#performance-configuration","title":"Performance Configuration","text":""},{"location":"reference/configuration.html#rate-limiting","title":"Rate Limiting","text":"<pre><code>RATE_LIMIT_MAX_REQUESTS=60\nRATE_LIMIT_WINDOW_SECONDS=60\nRATE_LIMIT_ENABLED=true\n</code></pre> <p>Controls request rate limiting per IP address. Protects against abuse and DoS attacks.</p> Variable Default Description <code>RATE_LIMIT_MAX_REQUESTS</code> 60 Maximum requests per time window per IP <code>RATE_LIMIT_WINDOW_SECONDS</code> 60 Time window in seconds <code>RATE_LIMIT_ENABLED</code> true Set to <code>false</code> to disable (not recommended for production) <p>Note: Rate limiting only applies to HTTP transport mode. SSE/stdio modes do not use rate limiting.</p>"},{"location":"reference/configuration.html#concurrencysemaphore-limit","title":"Concurrency/Semaphore Limit","text":"<pre><code>MADEINOZ_KNOWLEDGE_SEMAPHORE_LIMIT=10\n</code></pre> <p>Controls concurrent API requests to LLM provider. Tune based on API rate limits:</p> Tier Rate Limit Recommended Cost Free 0 RPM 1-2 $0/month Tier 1 10 RPM 3-5 $5/month Tier 2 60 RPM 8-10 $20/month Tier 3 500 RPM 10-15 $100/month Tier 4 5000+ RPM 20-50 $250/month <p>If experiencing rate limit errors:</p> <ol> <li>Lower this value (e.g., 5)</li> <li>Upgrade your API tier</li> <li>Check <code>MADEINOZ_KNOWLEDGE_OPENAI_API_KEY</code> has credits/quota</li> </ol>"},{"location":"reference/configuration.html#neo4j-specific-features","title":"Neo4j-Specific Features","text":""},{"location":"reference/configuration.html#search-all-groups-neo4j-only","title":"Search All Groups (Neo4j only)","text":"<pre><code>GRAPHITI_SEARCH_ALL_GROUPS=true\n</code></pre> <p>When enabled, searches automatically query all available group_ids without explicitly specifying them. This ensures knowledge stored in different groups is discoverable:</p> <pre><code># With SEARCH_ALL_GROUPS=true:\n# Finds knowledge in: main, research, osint-intel, etc.\n\n# With SEARCH_ALL_GROUPS=false:\n# Only searches specified group_ids (original behavior)\n</code></pre> <p>Default: <code>true</code> (enabled)</p> <p>Cache duration: 30 seconds (balance between performance and freshness)</p>"},{"location":"reference/configuration.html#telemetry-configuration","title":"Telemetry Configuration","text":""},{"location":"reference/configuration.html#telemetry-enabled","title":"Telemetry Enabled","text":"<pre><code>MADEINOZ_KNOWLEDGE_GRAPHITI_TELEMETRY_ENABLED=false\n</code></pre> <p>Controls whether Graphiti sends anonymous telemetry. Default is <code>false</code> (disabled).</p>"},{"location":"reference/configuration.html#metrics-observability-configuration","title":"Metrics &amp; Observability Configuration","text":""},{"location":"reference/configuration.html#metrics-collection","title":"Metrics Collection","text":"<pre><code>MADEINOZ_KNOWLEDGE_PROMPT_CACHE_METRICS_ENABLED=true\n</code></pre> <p>Controls whether Prometheus metrics are collected and exported. Default is <code>true</code> (enabled).</p> <p>Metrics endpoint: <code>http://localhost:9091/metrics</code> (dev) or <code>http://localhost:9090/metrics</code> (prod)</p>"},{"location":"reference/configuration.html#debug-logging","title":"Debug Logging","text":"<pre><code>MADEINOZ_KNOWLEDGE_PROMPT_CACHE_LOG_REQUESTS=false\n</code></pre> <p>Enables detailed per-request metrics logging. When <code>true</code> and <code>LOG_LEVEL=DEBUG</code>, logs show:</p> <pre><code>\ud83d\udcca Metrics: prompt=1234, completion=567, cost=$0.000089\n</code></pre> <p>Default is <code>false</code> (disabled).</p>"},{"location":"reference/configuration.html#prompt-caching","title":"Prompt Caching","text":"<pre><code># Prompt caching is DISABLED by default - set to true to enable\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED=true\n</code></pre> <p>Default: <code>false</code> (disabled)</p> <p>Controls prompt caching for Gemini models via OpenRouter. When enabled, the system uses explicit <code>cache_control</code> markers in requests (similar to Anthropic's approach), not implicit caching. The <code>/chat/completions</code> endpoint supports multipart format with cache control markers.</p> <p>Now Available for Gemini</p> <p>Prompt caching is now functional for Gemini models on OpenRouter. The system routes Gemini models through the <code>/chat/completions</code> endpoint which supports multipart format with cache control markers. Set to <code>true</code> to enable caching and reduce costs on repeated prompts.</p> <p>For detailed metrics documentation, see the Observability &amp; Metrics reference.</p>"},{"location":"reference/configuration.html#memory-decay-configuration-feature-009","title":"Memory Decay Configuration (Feature 009)","text":"<p>Feature 009: Memory Decay Scoring</p> <p>The memory decay system automatically prioritizes important memories, allows stale information to fade, and maintains sustainable graph growth. See Memory Decay &amp; Lifecycle Management for complete user guide.</p>"},{"location":"reference/configuration.html#configuration-file","title":"Configuration File","text":"<p>Location: <code>config/decay-config.yaml</code></p> <p>This YAML file controls all memory decay behavior. It is copied into the Docker container at build time.</p> <p>To modify configuration:</p> <ol> <li>Edit <code>config/decay-config.yaml</code></li> <li>Rebuild the Docker image: <code>docker build -f docker/Dockerfile -t madeinoz-knowledge-system:local .</code></li> <li>Restart containers: <code>bun run server-cli stop &amp;&amp; bun run server-cli start --dev</code></li> </ol>"},{"location":"reference/configuration.html#decay-thresholds","title":"Decay Thresholds","text":"<p>Control when memories transition between lifecycle states:</p> <pre><code>decay:\n  thresholds:\n    dormant:\n      days: 30           # Days inactive before ACTIVE \u2192 DORMANT\n      decay_score: 0.3   # Decay score threshold for transition\n    archived:\n      days: 90           # Days inactive before DORMANT \u2192 ARCHIVED\n      decay_score: 0.6   # Decay score threshold for transition\n    expired:\n      days: 180          # Days inactive before ARCHIVED \u2192 EXPIRED\n      decay_score: 0.9   # Decay score threshold for transition\n      max_importance: 3  # Only expire if importance \u2264 3\n</code></pre> <p>Lifecycle states: - ACTIVE - Recently accessed, full relevance - DORMANT - Not accessed 30+ days, lower search priority - ARCHIVED - Not accessed 90+ days, much lower priority - EXPIRED - Marked for deletion (soft-delete) - SOFT_DELETED - Deleted but recoverable for 90 days</p> <p>See Memory Decay Guide for details.</p>"},{"location":"reference/configuration.html#maintenance-schedule","title":"Maintenance Schedule","text":"<p>Configure automatic maintenance operations:</p> <pre><code>decay:\n  maintenance:\n    batch_size: 500             # Memories to process per batch\n    max_duration_minutes: 10    # Maximum maintenance run time\n    schedule_interval_hours: 24 # Hours between automatic runs (0 = disabled)\n</code></pre> <p>What maintenance does: - Recalculates decay scores for all memories - Transitions memories between lifecycle states - Soft-deletes expired memories (90-day retention) - Generates health metrics for Grafana</p> <p>To disable automatic maintenance: Set <code>schedule_interval_hours: 0</code></p>"},{"location":"reference/configuration.html#search-weights","title":"Search Weights","text":"<p>Configure how search results are ranked:</p> <pre><code>decay:\n  weights:\n    semantic: 0.60    # Vector similarity weight (0.0-1.0)\n    recency: 0.25     # Temporal freshness weight (0.0-1.0)\n    importance: 0.15  # Importance score weight (0.0-1.0)\n</code></pre> <p>Must sum to 1.0</p> <p>Formula: <code>weighted_score = (semantic \u00d7 0.60) + (recency \u00d7 0.25) + (importance \u00d7 0.15)</code></p> <p>Tuning guidelines: - Want recent stuff more? Increase <code>recency</code> - Only care about accuracy? Increase <code>semantic</code> - Always show important stuff? Increase <code>importance</code></p> <p>See Weighted Search Results for examples.</p>"},{"location":"reference/configuration.html#classification-defaults","title":"Classification Defaults","text":"<p>Configure fallback values when LLM is unavailable:</p> <pre><code>classification:\n  default_importance: 3  # MODERATE (1-5)\n  default_stability: 3   # MODERATE (1-5)\n</code></pre> <p>Importance levels: 1=TRIVIAL, 2=LOW, 3=MODERATE, 4=HIGH, 5=CORE Stability levels: 1=VOLATILE, 2=LOW, 3=MODERATE, 4=HIGH, 5=PERMANENT</p>"},{"location":"reference/configuration.html#permanent-memory-thresholds","title":"Permanent Memory Thresholds","text":"<p>Configure which memories are exempt from decay:</p> <pre><code>permanent:\n  importance_threshold: 4  # Minimum importance for permanent\n  stability_threshold: 4   # Minimum stability for permanent\n</code></pre> <p>Memories with importance \u22654 AND stability \u22654 are classified as PERMANENT: - Never accumulate decay - Never transition lifecycle states - Exempt from archival and deletion - Always prioritized in search</p>"},{"location":"reference/configuration.html#half-life-configuration","title":"Half-Life Configuration","text":"<p>Base decay rate (adjusted by stability factor):</p> <pre><code>decay:\n  base_half_life_days: 180  # Base half-life in days (1-365)\n</code></pre> <p>How it works: - Stability 1 (VOLATILE): 0.33\u00d7 half-life (60 days) - Stability 3 (MODERATE): 1.0\u00d7 half-life (180 days) - Stability 5 (PERMANENT): \u221e half-life (never decays)</p> <p>Higher values = slower decay. See Decay Score for details.</p>"},{"location":"reference/configuration.html#retention-policy","title":"Retention Policy","text":"<p>Configure soft-delete retention period:</p> <pre><code>decay:\n  retention:\n    soft_delete_days: 90  # Days to retain soft-deleted memories\n</code></pre> <p>Soft-deleted memories are permanently purged after this period. Recovery only possible within the retention window.</p>"},{"location":"reference/configuration.html#query-sanitization-falkordb-only","title":"Query Sanitization (FalkorDB Only)","text":"<p>For FalkorDB backend, special characters in group_ids and search queries are automatically sanitized to prevent Lucene query syntax errors.</p> <p>Escaped characters: <code>+ - &amp;&amp; || ! ( ) { } [ ] ^ \" ~ * ? : \\ /</code></p> <p>Example:</p> <pre><code>Input:  group_id:madeinoz-threat-intel\nOutput: group_id:\"madeinoz-threat-intel\"\n</code></pre> <p>This is automatic and transparent.</p>"},{"location":"reference/configuration.html#legacy-configuration-deprecated","title":"Legacy Configuration (Deprecated)","text":"<p>These variables are deprecated in favor of <code>MADEINOZ_KNOWLEDGE_*</code> prefixed versions:</p> <pre><code># Old style (still supported as fallback)\nOPENAI_API_KEY=sk-your-key\nANTHROPIC_API_KEY=sk-ant-your-key\n\n# New style (preferred)\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-your-key\nMADEINOZ_KNOWLEDGE_ANTHROPIC_API_KEY=sk-ant-your-key\n</code></pre> <p>Benefits of migration:</p> <ul> <li>Isolated configuration per pack</li> <li>No conflicts with other tools</li> <li>Better organization in .env file</li> </ul>"},{"location":"reference/configuration.html#configuration-examples","title":"Configuration Examples","text":""},{"location":"reference/configuration.html#minimal-setup-ollama-llm-local-embeddings","title":"Minimal Setup (Ollama LLM + Local Embeddings)","text":"<pre><code># LLM - must use cloud provider due to Graphiti limitations\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=openai/gpt-4o-mini\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-or-v1-your-key\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\n\n# Embeddings - free local Ollama\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://host.docker.internal:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n\n# Database\nMADEINOZ_KNOWLEDGE_DATABASE_TYPE=neo4j\n\n# Group\nMADEINOZ_KNOWLEDGE_GROUP_ID=main\n</code></pre>"},{"location":"reference/configuration.html#premium-setup-all-cloud","title":"Premium Setup (All Cloud)","text":"<pre><code># LLM\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=gpt-4o\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-your-openai-key\n\n# Embeddings\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=text-embedding-3-small\n\n# Database\nMADEINOZ_KNOWLEDGE_DATABASE_TYPE=neo4j\n\n# Performance\nMADEINOZ_KNOWLEDGE_SEMAPHORE_LIMIT=20\n</code></pre>"},{"location":"reference/configuration.html#multi-group-cti-setup","title":"Multi-Group CTI Setup","text":"<pre><code># LLM with special character support (Neo4j)\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=gpt-4o\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-your-key\n\n# Embeddings\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://host.docker.internal:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\n\n# Database - Neo4j for hyphenated identifiers\nMADEINOZ_KNOWLEDGE_DATABASE_TYPE=neo4j\nMADEINOZ_KNOWLEDGE_NEO4J_URI=bolt://neo4j:7687\n\n# Multiple groups for CTI domains\nMADEINOZ_KNOWLEDGE_GROUP_ID=main\n# Create additional groups by using different group_id in workflows\n\n# Search all groups automatically\nGRAPHITI_SEARCH_ALL_GROUPS=true\n</code></pre>"},{"location":"reference/configuration.html#verification","title":"Verification","text":"<p>To verify your configuration is correct:</p> <pre><code># Check configuration is loaded\ngrep MADEINOZ_KNOWLEDGE ~/.claude/.env\n\n# Verify API key is set (don't expose the value)\ngrep -c MADEINOZ_KNOWLEDGE_OPENAI_API_KEY ~/.claude/.env\n\n# Test connectivity\ncurl http://localhost:8000/health\n</code></pre>"},{"location":"reference/configuration.html#changing-configuration","title":"Changing Configuration","text":"<p>To update configuration after installation:</p> <ol> <li>Edit PAI .env file:</li> </ol> <pre><code>nano ~/.claude/.env\n# or: vim ~/.claude/.env\n</code></pre> <ol> <li>Restart the server:</li> </ol> <pre><code>bun run server-cli stop\nbun run server-cli start\n</code></pre> <ol> <li>Verify changes:</li> </ol> <pre><code>curl http://localhost:8000/health\n</code></pre>"},{"location":"reference/configuration.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/configuration.html#invalid-api-key","title":"\"Invalid API key\"","text":"<ul> <li>Verify key is correctly copied (no spaces, correct prefix)</li> <li>Check key has available credits/quota</li> <li>Confirm key is for the right provider</li> </ul>"},{"location":"reference/configuration.html#unknown-model","title":"\"Unknown model\"","text":"<ul> <li>Verify model name matches provider's catalog</li> <li>Check provider base URL is correct</li> <li>Confirm API key can access the model</li> </ul>"},{"location":"reference/configuration.html#connection-refused","title":"\"Connection refused\"","text":"<ul> <li>Verify <code>MADEINOZ_KNOWLEDGE_*_BASE_URL</code> is correct</li> <li>Check firewall allows connection to provider</li> <li>Ensure Ollama server is running (if using local embeddings)</li> </ul>"},{"location":"reference/configuration.html#rate-limit-exceeded","title":"\"Rate limit exceeded\"","text":"<ul> <li>Lower <code>MADEINOZ_KNOWLEDGE_SEMAPHORE_LIMIT</code></li> <li>Upgrade API tier</li> <li>Wait for rate limit window to reset</li> </ul>"},{"location":"reference/configuration.html#vector-dimension-mismatch","title":"\"Vector dimension mismatch\"","text":"<ul> <li>Verify <code>MADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS</code> matches your embedding model</li> <li>Cannot change embedding models without clearing the graph</li> <li>See \"Database Backend Configuration\" section for model-to-dimension mapping</li> </ul>"},{"location":"reference/configuration.html#system-limits-summary","title":"System Limits Summary","text":"<p>All configurable limits in one place:</p> Limit Default Variable Notes Rate limit 60 req/60s per IP <code>RATE_LIMIT_MAX_REQUESTS</code>, <code>RATE_LIMIT_WINDOW_SECONDS</code> HTTP mode only Concurrent LLM requests 10 <code>SEMAPHORE_LIMIT</code> Tune based on API tier Search results (nodes) 10 MCP <code>max_nodes</code> param Per-request Search results (facts) 10 MCP <code>max_facts</code> param Per-request Search results (episodes) 10 MCP <code>max_episodes</code> param Per-request Cache minimum tokens 1024 N/A Requests &lt; 1024 tokens skip caching Episode body size No limit N/A Bounded by LLM context window"},{"location":"reference/configuration.html#content-size-guidelines","title":"Content Size Guidelines","text":"<p>While there's no hard limit on episode body size, consider these guidelines:</p> Content Size Behavior &lt; 10 KB Optimal - fast processing, reliable extraction 10-50 KB Good - may take longer to process 50-100 KB Acceptable - consider chunking for bulk import &gt; 100 KB Not recommended - may hit LLM context limits or timeout <p>For large documents: Use the bulk import workflow which automatically handles chunking. See Advanced Usage.</p>"},{"location":"reference/configuration.html#environment-variable-precedence","title":"Environment Variable Precedence","text":"<p>Configuration is loaded in this order (later values override earlier):</p> <ol> <li><code>.env.example</code> in pack (reference only)</li> <li>PAI .env file (<code>~/.claude/.env</code> or <code>$PAI_DIR/.env</code>)</li> <li>Shell environment variables (if set directly)</li> </ol> <p>Recommended: Keep all configuration in PAI .env file for consistency.</p>"},{"location":"reference/developer-notes.html","title":"Developer Notes","text":""},{"location":"reference/developer-notes.html#developer-notes","title":"Developer Notes","text":""},{"location":"reference/developer-notes.html#custom-docker-image","title":"Custom Docker Image","text":""},{"location":"reference/developer-notes.html#why-we-use-a-custom-image","title":"Why We Use a Custom Image","text":"<p>This project currently uses a custom Docker image (<code>ghcr.io/madeinoz67/madeinoz-knowledge-system:latest</code>) instead of the official upstream images. This is a temporary workaround while waiting for upstream fixes to be merged.</p> <p>Published Image:</p> <ul> <li>Registry: GitHub Container Registry</li> <li>Image: <code>ghcr.io/madeinoz67/madeinoz-knowledge-system:latest</code></li> </ul> <p>Upstream images we're NOT using:</p> <ul> <li><code>falkordb/graphiti-knowledge-graph-mcp:latest</code> (FalkorDB backend)</li> <li><code>zepai/knowledge-graph-mcp:standalone</code> (Neo4j backend)</li> </ul>"},{"location":"reference/developer-notes.html#patches-applied","title":"Patches Applied","text":"<p>The custom Docker image includes four critical patches applied at image build time:</p>"},{"location":"reference/developer-notes.html#1-async-iteration-bug-fix","title":"1. Async Iteration Bug Fix","text":"<p>Issue: [Upstream GitHub Issue - async iteration on NoneType]</p> <p>Fix: Added None check before async for loop in <code>get_all_group_ids()</code></p> <pre><code># Before: Would crash with \"async for requires __aiter__ method, got NoneType\"\nrecords = [record async for record in result]\n\n# After: Safe None check\nif result:\n    records = [record async for record in result]\nelse:\n    group_ids = []\n</code></pre>"},{"location":"reference/developer-notes.html#2-ollamacustom-endpoint-support","title":"2. Ollama/Custom Endpoint Support","text":"<p>Issue: Upstream GitHub Issue #1116</p> <p>Fix: Added explicit Ollama embedder client and OpenAI-compatible API support</p> <p>This enables:</p> <ul> <li>Ollama embeddings with custom models</li> <li>OpenRouter and other OpenAI-compatible LLM providers</li> <li>Custom embedding dimensions</li> </ul>"},{"location":"reference/developer-notes.html#3-search-all-groups-functionality","title":"3. Search All Groups Functionality","text":"<p>Issue: Default behavior only searches specified group_ids, making cross-group discovery impossible</p> <p>Fix: When no <code>group_ids</code> are specified, search queries ALL groups in the knowledge graph</p> <p>This is essential for PAI pack usage where knowledge may be stored across multiple groups (e.g., <code>osint-profiles</code>, <code>main</code>, <code>research</code>).</p>"},{"location":"reference/developer-notes.html#4-temporal-search-feature","title":"4. Temporal Search Feature","text":"<p>Issue: Original search functionality lacked time-based filtering capabilities</p> <p>Fix: Added <code>start_date</code> and <code>end_date</code> parameters to search functions for temporal queries</p> <p>This enables:</p> <ul> <li>Time-range filtered searches (e.g., \"find entities created in last 7 days\")</li> <li>Historical knowledge tracking and analysis</li> <li>Temporal relationship discovery</li> <li>Episode-based timeline reconstruction</li> </ul> <p>The temporal search feature integrates with the existing search API, allowing combined spatial and temporal queries for comprehensive knowledge retrieval.</p>"},{"location":"reference/developer-notes.html#configuration-selection","title":"Configuration Selection","text":"<p>File: <code>src/skills/server/entrypoint.sh</code> (development) or baked into image</p> <p>Purpose: Dynamically select the correct config file based on <code>DATABASE_TYPE</code> environment variable</p> <pre><code>case \"$DATABASE_TYPE\" in\n  falkordb|redis)\n    cp /tmp/config-falkordb.yaml /app/mcp/config/config.yaml\n    ;;\n  neo4j|*)\n    cp /tmp/config-neo4j.yaml /app/mcp/config/config.yaml\n    ;;\nesac\n</code></pre> <p>Both config files are baked into the image at build time:</p> <ul> <li><code>/tmp/config-neo4j.yaml</code></li> <li><code>/tmp/config-falkordb.yaml</code></li> </ul>"},{"location":"reference/developer-notes.html#building-the-custom-image","title":"Building the Custom Image","text":"<pre><code># From pack root directory\ndocker build -t madeinoz-knowledge-system:fixed .\n</code></pre> <p>The Dockerfile applies patches during the image build process:</p> <ol> <li>Copies patch files from the patches directory</li> <li>Copies both config files</li> <li>Copies <code>entrypoint.sh</code> for runtime config selection</li> <li>Applies all patches to the upstream source code during build</li> </ol>"},{"location":"reference/developer-notes.html#migration-path-to-official-images","title":"Migration Path to Official Images","text":"<p>When upstream merges these fixes, we will:</p> <ol> <li>Update <code>src/skills/server/lib/container.ts</code> to use official images:</li> </ol> <pre><code>static readonly IMAGES = {\n  falkordb: {\n    database: \"falkordb/falkordb:latest\",\n    mcp: \"falkordb/graphiti-knowledge-graph-mcp:latest\",  // \u2190 Revert here\n  },\n  neo4j: {\n    database: \"neo4j:5.26.0\",\n    mcp: \"zepai/knowledge-graph-mcp:standalone\",  // \u2190 Revert here\n  },\n}\n</code></pre> <ol> <li>Remove custom image build from documentation</li> <li>Archive patches to <code>docker/patches/archived/</code> for reference</li> <li>Update this document with migration completion date</li> </ol> <p>Status: Waiting for upstream to merge fixes. Track progress at:</p> <ul> <li>Graphiti GitHub Issues</li> <li>Zep AI releases</li> </ul>"},{"location":"reference/developer-notes.html#environment-variable-prefix-workaround","title":"Environment Variable Prefix Workaround","text":""},{"location":"reference/developer-notes.html#the-problem","title":"The Problem","text":"<p>PAI (Personal AI Infrastructure) packs must isolate their configuration to avoid conflicts with other packs. However, the Graphiti MCP server expects unprefixed environment variables like:</p> <ul> <li><code>OPENAI_API_KEY</code></li> <li><code>MODEL_NAME</code></li> <li><code>LLM_PROVIDER</code></li> <li><code>NEO4J_URI</code></li> <li>etc.</li> </ul> <p>If every pack used these generic names, they would collide in the shared PAI <code>.env</code> file.</p>"},{"location":"reference/developer-notes.html#the-solution-variable-mapping","title":"The Solution: Variable Mapping","text":"<p>We use prefixed variables in the PAI <code>.env</code> file and map them to unprefixed variables inside the container.</p>"},{"location":"reference/developer-notes.html#in-pai-env-file-claudeenv-or-pai_direnv","title":"In PAI .env File (<code>~/.claude/.env</code> or <code>$PAI_DIR/.env</code>)","text":"<pre><code># Pack-isolated configuration (prefixed)\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-...\nMADEINOZ_KNOWLEDGE_MODEL_NAME=google/gemini-2.0-flash-001\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openrouter\nMADEINOZ_KNOWLEDGE_DATABASE_TYPE=neo4j\nMADEINOZ_KNOWLEDGE_NEO4J_PASSWORD=madeinozknowledge\n</code></pre>"},{"location":"reference/developer-notes.html#in-container-unprefixed","title":"In Container (unprefixed)","text":"<p>The <code>entrypoint.sh</code> script automatically maps prefixed \u2192 unprefixed:</p> <pre><code># entrypoint.sh mapping logic\ntest -n \"$MADEINOZ_KNOWLEDGE_OPENAI_API_KEY\" &amp;&amp; export OPENAI_API_KEY=\"$MADEINOZ_KNOWLEDGE_OPENAI_API_KEY\"\ntest -n \"$MADEINOZ_KNOWLEDGE_MODEL_NAME\" &amp;&amp; export MODEL_NAME=\"$MADEINOZ_KNOWLEDGE_MODEL_NAME\"\ntest -n \"$MADEINOZ_KNOWLEDGE_LLM_PROVIDER\" &amp;&amp; export LLM_PROVIDER=\"$MADEINOZ_KNOWLEDGE_LLM_PROVIDER\"\n\n# Fallback to unprefixed if no prefix found (for standalone use)\ntest -n \"$OPENAI_API_KEY\" &amp;&amp; export OPENAI_API_KEY=\"$OPENAI_API_KEY\"\n</code></pre>"},{"location":"reference/developer-notes.html#how-it-works","title":"How It Works","text":"<ol> <li>PAI .env contains only prefixed variables (<code>MADEINOZ_KNOWLEDGE_*</code>)</li> <li>Docker Compose passes these to the container via a targeted <code>env_file</code></li> <li>entrypoint.sh runs before the MCP server starts</li> <li>Script checks for prefixed variables and exports unprefixed versions</li> <li>MCP server sees standard unprefixed variables it expects</li> </ol>"},{"location":"reference/developer-notes.html#benefits","title":"Benefits","text":"<p>\u2713 Pack isolation - No conflicts with other PAI packs \u2713 Portable - Works across all PAI installations (<code>$PAI_DIR</code> or <code>~/.claude/</code>) \u2713 Backward compatible - Fallback to unprefixed for standalone usage \u2713 Maintainable - Single <code>.env</code> file for entire PAI system</p>"},{"location":"reference/developer-notes.html#fallback-strategy","title":"Fallback Strategy","text":"<p>The entrypoint uses a two-tier approach:</p> <pre><code># Try prefixed first (PAI pack mode)\ntest -n \"$MADEINOZ_KNOWLEDGE_OPENAI_API_KEY\" &amp;&amp; export OPENAI_API_KEY=\"$MADEINOZ_KNOWLEDGE_OPENAI_API_KEY\"\n\n# Fall back to unprefixed (standalone mode)\ntest -n \"$OPENAI_API_KEY\" &amp;&amp; export OPENAI_API_KEY=\"$OPENAI_API_KEY\"\n</code></pre> <p>This ensures the system works both:</p> <ul> <li>As a PAI pack (using prefixed variables)</li> <li>Standalone (using unprefixed variables)</li> </ul>"},{"location":"reference/developer-notes.html#example-complete-variable-flow","title":"Example: Complete Variable Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ~/.claude/.env (PAI central config) \u2502\n\u2502                                     \u2502\n\u2502 MADEINOZ_KNOWLEDGE_OPENAI_API_KEY= \u2502\n\u2502 sk-proj-abc123...                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u2502 env_file: ${PAI_DIR:-~/.claude}/.env\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Docker Container Environment        \u2502\n\u2502                                     \u2502\n\u2502 MADEINOZ_KNOWLEDGE_OPENAI_API_KEY= \u2502\n\u2502 sk-proj-abc123...                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u2502 entrypoint.sh mapping\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Container Runtime (after mapping)   \u2502\n\u2502                                     \u2502\n\u2502 OPENAI_API_KEY=sk-proj-abc123...   \u2502\n\u2502 (unprefixed - what MCP expects)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Graphiti MCP Server                 \u2502\n\u2502 \u2713 Reads standard OPENAI_API_KEY    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reference/developer-notes.html#adding-new-configuration-variables","title":"Adding New Configuration Variables","text":"<p>When adding a new config variable:</p> <ol> <li>Add to <code>.env.example</code> with <code>MADEINOZ_KNOWLEDGE_</code> prefix</li> <li>Add mapping in <code>entrypoint.sh</code>:</li> </ol> <pre><code>test -n \"$MADEINOZ_KNOWLEDGE_NEW_VAR\" &amp;&amp; export NEW_VAR=\"$MADEINOZ_KNOWLEDGE_NEW_VAR\"\n</code></pre> <ol> <li>Document in <code>docs/reference/configuration.md</code></li> <li>Test both prefixed and unprefixed modes</li> </ol>"},{"location":"reference/developer-notes.html#network-alias-fix-fixed","title":"Network Alias Fix (Fixed)","text":"<p>Issue: Podman requires explicit <code>--network-alias</code> for DNS service discovery Impact: MCP container couldn't resolve <code>bolt://neo4j:7687</code> or <code>redis://falkordb:6379</code> Fixed in: <code>src/skills/server/server-cli.ts</code></p> <p>Solution:</p> <pre><code>// FalkorDB\n\"--network-alias=falkordb\",  // DNS alias for service discovery\n\n// Neo4j\n\"--network-alias=neo4j\",  // DNS alias for service discovery\n</code></pre> <p>Docker handles this automatically, but Podman requires explicit aliases.</p>"},{"location":"reference/developer-notes.html#volume-mount-conflict-fixed","title":"Volume Mount Conflict (Fixed)","text":"<p>Issue: Read-only volume mounts conflicted with entrypoint.sh config selection Impact: MCP container crash-looped with \"Read-only file system\" error Fixed in: <code>src/skills/server/server-cli.ts</code></p> <p>Before (broken):</p> <pre><code>args.push(`-v=${configPath}:/app/mcp/config/config.yaml:ro`);\nargs.push(`-v=${patchesDir}/factories.py:/app/mcp/src/services/factories.py:ro`);\n</code></pre> <p>After (fixed):</p> <pre><code>// Config files and patches are baked into the custom image\n// The entrypoint.sh selects the correct config based on DATABASE_TYPE\n</code></pre> <p>Since the custom image has all configs and patches baked in, external mounts are not needed.</p>"},{"location":"reference/developer-notes.html#future-improvements","title":"Future Improvements","text":""},{"location":"reference/developer-notes.html#when-upstream-issues-are-resolved","title":"When Upstream Issues Are Resolved","text":"<ul> <li>[ ] Migrate back to official upstream images</li> <li>[ ] Remove custom Dockerfile</li> <li>[ ] Archive patches for reference</li> <li>[ ] Update documentation</li> <li>[ ] Simplify container.ts image references</li> </ul>"},{"location":"reference/developer-notes.html#potential-enhancements","title":"Potential Enhancements","text":"<ul> <li>[ ] Add health check retries with exponential backoff</li> <li>[ ] Implement multi-stage Docker build for smaller image</li> <li>[ ] Add version pinning for reproducible builds</li> <li>[ ] Create integration tests for patch functionality</li> <li>[ ] Add automated upstream tracking (check for merged PRs)</li> </ul> <p>Last Updated: 2026-01-21 Maintainer: @madeinoz67 Upstream: getzep/graphiti</p>"},{"location":"reference/model-guide.html","title":"Ollama Model Compatibility Guide","text":""},{"location":"reference/model-guide.html#ollama-model-compatibility-guide","title":"Ollama Model Compatibility Guide","text":"<p>This guide documents which Ollama models work with the Madeinoz Knowledge System for entity extraction and embeddings.</p>"},{"location":"reference/model-guide.html#architecture-overview","title":"Architecture Overview","text":"<p>The knowledge system uses two types of AI models:</p> Component Purpose Requirements LLM Entity extraction, relationship detection Must output valid JSON matching Pydantic schemas Embedder Vector embeddings for semantic search Must support <code>/v1/embeddings</code> endpoint"},{"location":"reference/model-guide.html#recommended-configuration","title":"Recommended Configuration","text":"<p>For cost savings while maintaining reliability, we recommend a hybrid setup:</p>"},{"location":"reference/model-guide.html#option-1-free-trinity-models-recommended-for-cost-savings","title":"Option 1: Free Trinity Models (Recommended for cost savings)","text":"<p>Option 1a: Trinity Large Preview (More detailed extraction)</p> <pre><code># LLM: Trinity Large Preview (FREE, passes all tests)\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=arcee-ai/trinity-large-preview:free\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=your-openrouter-api-key\n\n# Embedder: Ollama (free local embeddings)\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://your-ollama-server:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>Option 1b: Trinity Mini (Faster processing)</p> <pre><code># LLM: Trinity Mini (FREE, faster processing ~16s)\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=arcee-ai/trinity-mini:free\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=your-openrouter-api-key\n\n# Embedder: Ollama (free local embeddings)\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://your-ollama-server:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>Both configurations: - Use Trinity models via OpenRouter for LLM (completely free) - Successfully pass all Graphiti entity extraction tests - Use Ollama for embeddings (free, runs locally) - Zero cloud LLM costs while maintaining extraction quality</p> <p>Trinity Large Preview: More detailed entity extraction, ~25s processing time Trinity Mini: Faster processing ~16s, good quality extraction</p>"},{"location":"reference/model-guide.html#option-2-gemini-flash-fast-reliable-retiring-march-2026","title":"Option 2: Gemini Flash (Fast, reliable - RETIRING MARCH 2026)","text":"<pre><code># LLM: gemini-2.0-Flash (reliable JSON output for entity extraction)\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=google/gemini-2.0-flash-001\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=your-openrouter-api-key\n\n# Embedder: Ollama (free local embeddings)\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://your-ollama-server:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>This configuration:</p> <ul> <li>Uses gemini-2.0-flash-001 via OpenRouter for LLM (reliable and cost effective)</li> <li>WARNING: Gemini will be RETIRED in March 2026 - use Trinity as alternative</li> <li>Note: <code>LLM_PROVIDER=openai</code> because OpenRouter is OpenAI-compatible</li> <li>Uses Ollama for embeddings (free, runs locally)</li> <li>Reduces cloud costs while maintaining extraction quality</li> </ul>"},{"location":"reference/model-guide.html#cost-comparison","title":"Cost Comparison","text":"Configuration LLM Cost Embedding Cost Total Full OpenAI ~$0.15/1M tokens ~$0.02/1M tokens $$$ Hybrid (recommended) ~$0.15/1M tokens Free $$ Full Ollama Free Free Free* <p>*Full Ollama has reliability trade-offs for entity extraction.</p>"},{"location":"reference/model-guide.html#model-test-results","title":"Model Test Results","text":"<p>Tested on: 2026-01-18</p>"},{"location":"reference/model-guide.html#basic-json-extraction-test","title":"Basic JSON Extraction Test","text":"<p>We tested 16 Ollama LLM models for basic JSON extraction capability. This test uses a simple entity extraction prompt to evaluate JSON output quality.</p>"},{"location":"reference/model-guide.html#passed-15-models","title":"Passed (15 models)","text":"Model Entities Relationships Response Time Deepseek-r1:8b 5 4 3,164ms mistral:instruct 4 3 3,210ms tulu3:latest 4 2 3,742ms llama3.1:latest 4 1 3,837ms mistral:latest 3 1 4,257ms phi4:latest 4 3 5,459ms qwen3-coder:latest 5 4 5,852ms Deepseek-r1:latest 5 4 5,896ms deepseek-coder-v2:latest 4 2 6,243ms gemma2:9b 5 2 6,802ms dolphin-mistral 5 1 6,844ms phi3:medium 5 2 7,993ms Qwen3:latest 5 3 9,581ms codestral:latest 4 3 10,108ms Qwen3:8b 5 3 16,005ms"},{"location":"reference/model-guide.html#failed-1-model","title":"Failed (1 model)","text":"Model Reason Llama3.2:latest Truncated JSON response"},{"location":"reference/model-guide.html#important-caveats","title":"Important Caveats","text":"<p>Basic Test vs Real Usage: The test above uses a simplified entity extraction prompt. Graphiti uses more complex Pydantic schemas with specific field requirements. Models that pass the basic test may still fail with Graphiti's actual schemas.</p> <p>Observed Issues in Production:</p> Model Basic Test Graphiti Production Issue Llama3.2:latest \u274c \u274c Truncated responses Deepseek-r1:8b \u2705 \u274c ValidationError on NodeResolutions - outputs schema instead of data Deepseek-r1:latest \u2705 \u274c ValidationError on NodeResolutions Mistral \u2705 \u274c Malformed JSON on ExtractedEdges <p>Latest Test (2026-01-18): Deepseek-r1:8b tested with actual Graphiti schemas:</p> <pre><code>Error processing queued episode: 1 validation error for NodeResolutions\nentity_resolutions\n  Field required [type=missing, input_value={'$defs': {'NodeDuplicate...\n</code></pre> <p>The model outputs JSON schema definitions instead of data conforming to the schema.</p> <p>Recommendation: For production LLM use, choose from these tested options: - Free: <code>arcee-ai/trinity-large-preview:free</code> (passes all tests) Low-cost: <code>google/gemini-2.0-flash-001</code> (fast, reliable - but RETIRING March 2026) - Premium: OpenAI models (gpt-4o-mini, gpt-4o) which reliably produce valid JSON matching Graphiti's Pydantic schemas</p>"},{"location":"reference/model-guide.html#embedding-models","title":"Embedding Models","text":""},{"location":"reference/model-guide.html#performance-comparison-tested-2026-01-18","title":"Performance Comparison (Tested 2026-01-18)","text":"Rank Model Quality Speed Dimensions \ud83e\udd47 mxbai-embed-large 77.0% 156ms 1024 \ud83e\udd48 nomic-embed-text-v2-moe 76.4% 2507ms 768 \ud83e\udd49 embeddinggemma 75.8% 384ms 768 4 qwen3-embedding:0.6b 73.3% 312ms 1024 5 nomic-embed-text 66.0% 426ms 768 <p>Quality Score: Based on semantic similarity tests (higher = better at distinguishing similar vs dissimilar content)</p>"},{"location":"reference/model-guide.html#recommended-configuration_1","title":"Recommended Configuration","text":"<pre><code>MADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://your-ollama-server:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>Why mxbai-embed-large?</p> <ul> <li>Fastest response time (156ms avg)</li> <li>Highest semantic quality (77.0%)</li> <li>Higher dimensions (1024) capture more nuance</li> </ul>"},{"location":"reference/model-guide.html#ollama-vs-openai-embeddings-tested-2026-01-18","title":"Ollama vs OpenAI Embeddings (Tested 2026-01-18)","text":"<p>Direct comparison between Ollama mxbai-embed-large and OpenAI text-embedding-3-small:</p> Test Case Expected Ollama OpenAI Winner Cat/Feline synonyms high 80.8% 75.7% Ollama Job title synonyms high 93.5% 87.1% Ollama Weather vs Programming low 43.4% 22.7% OpenAI ML/DL related high 73.8% 79.9% OpenAI Different countries medium 61.7% 57.9% Tie Finance vs Personal low 28.5% 19.9% OpenAI <p>Summary:</p> Metric Ollama mxbai-embed-large OpenAI text-embedding-3-small Dimensions 1024 1536 Avg response time ~21ms ~610ms Quality wins 3 2 Cost Free ~$0.02/1M tokens <p>Verdict: Ollama mxbai-embed-large wins on quality (3-2), is 29x faster, and is completely free. This makes the hybrid configuration (OpenAI LLM + Ollama embeddings) the clear choice.</p>"},{"location":"reference/model-guide.html#full-ollama-configuration-experimental","title":"Full Ollama Configuration (Experimental)","text":"<p>If you want to run both LLM and embeddings on Ollama (completely free, no cloud costs):</p> <pre><code># LLM Configuration (Ollama)\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=mistral:instruct\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=http://your-ollama-server:11434/v1\n\n# Embedder Configuration (Ollama)\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://your-ollama-server:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>Warning: Full Ollama mode may have reliability issues with entity extraction due to JSON output formatting. Monitor logs for validation errors.</p>"},{"location":"reference/model-guide.html#best-models-for-full-ollama-mode","title":"Best Models for Full Ollama Mode","text":"<p>Based on our testing, if you must use Ollama for LLM, try these in order:</p> <ol> <li>mistral:instruct - Fast (3.2s), good JSON compliance</li> <li>Deepseek-r1:8b - Fast (3.1s), extracts more relationships</li> <li>phi4:latest - Good balance of speed and quality</li> <li>qwen3-coder:latest - Thorough extraction, slower</li> </ol>"},{"location":"reference/model-guide.html#running-the-model-test","title":"Running the Model Test","text":"<p>To test models on your Ollama server:</p> <pre><code>cd src/server\nbun test-ollama-models.ts\n</code></pre> <p>Results are saved to <code>test-results.json</code>.</p>"},{"location":"reference/model-guide.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/model-guide.html#using-embedder-provider-openai-in-logs","title":"\"Using Embedder provider: openai\" in logs","text":"<p>This is expected. The provider label indicates the API format (OpenAI-compatible), not the actual service. Check the HTTP request logs to verify the actual endpoint:</p> <pre><code>POST http://your-server:11434/v1/embeddings \"HTTP/1.1 200 OK\"  # Ollama\nPOST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"   # OpenAI\n</code></pre>"},{"location":"reference/model-guide.html#validationerror-on-entity-extraction","title":"ValidationError on entity extraction","text":"<p>Local models may produce JSON that doesn't match Graphiti's exact Pydantic schema requirements. Solutions:</p> <ol> <li>Switch to OpenAI for LLM (recommended)</li> <li>Try a different local model</li> <li>Use hybrid mode (OpenAI LLM + Ollama embeddings)</li> </ol>"},{"location":"reference/model-guide.html#model-not-found","title":"Model not found","text":"<p>Ensure the model is pulled on your Ollama server:</p> <pre><code>ollama pull mistral:instruct\nollama pull mxbai-embed-large\n</code></pre>"},{"location":"reference/model-guide.html#connection-refused","title":"Connection refused","text":"<p>Check that Ollama is running and accessible:</p> <pre><code>curl http://your-ollama-server:11434/api/tags\n</code></pre>"},{"location":"reference/model-guide.html#openai-compatible-cloud-providers","title":"OpenAI-Compatible Cloud Providers","text":"<p>In addition to Ollama (local), the Madeinoz patch supports OpenAI-compatible cloud providers:</p>"},{"location":"reference/model-guide.html#supported-providers","title":"Supported Providers","text":"Provider Base URL Best For OpenRouter <code>https://openrouter.ai/api/v1</code> Access to 200+ models (Claude, GPT-4, Llama) Together AI <code>https://api.together.xyz/v1</code> Fast Llama inference Fireworks AI <code>https://api.fireworks.ai/inference/v1</code> Low latency DeepInfra <code>https://api.deepinfra.com/v1/openai</code> Serverless GPUs"},{"location":"reference/model-guide.html#configuration-example-openrouter","title":"Configuration Example (OpenRouter)","text":"<pre><code># LLM: OpenRouter\nMADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=anthropic/claude-3.5-sonnet\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-or-v1-your-openrouter-key\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\n\n# Embedder: Ollama (free, local)\nMADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://your-ollama-server:11434/v1\nMADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large\nMADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024\n</code></pre> <p>This configuration:</p> <ul> <li>Uses OpenRouter for LLM (access to Claude, GPT-4, etc.)</li> <li>Uses Ollama for embeddings (free, runs locally)</li> <li>Gives you flexibility to choose any model on OpenRouter</li> </ul>"},{"location":"reference/model-guide.html#free-cloud-models-openrouter","title":"Free Cloud Models (OpenRouter)","text":"<p>Several free models on OpenRouter work correctly with Graphiti. These are excellent options for cost-conscious users.</p>"},{"location":"reference/model-guide.html#tested-free-models","title":"Tested Free Models","text":"Model Status Notes Test Date arcee-ai/trinity-large-preview:free \u2705 PASSING Reliable JSON output, recommended 2026-02-03 google/gemini-2.0-flash-001 \u2705 PASSING Fast, reliable, but RETIRING March 2026 2026-02-03 z-ai/glm-4.5-air:free \u274c FAILING ValidationError on ExtractedEntities 2026-02-03"},{"location":"reference/model-guide.html#trinity-model-test-results","title":"Trinity Model Test Results","text":"<p>Model: <code>arcee-ai/trinity-large-preview:free</code> (via OpenRouter)</p> <p>Test Results (2026-02-03):</p> Test Result Details Episode processing \u2705 PASS All episodes processed successfully Entity extraction \u2705 PASS Proper JSON schema compliance Relationship extraction \u2705 PASS Entities and relationships extracted correctly Validation errors \u2705 NONE No ValidationError for ExtractedEntities <p>Example Test Episodes: - SQL Databases \u2192 Extracted: \"SQL\", \"tables\", \"primary keys\", \"foreign keys\" - GraphQL \u2192 Extracted: \"GraphQL\", \"query language\", \"strongly typed schemas\" - Black Holes \u2192 Extracted: \"Black holes\", \"event horizon\", \"Sagittarius A*\"</p> <p>Configuration:</p> <pre><code>MADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai\nMADEINOZ_KNOWLEDGE_MODEL_NAME=arcee-ai/trinity-large-preview:free\nMADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1\nMADEINOZ_KNOWLEDGE_OPENAI_API_KEY=your-openrouter-api-key\n</code></pre> <p>Advantages: - Completely free - no LLM costs - Reliable entity extraction - No JSON validation errors - Works with OpenRouter's free tier</p> <p>Comparison with Paid Models:</p> Model Cost Speed Quality Trinity (free) $0 Medium Good Gemini 2.0 Flash ~$0.07/1M tokens Fast Excellent GPT-4o-mini ~$0.15/1M tokens Fast Excellent"},{"location":"reference/model-guide.html#failing-free-models","title":"Failing Free Models","text":"<p>GLM Models (<code>z-ai/glm-4.5-air:free</code> and variants)</p> <p>These models fail with Graphiti due to JSON output issues:</p> <pre><code>ValidationError: 1 validation error for ExtractedEntities\n</code></pre> <p>Issue: GLM models return text or malformed JSON instead of the structured Pydantic schema required by Graphiti.</p> <p>Workaround: Use Trinity, Gemini, or paid models (GPT-4o-mini, Claude) instead.</p> <p>Avoid GLM Models</p> <p>Do not use <code>z-ai/glm-*</code> models with this knowledge system. They consistently fail entity extraction due to incompatible JSON output.</p>"},{"location":"reference/model-guide.html#running-the-interactive-installer","title":"Running the Interactive Installer","text":"<p>The easiest way to configure OpenAI-compatible providers is through the interactive installer:</p> <pre><code>cd src/server\nbun run install.ts\n</code></pre> <p>The installer will:</p> <ol> <li>Ask you to select \"OpenAI-compatible (OpenRouter, Together, etc.)\"</li> <li>Let you choose a specific provider</li> <li>Prompt for the API key</li> <li>Offer Ollama or OpenAI for embeddings</li> <li>Let you select models from the provider's catalog</li> </ol>"},{"location":"reference/model-guide.html#references","title":"References","text":"<ul> <li>Graphiti GitHub Issue #1116 - Ollama compatibility</li> <li>Ollama API Documentation</li> <li>OpenAI-compatible endpoints</li> <li>OpenRouter Documentation</li> <li>Together AI Documentation</li> <li>Fireworks AI Documentation</li> <li>DeepInfra Documentation</li> </ul>"},{"location":"reference/observability.html","title":"Observability & Metrics","text":""},{"location":"reference/observability.html#observability-metrics","title":"Observability &amp; Metrics","text":"<p>The Madeinoz Knowledge System exports Prometheus metrics for monitoring LLM API usage, token consumption, and costs. This enables integration with existing observability infrastructure.</p>"},{"location":"reference/observability.html#overview","title":"Overview","text":"<p>Metrics are exported via OpenTelemetry with a Prometheus exporter. The system tracks:</p> <ul> <li>Token usage - Input, output, and total tokens per model</li> <li>API costs - Real-time cost tracking in USD</li> <li>Cache statistics - Hit rates, tokens saved, cost savings (when caching is enabled)</li> <li>Memory decay - Lifecycle states, maintenance operations, classification performance (Feature 009)</li> </ul>"},{"location":"reference/observability.html#quick-start","title":"Quick Start","text":""},{"location":"reference/observability.html#accessing-metrics","title":"Accessing Metrics","text":"<p>The metrics endpoint is exposed at:</p> Environment Endpoint Development <code>http://localhost:9091/metrics</code> Production <code>http://localhost:9090/metrics</code>"},{"location":"reference/observability.html#basic-query","title":"Basic Query","text":"<pre><code># Fetch all metrics\ncurl http://localhost:9091/metrics\n\n# Filter to graphiti metrics only\ncurl -s http://localhost:9091/metrics | grep \"^graphiti_\"\n</code></pre>"},{"location":"reference/observability.html#configuration","title":"Configuration","text":""},{"location":"reference/observability.html#environment-variables","title":"Environment Variables","text":"<p>Add these to your <code>~/.claude/.env</code> file:</p> <pre><code># Enable/disable metrics collection (default: true)\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_METRICS_ENABLED=true\n\n# Enable detailed per-request logging (default: false)\n# Set LOG_LEVEL=DEBUG to see metrics in logs\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_LOG_REQUESTS=false\n\n# Enable/disable prompt caching (default: false)\n# Note: Currently blocked due to OpenRouter API limitation\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED=false\n</code></pre>"},{"location":"reference/observability.html#restart-after-configuration","title":"Restart After Configuration","text":"<pre><code>bun run server-cli stop\nbun run server-cli start\n</code></pre>"},{"location":"reference/observability.html#available-metrics","title":"Available Metrics","text":""},{"location":"reference/observability.html#token-counters","title":"Token Counters","text":"<p>Track cumulative token usage across all requests.</p> Metric Labels Description <code>graphiti_prompt_tokens_total</code> <code>model</code> Total input/prompt tokens <code>graphiti_completion_tokens_total</code> <code>model</code> Total output/completion tokens <code>graphiti_total_tokens_total</code> <code>model</code> Total tokens (prompt + completion) <code>graphiti_prompt_tokens_all_models_total</code> - Input tokens across all models <code>graphiti_completion_tokens_all_models_total</code> - Output tokens across all models <code>graphiti_total_tokens_all_models_total</code> - Total tokens across all models"},{"location":"reference/observability.html#cost-counters","title":"Cost Counters","text":"<p>Track cumulative API costs in USD.</p> Metric Labels Description <code>graphiti_api_cost_total</code> <code>model</code> Total API cost per model <code>graphiti_api_input_cost_total</code> <code>model</code> Input/prompt cost per model <code>graphiti_api_output_cost_total</code> <code>model</code> Output/completion cost per model <code>graphiti_api_cost_all_models_total</code> - Total cost across all models <code>graphiti_api_input_cost_all_models_total</code> - Input cost across all models <code>graphiti_api_output_cost_all_models_total</code> - Output cost across all models"},{"location":"reference/observability.html#token-histograms","title":"Token Histograms","text":"<p>Track per-request token distributions for percentile analysis.</p> Metric Bucket Range Description <code>graphiti_prompt_tokens_per_request</code> 10 - 200,000 Input tokens per request <code>graphiti_completion_tokens_per_request</code> 10 - 200,000 Output tokens per request <code>graphiti_total_tokens_per_request</code> 10 - 200,000 Total tokens per request <p>Token bucket boundaries:</p> <pre><code>10, 25, 50, 100, 250, 500, 1000, 2000, 3000, 5000, 10000, 25000, 50000, 100000, 200000\n</code></pre>"},{"location":"reference/observability.html#cost-histograms","title":"Cost Histograms","text":"<p>Track per-request cost distributions for percentile analysis.</p> Metric Bucket Range Description <code>graphiti_api_cost_per_request</code> $0.000005 - $5.00 Total cost per request <code>graphiti_api_input_cost_per_request</code> $0.000005 - $5.00 Input cost per request <code>graphiti_api_output_cost_per_request</code> $0.000005 - $5.00 Output cost per request <p>Cost bucket boundaries:</p> <pre><code>$0.000005, $0.00001, $0.000025, $0.00005, $0.0001, $0.00025, $0.0005, $0.001,\n$0.0025, $0.005, $0.01, $0.025, $0.05, $0.1, $0.25, $0.5, $1.0, $2.5, $5.0\n</code></pre> <p>Bucket coverage by model tier:</p> Range Model Examples $0.000005 - $0.01 Gemini Flash, GPT-4o-mini $0.01 - $0.10 GPT-4o, Claude Sonnet $0.10 - $1.00 GPT-4, Claude Opus $1.00 - $5.00 Large context on expensive models"},{"location":"reference/observability.html#gauge-metrics","title":"Gauge Metrics","text":"<p>Track current state values.</p> Metric Values Description <code>graphiti_cache_enabled</code> 0 or 1 Whether prompt caching is enabled <code>graphiti_cache_hit_rate</code> 0-100 Current session cache hit rate (%)"},{"location":"reference/observability.html#cache-metrics-when-enabled","title":"Cache Metrics (When Enabled)","text":"<p>These metrics populate when <code>MADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED=true</code>:</p> Metric Labels Description <code>graphiti_cache_hits_total</code> <code>model</code> Cache hits per model <code>graphiti_cache_misses_total</code> <code>model</code> Cache misses per model <code>graphiti_cache_tokens_saved_total</code> <code>model</code> Tokens saved via caching <code>graphiti_cache_cost_saved_total</code> <code>model</code> Cost savings from caching (USD) <code>graphiti_cache_write_tokens_total</code> <code>model</code> Tokens written to cache (cache creation) <p>Cache Savings Histograms:</p> Metric Labels Description <code>graphiti_cache_tokens_saved_per_request</code> <code>model</code> Distribution of tokens saved per cache hit <code>graphiti_cache_cost_saved_per_request</code> <code>model</code> Distribution of cost saved per cache hit (USD) <p>Prompt Caching via OpenRouter</p> <p>Prompt caching is available for Gemini models via OpenRouter. The system uses explicit <code>cache_control</code> markers (similar to Anthropic's approach) with a minimum of 1,024 tokens. To enable caching, set <code>MADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED=true</code>. See Prompt Caching for details.</p>"},{"location":"reference/observability.html#duration-metrics","title":"Duration Metrics","text":"<p>Track LLM request latency for performance monitoring.</p> Metric Labels Description <code>graphiti_llm_request_duration_seconds</code> <code>model</code> Distribution of LLM request latency <p>Duration bucket boundaries (seconds):</p> <pre><code>0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 15.0, 30.0, 60.0, 120.0, 300.0\n</code></pre> <p>Bucket coverage:</p> Range Request Type 0.05s - 1s Cached/simple requests 1s - 10s Typical LLM calls 10s - 60s Complex reasoning, large context 60s - 300s Timeout territory"},{"location":"reference/observability.html#error-metrics","title":"Error Metrics","text":"<p>Track LLM API errors for reliability monitoring.</p> Metric Labels Description <code>graphiti_llm_errors_total</code> <code>model</code>, <code>error_type</code> Error count by model and type <code>graphiti_llm_errors_all_models_total</code> - Total errors across all models <p>Error types:</p> <ul> <li><code>rate_limit</code> - API rate limit exceeded</li> <li><code>timeout</code> - Request timeout</li> <li><code>BadRequestError</code>, <code>APIError</code>, etc. - Exception class names</li> </ul> <p>Error Metrics Visibility</p> <p>Error counters only appear in Prometheus after at least one error has been recorded. If you don't see these metrics, it means no LLM errors have occurred.</p>"},{"location":"reference/observability.html#throughput-metrics","title":"Throughput Metrics","text":"<p>Track episode processing volume.</p> Metric Labels Description <code>graphiti_episodes_processed_total</code> <code>group_id</code> Episodes processed per group <code>graphiti_episodes_processed_all_groups_total</code> - Total episodes across all groups <p>Throughput Metrics Integration</p> <p>Episode metrics require integration into the MCP tool handler and may not be active in all deployments.</p>"},{"location":"reference/observability.html#memory-decay-metrics-feature-009","title":"Memory Decay Metrics (Feature 009)","text":"<p>The memory decay system tracks lifecycle state transitions, maintenance operations, and classification performance. These metrics use the <code>knowledge_</code> prefix.</p>"},{"location":"reference/observability.html#health-endpoint","title":"Health Endpoint","text":"<p>A dedicated health endpoint provides decay system status:</p> <pre><code>curl http://localhost:9090/health/decay\n</code></pre> <p>Returns:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"decay_enabled\": true,\n  \"last_maintenance\": \"2026-01-28T12:00:00Z\",\n  \"metrics_endpoint\": \"/metrics\"\n}\n</code></pre>"},{"location":"reference/observability.html#maintenance-metrics","title":"Maintenance Metrics","text":"<p>Track scheduled maintenance operations that recalculate decay scores and transition lifecycle states.</p> Metric Labels Description <code>knowledge_decay_maintenance_runs_total</code> <code>status</code> Maintenance runs by status (success/failure) <code>knowledge_decay_scores_updated_total</code> - Decay scores recalculated <code>knowledge_maintenance_duration_seconds</code> - Maintenance run duration (histogram) <code>knowledge_memories_purged_total</code> - Soft-deleted memories permanently removed <p>Duration bucket boundaries (seconds):</p> <pre><code>1, 5, 30, 60, 120, 300, 600\n</code></pre> <p>Performance target: Complete within 10 minutes (600 seconds).</p>"},{"location":"reference/observability.html#lifecycle-metrics","title":"Lifecycle Metrics","text":"<p>Track state transitions as memories age or are accessed.</p> Metric Labels Description <code>knowledge_lifecycle_transitions_total</code> <code>from_state</code>, <code>to_state</code> State transitions by type <code>knowledge_memories_by_state</code> <code>state</code> Current count per lifecycle state <code>knowledge_memories_total</code> - Total memory count (excluding soft-deleted) <p>Lifecycle states:</p> State Description <code>ACTIVE</code> Recently accessed, full relevance <code>DORMANT</code> Not accessed for 30+ days <code>ARCHIVED</code> Not accessed for 90+ days <code>EXPIRED</code> Marked for deletion <code>SOFT_DELETED</code> Deleted but recoverable for 90 days <code>PERMANENT</code> High importance + stability, never decays"},{"location":"reference/observability.html#classification-metrics","title":"Classification Metrics","text":"<p>Track LLM-based importance/stability classification.</p> Metric Labels Description <code>knowledge_classification_requests_total</code> <code>status</code> Classification attempts (success/failure/fallback) <code>knowledge_classification_latency_seconds</code> - LLM response time (histogram) <p>Latency bucket boundaries (seconds):</p> <pre><code>0.1, 0.5, 1, 2, 5\n</code></pre> <p>Classification statuses:</p> Status Description <code>success</code> LLM classified successfully <code>failure</code> LLM call failed, used defaults <code>fallback</code> LLM unavailable, used defaults"},{"location":"reference/observability.html#aggregate-metrics","title":"Aggregate Metrics","text":"<p>Track average scores across the knowledge graph.</p> Metric Description <code>knowledge_decay_score_avg</code> Average decay score (0.0-1.0) <code>knowledge_importance_avg</code> Average importance (1-5) <code>knowledge_stability_avg</code> Average stability (1-5)"},{"location":"reference/observability.html#search-metrics","title":"Search Metrics","text":"<p>Track weighted search operations that boost by relevance.</p> Metric Labels Description <code>knowledge_weighted_searches_total</code> - Weighted search operations <code>knowledge_search_weighted_latency_seconds</code> - Scoring overhead (histogram)"},{"location":"reference/observability.html#memory-access-pattern-metrics-feature-015","title":"Memory Access Pattern Metrics (Feature 015)","text":"<p>Track memory access patterns during search operations to validate decay scoring effectiveness.</p> Metric Labels Description <code>knowledge_access_by_importance_total</code> <code>level</code> Cumulative accesses by importance level (LOW/MEDIUM/HIGH/CRITICAL) <code>knowledge_access_by_state_total</code> <code>state</code> Cumulative accesses by lifecycle state (ACTIVE/DORMANT/ARCHIVED/PERMANENT) <code>knowledge_days_since_last_access</code> - Histogram of days since memory was last accessed <code>knowledge_reactivations_total</code> <code>from_state</code> Memories reactivated from DORMANT/ARCHIVED to ACTIVE <p>Importance level mapping:</p> Score Label Description 1-2 LOW Lower priority memories 3 MEDIUM Standard importance (default) 4 HIGH Important memories 5 CRITICAL Core/foundational memories <p>Days histogram bucket boundaries:</p> <pre><code>1, 7, 30, 90, 180, 365, 730, 1095\n</code></pre> Bucket Description 1 1 day ago 7 1 week ago 30 1 month ago 90 3 months ago 180 6 months (half-life threshold) 365 1 year ago 730 2 years ago 1095 3+ years ago <p>Metric Recording Behavior</p> <p>Access pattern metrics are recorded during <code>search_memory_nodes</code> and <code>search_memory_facts</code> operations. The histogram only records when nodes have a <code>last_accessed_at</code> attribute set.</p> <p>Access Pattern PromQL Queries:</p> <pre><code># Access rate by importance (per second)\nsum(rate(knowledge_access_by_importance_total[5m])) by (level)\n\n# Access distribution by state (current values)\nknowledge_access_by_state_total\n\n# Reactivation rate (last hour)\nincrease(knowledge_reactivations_total[1h])\n\n# Age distribution heatmap\nsum(rate(knowledge_days_since_last_access_bucket[5m])) by (le)\n\n# Access vs decay correlation (dual-axis)\n# Left axis: rate(knowledge_access_by_importance_total[5m])\n# Right axis: knowledge_decay_score_avg\n</code></pre>"},{"location":"reference/observability.html#example-promql-queries","title":"Example PromQL Queries","text":"<p>Maintenance success rate (last 24 hours):</p> <pre><code>sum(increase(knowledge_decay_maintenance_runs_total{status=\"success\"}[24h]))\n/\nsum(increase(knowledge_decay_maintenance_runs_total[24h]))\n</code></pre> <p>State distribution:</p> <pre><code>knowledge_memories_by_state\n</code></pre> <p>Classification fallback rate:</p> <pre><code>sum(rate(knowledge_classification_requests_total{status=\"fallback\"}[5m]))\n/\nsum(rate(knowledge_classification_requests_total[5m]))\n</code></pre> <p>Lifecycle transitions per hour:</p> <pre><code>sum by (from_state, to_state) (increase(knowledge_lifecycle_transitions_total[1h]))\n</code></pre> <p>P95 classification latency:</p> <pre><code>histogram_quantile(0.95, rate(knowledge_classification_latency_seconds_bucket[5m]))\n</code></pre>"},{"location":"reference/observability.html#alert-rules","title":"Alert Rules","text":"<p>Alert rules are defined in <code>config/monitoring/prometheus/alerts/knowledge.yml</code>:</p> Alert Condition Severity <code>MaintenanceTimeout</code> Duration &gt; 10 minutes warning <code>MaintenanceFailed</code> Any failure in last hour critical <code>ClassificationDegraded</code> Fallback rate &gt; 20% warning <code>ExcessiveExpiration</code> &gt; 100 expired/hour warning <code>SoftDeleteBacklog</code> &gt; 1000 awaiting purge warning"},{"location":"reference/observability.html#prometheus-integration","title":"Prometheus Integration","text":""},{"location":"reference/observability.html#metrics-naming-conventions","title":"Metrics Naming Conventions","text":"<p>The system follows OpenTelemetry Semantic Conventions for metric naming:</p> Convention Implementation Units in metadata Units specified via <code>unit</code> field in Grafana, not in metric names No unit suffixes Metrics use <code>_total</code> for counters, not <code>_cost_total_usd</code> or <code>_tokens_total_count</code> Descriptive base Metric names describe what is measured (e.g., <code>api_cost</code>, <code>total_tokens</code>) Counter suffix All cumulative counters use <code>_total</code> suffix per OpenTelemetry convention <p>Examples of correct naming:</p> Metric Correct Incorrect API cost <code>graphiti_api_cost_total</code> <code>graphiti_api_cost_USD_total</code> Cache hit rate <code>graphiti_cache_hit_rate</code> <code>graphiti_cache_hit_rate_percent</code> Tokens saved <code>graphiti_cache_tokens_saved_total</code> <code>graphiti_cache_tokens_saved_count</code> <p>Dashboard unit configuration:</p> <p>Instead of embedding units in metric names, Grafana dashboards use the <code>unit</code> field to display appropriate units: - <code>currencyUSD</code> - Cost metrics display in USD - <code>short</code> - Count metrics display as plain numbers - <code>percent</code> - Rate metrics display as percentages - <code>seconds</code> - Duration metrics display in seconds - <code>locale</code> - Token count display with locale formatting</p>"},{"location":"reference/observability.html#handling-service-restarts","title":"Handling Service Restarts","text":"<p>Counter metrics reset to zero when the service restarts, which causes <code>rate()</code> calculations to show brief gaps or spikes in visualizations. This is expected Prometheus behavior for counter resets.</p> <p>Current dashboard behavior:</p> <ul> <li><code>rate()</code> queries will briefly show gaps during counter resets</li> <li>Grafana automatically interpolates across short gaps</li> <li>For longer gaps, consider increasing the scrape interval</li> </ul> <p>Note: Time-over-time functions like <code>max_over_time()</code> cannot wrap <code>rate()</code> results in PromQL. They must wrap range vector selectors directly (e.g., <code>max_over_time(metric[1h])</code>). For rate-based metrics, accepting brief gaps during restarts is the standard approach.</p>"},{"location":"reference/observability.html#scrape-configuration","title":"Scrape Configuration","text":"<p>Add to your <code>prometheus.yml</code>:</p> <pre><code>scrape_configs:\n  - job_name: 'madeinoz-knowledge'\n    static_configs:\n      - targets: ['localhost:9091']  # dev port\n    scrape_interval: 15s\n</code></pre>"},{"location":"reference/observability.html#example-promql-queries_1","title":"Example PromQL Queries","text":"<p>Token usage in last hour:</p> <pre><code>increase(graphiti_total_tokens_all_models_total[1h])\n</code></pre> <p>Tokens per model:</p> <pre><code>sum by (model) (increase(graphiti_total_tokens_total[1h]))\n</code></pre> <p>Total cost in last 24 hours:</p> <pre><code>increase(graphiti_api_cost_all_models_total[24h])\n</code></pre> <p>Cost per model:</p> <pre><code>sum by (model) (increase(graphiti_api_cost_total[24h]))\n</code></pre> <p>P95 cost per request:</p> <pre><code>histogram_quantile(0.95, rate(graphiti_api_cost_per_request_bucket[5m]))\n</code></pre> <p>P99 tokens per request:</p> <pre><code>histogram_quantile(0.99, rate(graphiti_total_tokens_per_request_bucket[5m]))\n</code></pre> <p>Median (P50) cost per request:</p> <pre><code>histogram_quantile(0.50, rate(graphiti_api_cost_per_request_bucket[5m]))\n</code></pre> <p>P95 request duration:</p> <pre><code>histogram_quantile(0.95, rate(graphiti_llm_request_duration_seconds_bucket[5m]))\n</code></pre> <p>Average request duration:</p> <pre><code>rate(graphiti_llm_request_duration_seconds_sum[5m]) / rate(graphiti_llm_request_duration_seconds_count[5m])\n</code></pre> <p>Error rate by model:</p> <pre><code>sum by (model) (rate(graphiti_llm_errors_total[5m]))\n</code></pre>"},{"location":"reference/observability.html#understanding-histogram-buckets","title":"Understanding Histogram Buckets","text":"<p>Prometheus histograms are cumulative. Each bucket shows the count of observations less than or equal to that boundary.</p> <p>Example output:</p> <pre><code>graphiti_api_cost_per_request_USD_bucket{le=\"0.0001\"} 2.0\ngraphiti_api_cost_per_request_USD_bucket{le=\"0.00025\"} 5.0\ngraphiti_api_cost_per_request_USD_bucket{le=\"0.0005\"} 5.0\n</code></pre> <p>Interpretation:</p> <ul> <li>2 requests cost \u2264 $0.0001</li> <li>3 more requests cost between $0.0001 and $0.00025</li> <li>0 requests cost more than $0.00025 (count stays at 5)</li> </ul>"},{"location":"reference/observability.html#grafana-dashboard","title":"Grafana Dashboard","text":"<p>The system includes a pre-configured Grafana dashboard with comprehensive monitoring panels.</p>"},{"location":"reference/observability.html#quick-start-development","title":"Quick Start (Development)","text":"<p>The development environment includes Prometheus and Grafana by default:</p> <pre><code># Start dev environment with monitoring\ndocker compose -f src/skills/server/docker-compose-neo4j-dev.yml up -d\n\n# Access points:\n# - Grafana: http://localhost:3002 (login: admin/admin)\n# - Prometheus UI: http://localhost:9092\n</code></pre>"},{"location":"reference/observability.html#production-setup-optional","title":"Production Setup (Optional)","text":"<p>Production monitoring uses Docker Compose profiles and is disabled by default:</p> <pre><code># Start with monitoring enabled\ndocker compose -f src/skills/server/docker-compose-neo4j.yml --profile monitoring up -d\n\n# Start without monitoring (default)\ndocker compose -f src/skills/server/docker-compose-neo4j.yml up -d\n\n# Access points (when enabled):\n# - Grafana: http://localhost:3001 (login: admin/admin or custom password)\n# - Prometheus UI: http://localhost:9092\n</code></pre> <p>Custom Grafana Password</p> <p>Set <code>GRAFANA_ADMIN_PASSWORD</code> environment variable for a secure password: </p><pre><code>export GRAFANA_ADMIN_PASSWORD=your-secure-password\ndocker compose -f src/skills/server/docker-compose-neo4j.yml --profile monitoring up -d\n</code></pre><p></p>"},{"location":"reference/observability.html#dashboard-panels","title":"Dashboard Panels","text":"<p>The pre-configured dashboard includes these sections:</p> <p>Overview Row:</p> <ul> <li>Total API Cost (USD)</li> <li>Total Tokens Used</li> <li>Cache Status (Enabled/Disabled)</li> <li>Cache Hit Rate (%)</li> <li>Total Errors</li> </ul> <p>Token Usage Row:</p> <ul> <li>Token Usage Rate (by Model) - Time series</li> <li>Prompt vs Completion Tokens - Stacked area</li> </ul> <p>Cost Tracking Row:</p> <ul> <li>Cost Rate ($/hour by Model) - Time series</li> <li>Cost by Model - Pie chart</li> <li>Input vs Output Cost - Donut chart</li> </ul> <p>Request Duration Row:</p> <ul> <li>Request Duration Percentiles (P50, P95, P99) - Time series</li> <li>Average Request Duration (by Model) - Bar chart</li> </ul> <p>Cache Performance Row:</p> <ul> <li>Cache Hit Rate Over Time - Time series</li> <li>Cache Cost Savings Rate - Time series</li> <li>Cache Hits vs Misses - Stacked area</li> </ul> <p>Errors Row:</p> <ul> <li>Error Rate (by Model &amp; Type) - Stacked bars</li> <li>Errors by Type - Pie chart</li> </ul>"},{"location":"reference/observability.html#port-assignments","title":"Port Assignments","text":"Environment Service Port Notes Development Grafana 3002 Neo4j backend Development Grafana 3003 FalkorDB backend (avoids UI conflict) Development Prometheus UI 9092 Query interface Production Grafana 3001 Neo4j backend Production Grafana 3002 FalkorDB backend Production Prometheus UI 9092 Query interface"},{"location":"reference/observability.html#available-dashboards","title":"Available Dashboards","text":"<p>The system includes multiple pre-configured Grafana dashboards:</p> Dashboard UID Purpose Graph Health <code>graph-health-dashboard</code> Entity states, episodes, operation rates, error tracking Memory Decay <code>memory-decay-dashboard</code> Lifecycle transitions, maintenance operations, classification metrics Memory Access Patterns <code>memory-access-dashboard</code> Access distribution by importance/state, reactivation tracking, decay correlation Knowledge System <code>madeinoz-knowledge</code> Token usage, cost tracking, request duration, cache performance Prompt Cache Effectiveness <code>prompt-cache-effectiveness</code> Cache ROI, hit/miss patterns, write overhead, per-model comparison Queue Processing Metrics <code>queue-metrics</code> Queue depth, latency, consumer health, throughput, errors"},{"location":"reference/observability.html#prompt-cache-effectiveness-dashboard","title":"Prompt Cache Effectiveness Dashboard","text":"<p>Purpose: Dedicated monitoring for Gemini prompt caching performance and ROI</p> <p>Access: <code>http://localhost:3002/d/prompt-cache-effectiveness</code> (dev)</p> <p>Panels:</p> Panel Metric Description Total Cost Savings <code>graphiti_cache_cost_saved_all_models_total</code> USD saved from caching (uses time-over-time for restart resilience) Hit Rate <code>graphiti_cache_hit_rate</code> Current cache hit percentage (gauge: &gt;50% green, 20-50% yellow, &lt;20% red) Tokens Saved <code>graphiti_cache_tokens_saved_all_models_total</code> Total tokens saved from caching Tokens Written <code>graphiti_cache_write_tokens_all_models_total</code> Tokens consumed to create cache entries (overhead) Savings Rate <code>rate(...[1h]) * 3600</code> Cost savings per hour trend Hit Rate Trend <code>graphiti_cache_hit_rate</code> Hit rate over time for anomaly detection Hits vs Misses Dual time series Comparison of cache hits vs misses rate Tokens Saved Distribution <code>graphiti_cache_tokens_saved_per_request_bucket</code> Heatmap showing cache hit size distribution Per-Model Performance Table Side-by-side comparison of caching by LLM model <p>Key Features: - Time-over-time queries (<code>max_over_time()[1h]</code>) handle service restarts without data gaps - Color-coded thresholds for quick health assessment - 30-second auto-refresh (user-configurable) - Single 1080p screen layout (no scrolling required)</p> <p>Troubleshooting Dashboard:</p> <ol> <li>No data showing: Verify cache is enabled (<code>curl http://localhost:9091/metrics | grep cache_enabled</code>)</li> <li>Gaps in charts: Check for service restarts - time-over-time functions should smooth gaps</li> <li>Zero hit rate: Normal for new deployments; requires repeated similar prompts to build cache</li> </ol>"},{"location":"reference/observability.html#memory-access-patterns-dashboard","title":"Memory Access Patterns Dashboard","text":"<p>Purpose: Validate decay scoring effectiveness by visualizing memory access patterns across importance levels, lifecycle states, and time periods</p> <p>Access: <code>http://localhost:3002/d/memory-access-dashboard</code> (dev)</p> <p>Panels:</p> Panel Metric Description Total Access Count <code>knowledge_memory_access_total</code> Cumulative memory accesses (uses max_over_time for restart resilience) Access Rate <code>rate(...[5m])</code> Current memory accesses per second Reactivations (Dormant) <code>knowledge_reactivations_total{from_state=\"DORMANT\"}</code> Memories revived from dormant state (thresholds: green=0, yellow=5, red=20) Reactivations (Archived) <code>knowledge_reactivations_total{from_state=\"ARCHIVED\"}</code> Memories revived from archived state (thresholds: green=0, yellow=3, red=10) Access by Importance <code>knowledge_access_by_importance_total</code> Pie chart showing access distribution by CRITICAL/HIGH/MEDIUM/LOW Access by State <code>knowledge_access_by_state_total</code> Pie chart showing access distribution by ACTIVE/STABLE/DORMANT/ARCHIVED Access Rate Over Time <code>rate(knowledge_memory_access_total[5m])</code> Time series trend of access velocity Age Distribution <code>knowledge_days_since_last_access_bucket</code> Heatmap showing when memories were last accessed Access vs Decay Correlation Dual-axis Compares access rate (left) with average decay score (right) <p>Key Features:</p> <ul> <li>Time-over-time queries (<code>max_over_time()[1h]</code>) handle service restarts without data gaps</li> <li>Dual-axis correlation panel for validating decay effectiveness</li> <li>Color-coded reactivation thresholds for quick anomaly detection</li> <li>30-second auto-refresh with 24-hour default time range</li> </ul> <p>Common Tasks:</p> <ol> <li>Validate Decay Scoring: Check if CRITICAL/HIGH importance memories have proportionally more accesses</li> <li>Tune Decay Parameters: Use age distribution heatmap to identify if 180-day half-life is appropriate</li> <li>Investigate Reactivations: High reactivation counts suggest decay is too aggressive</li> </ol>"},{"location":"reference/observability.html#customizing-dashboards","title":"Customizing Dashboards","text":"<p>Dashboard configurations are stored at:</p> <pre><code>config/monitoring/grafana/dashboards/\n\u251c\u2500\u2500 graph-health-dashboard.json\n\u251c\u2500\u2500 memory-access-dashboard.json\n\u251c\u2500\u2500 memory-decay-dashboard.json\n\u251c\u2500\u2500 madeinoz-knowledge.json\n\u2514\u2500\u2500 prompt-cache-effectiveness.json\n</code></pre> <p>To customize:</p> <ol> <li>Open Grafana and make changes via the UI</li> <li>Export the dashboard JSON (Share &gt; Export &gt; Save to file)</li> <li>Replace the provisioned dashboard file</li> <li>Restart Grafana to apply changes</li> </ol>"},{"location":"reference/observability.html#manual-panel-examples","title":"Manual Panel Examples","text":"<p>If building a custom dashboard, use these PromQL queries:</p> <p>Usage &amp; Cost:</p> <ol> <li>Token Usage Rate - <code>rate(graphiti_total_tokens_all_models_total[5m])</code></li> <li>Cost Rate ($/hour) - <code>rate(graphiti_api_cost_all_models_total[5m]) * 3600</code></li> <li>Request Cost Distribution - Histogram panel with <code>graphiti_api_cost_per_request_bucket</code></li> <li>Token Usage by Model - <code>sum by (model) (rate(graphiti_total_tokens_total[5m]))</code></li> </ol> <p>Performance:</p> <ol> <li>Request Duration P95 - <code>histogram_quantile(0.95, rate(graphiti_llm_request_duration_seconds_bucket[5m]))</code></li> <li>Request Duration Heatmap - Heatmap panel with <code>graphiti_llm_request_duration_seconds_bucket</code></li> <li>Error Rate - <code>sum(rate(graphiti_llm_errors_total[5m]))</code></li> </ol> <p>Caching (when enabled):</p> <ol> <li>Cache Hit Rate - <code>graphiti_cache_hit_rate</code> (gauge metric)</li> <li>Cost Savings Rate - <code>rate(graphiti_cache_cost_saved_all_models_total[5m]) * 3600</code></li> <li>Tokens Saved - <code>increase(graphiti_cache_tokens_saved_all_models_total[1h])</code></li> </ol>"},{"location":"reference/observability.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/observability.html#metrics-not-appearing","title":"Metrics Not Appearing","text":"<ol> <li> <p>Check metrics are enabled: </p><pre><code>grep MADEINOZ_KNOWLEDGE_PROMPT_CACHE_METRICS_ENABLED ~/.claude/.env\n</code></pre><p></p> </li> <li> <p>Verify endpoint is accessible: </p><pre><code>curl http://localhost:9091/metrics\n</code></pre><p></p> </li> <li> <p>Check container logs: </p><pre><code>docker logs madeinoz-knowledge-graph-mcp-dev 2&gt;&amp;1 | grep -i metric\n</code></pre><p></p> </li> </ol>"},{"location":"reference/observability.html#counters-not-incrementing","title":"Counters Not Incrementing","text":"<p>Counter and histogram metrics only appear after LLM API calls are made. Metrics populate when:</p> <ul> <li><code>add_memory</code> tool is used (triggers entity extraction)</li> <li>Any operation requiring LLM inference</li> </ul> <p>Search operations (<code>search_memory_facts</code>, <code>search_memory_nodes</code>) use embeddings only and do not increment LLM metrics.</p>"},{"location":"reference/observability.html#debug-logging","title":"Debug Logging","text":"<p>Enable detailed per-request logging:</p> <pre><code># In ~/.claude/.env\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_LOG_REQUESTS=true\nLOG_LEVEL=DEBUG\n</code></pre> <p>This shows per-request metrics in container logs:</p> <pre><code>\ud83d\udcca Metrics: prompt=1234, completion=567, cost=$0.000089, input_cost=$0.000062, output_cost=$0.000027\n</code></pre>"},{"location":"reference/observability.html#prompt-caching-gemini-via-openrouter","title":"Prompt Caching (Gemini via OpenRouter)","text":"<p>Prompt caching reduces API costs by up to 15-20% by reusing previously processed prompt content. The system adds explicit <code>cache_control</code> markers to requests when enabled, allowing OpenRouter to serve cached content at reduced cost (0.25x normal price).</p> <p>Note: Prompt caching is disabled by default and must be explicitly enabled via configuration.</p> <p>Developer Documentation</p> <p>For implementation details including architecture diagrams, code flow, and metrics internals, see the Cache Implementation Guide.</p>"},{"location":"reference/observability.html#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    First Request (Cache Miss)                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  System Prompt (800 tokens) \u2500\u2500\u25ba LLM processes \u2500\u2500\u25ba Cache stored  \u2502\n\u2502  User Message (200 tokens)  \u2500\u2500\u25ba LLM processes \u2500\u2500\u25ba Response      \u2502\n\u2502                                                                  \u2502\n\u2502  Cost: Full price for 1000 tokens                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Second Request (Cache Hit)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  System Prompt (800 tokens) \u2500\u2500\u25ba Retrieved from cache (0.25x)    \u2502\n\u2502  User Message (200 tokens)  \u2500\u2500\u25ba LLM processes \u2500\u2500\u25ba Response      \u2502\n\u2502                                                                  \u2502\n\u2502  Cost: 0.25x for cached 800 + full for 200 = 75% savings        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reference/observability.html#how-caching-works-via-openrouter","title":"How Caching Works via OpenRouter","text":"<p>The Madeinoz Knowledge System implements explicit prompt caching via OpenRouter using <code>cache_control</code> markers (similar to Anthropic's approach):</p> Aspect Description Implementation Explicit <code>cache_control</code> markers added to last message part Format Multipart messages with content parts array Cache lifecycle Managed by OpenRouter automatically Minimum tokens 1,024 tokens for caching to be applied Default state Disabled - must be explicitly enabled <p>Recommended Model: <code>google/gemini-2.0-flash-001</code> via OpenRouter</p> <p>This implementation uses the CachingLLMClient wrapper which: 1. Checks if caching is enabled (environment variable) 2. Verifies the model is Gemini via OpenRouter 3. Converts messages to multipart format 4. Adds <code>cache_control</code> marker to the last content part 5. Extracts cache metrics from responses (cache_read_tokens, cache_write_tokens)</p>"},{"location":"reference/observability.html#configuration_1","title":"Configuration","text":"<pre><code># Enable prompt caching (disabled by default)\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED=true\n\n# Enable metrics collection for cache statistics (recommended)\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_METRICS_ENABLED=true\n\n# Enable verbose caching logs for debugging (optional)\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_LOG_REQUESTS=true\n\n# Recommended model for caching\nMADEINOZ_KNOWLEDGE_MODEL_NAME=google/gemini-2.0-flash-001\n</code></pre>"},{"location":"reference/observability.html#cache-pricing","title":"Cache Pricing","text":"<p>Cached tokens are billed at 0.25x the normal input token price:</p> Model Input Price Cached Price Savings Gemini 2.5 Flash $0.15/1M $0.0375/1M 75% Gemini 2.5 Pro $1.25/1M $0.3125/1M 75% Gemini 2.0 Flash $0.10/1M $0.025/1M 75%"},{"location":"reference/observability.html#cache-metrics-to-monitor","title":"Cache Metrics to Monitor","text":"Metric Purpose <code>graphiti_cache_hit_rate</code> Current session hit rate (%) <code>graphiti_cache_tokens_saved_total</code> Cumulative tokens served from cache <code>graphiti_cache_cost_saved_total</code> Cumulative USD saved <code>graphiti_cache_hits_total</code> / <code>graphiti_cache_misses_total</code> Hit/miss ratio"},{"location":"reference/observability.html#example-promql-queries_2","title":"Example PromQL Queries","text":"<p>Cache hit rate over time:</p> <pre><code>graphiti_cache_hit_rate\n</code></pre> <p>Cost savings rate ($/hour):</p> <pre><code>rate(graphiti_cache_cost_saved_all_models_total[1h]) * 3600\n</code></pre> <p>Tokens saved in last hour:</p> <pre><code>increase(graphiti_cache_tokens_saved_all_models_total[1h])\n</code></pre> <p>Cache effectiveness by model:</p> <pre><code>sum by (model) (graphiti_cache_hits_total) / sum by (model) (graphiti_cache_requests_total) * 100\n</code></pre>"},{"location":"reference/observability.html#troubleshooting-caching","title":"Troubleshooting Caching","text":""},{"location":"reference/observability.html#cache-hits-are-zero","title":"Cache Hits Are Zero","text":"<p>Possible causes:</p> <ol> <li>Model doesn't support caching - Only Gemini models support caching</li> <li>Token count below threshold - Gemini 2.0 requires 4,096+ tokens (use Gemini 2.5 instead)</li> <li>Caching not enabled - Set <code>MADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED=true</code></li> <li>Different prompts - Cache keys are content-based; slight variations = cache miss</li> </ol> <p>Debug steps:</p> <pre><code># Check caching is enabled\ncurl -s http://localhost:9091/metrics | grep graphiti_cache_enabled\n\n# Check for any cache activity\ncurl -s http://localhost:9091/metrics | grep graphiti_cache\n\n# Enable verbose logging\nMADEINOZ_KNOWLEDGE_PROMPT_CACHE_LOG_REQUESTS=true\n</code></pre>"},{"location":"reference/observability.html#low-cache-hit-rate","title":"Low Cache Hit Rate","text":"<p>Expected behavior:</p> <ul> <li>First request for any unique prompt = cache miss</li> <li>Subsequent identical prompts = cache hit</li> <li>Entity extraction uses similar system prompts = good cache reuse</li> </ul> <p>Typical hit rates:</p> Scenario Expected Hit Rate Single <code>add_memory</code> call 0% (first request) Bulk import (10+ episodes) 30-50% Steady-state operation 40-60%"},{"location":"reference/observability.html#implementation-details","title":"Implementation Details","text":"<p>The caching system consists of three components:</p> <ol> <li><code>caching_wrapper.py</code> - Wraps OpenAI client methods</li> <li>Adds timing for duration metrics</li> <li>Catches errors for error metrics</li> <li> <p>Extracts cache statistics from responses</p> </li> <li> <p><code>message_formatter.py</code> - Formats messages for caching</p> </li> <li>Adds <code>cache_control</code> markers for explicit caching</li> <li> <p>Detects Gemini model families</p> </li> <li> <p><code>metrics_exporter.py</code> - Exports to Prometheus</p> </li> <li>Counters for totals</li> <li>Histograms for distributions</li> <li>Gauges for current state</li> </ol> <p>Files modified (in <code>docker/patches/</code>):</p> <pre><code>docker/patches/\n\u251c\u2500\u2500 caching_wrapper.py      # Client wrapper with timing/error tracking\n\u251c\u2500\u2500 caching_llm_client.py   # LLM client routing\n\u251c\u2500\u2500 message_formatter.py    # Cache marker formatting\n\u251c\u2500\u2500 cache_metrics.py        # Metrics calculation\n\u251c\u2500\u2500 session_metrics.py      # Session-level aggregation\n\u2514\u2500\u2500 metrics_exporter.py     # Prometheus export\n</code></pre>"},{"location":"reference/observability.html#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     OpenRouter API                               \u2502\n\u2502  (returns: usage, cost, cost_details, prompt_tokens_details)    \u2502\n\u2502  (Gemini: cached_tokens in prompt_tokens_details)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u25b2\n                              \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   caching_wrapper.py                             \u2502\n\u2502  - Wraps responses.parse() and chat.completions.create()        \u2502\n\u2502  - Adds timing (record_request_duration)                         \u2502\n\u2502  - Catches errors (record_error)                                 \u2502\n\u2502  - Extracts cache metrics from response                          \u2502\n\u2502  - Records cache hits/misses and savings                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   metrics_exporter.py                            \u2502\n\u2502  - OpenTelemetry MeterProvider with custom Views                \u2502\n\u2502  - Prometheus exporter on port 9090/9091                        \u2502\n\u2502  - Counters: tokens, cost, cache hits/misses, errors            \u2502\n\u2502  - Histograms: tokens/request, cost/request, duration           \u2502\n\u2502  - Gauges: cache_enabled, cache_hit_rate                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Prometheus / Grafana                                \u2502\n\u2502  - Scrape /metrics endpoint                                      \u2502\n\u2502  - Visualize with dashboards                                     \u2502\n\u2502  - Alert on thresholds (cost, errors, latency)                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reference/observability.html#queue-metrics-feature-017","title":"Queue Metrics (Feature 017)","text":"<p>The queue processing metrics provide observability for message queue operations, tracking throughput, latency, consumer health, and failure patterns. These metrics use the <code>messaging_</code> prefix.</p>"},{"location":"reference/observability.html#overview_1","title":"Overview","text":"<p>Queue metrics monitor the full lifecycle of message processing:</p> <ul> <li>Enqueue - Messages added to queue</li> <li>Wait - Time spent in queue before processing</li> <li>Processing - Time to process each message</li> <li>Completion - Success or failure with error categorization</li> <li>Consumer Health - Lag, saturation, active consumer count</li> </ul>"},{"location":"reference/observability.html#available-metrics_1","title":"Available Metrics","text":""},{"location":"reference/observability.html#throughput-counters","title":"Throughput Counters","text":"<p>Track cumulative message counts.</p> Metric Labels Description <code>messaging_messages_processed_total</code> <code>queue_name</code>, <code>status</code> Total messages processed (success/failure) <code>messaging_messages_failed_total</code> <code>queue_name</code>, <code>error_type</code> Total failures by error category <code>messaging_retries_total</code> <code>queue_name</code> Total retry attempts <p>Error categories (coarse-grained to prevent high cardinality):</p> Category Example Errors <code>ConnectionError</code> <code>ConnectionError</code>, <code>ConnectionRefusedError</code>, <code>OperationalError</code> <code>ValidationError</code> <code>ValidationError</code>, <code>ValueError</code>, <code>PydanticException</code> <code>TimeoutError</code> <code>TimeoutError</code>, <code>AsyncTimeoutError</code> <code>RateLimitError</code> <code>RateLimitError</code>, <code>RateLimitExceededError</code> <code>UnknownError</code> Any uncategorized error"},{"location":"reference/observability.html#queue-depth-gauge","title":"Queue Depth Gauge","text":"<p>Track current queue size (messages waiting).</p> Metric Labels Description <code>messaging_queue_depth</code> <code>queue_name</code>, <code>priority</code> Current number of messages waiting"},{"location":"reference/observability.html#consumer-health-gauges","title":"Consumer Health Gauges","text":"<p>Track consumer pool state and utilization.</p> Metric Labels Description <code>messaging_active_consumers</code> <code>queue_name</code> Number of active consumers <code>messaging_consumer_saturation</code> <code>queue_name</code> Consumer utilization (0-1, 1=fully saturated) <code>messaging_consumer_lag_seconds</code> <code>queue_name</code> Time to catch up (seconds)"},{"location":"reference/observability.html#latency-histograms","title":"Latency Histograms","text":"<p>Track processing time distributions for percentile analysis.</p> Metric Bucket Range Description <code>messaging_processing_duration_seconds</code> 5ms - 10s Time to process a message <code>messaging_wait_time_seconds</code> 5ms - 10s Time spent in queue before processing <code>messaging_end_to_end_latency_seconds</code> 5ms - 10s Total time from enqueue to completion <p>Duration bucket boundaries (seconds):</p> <pre><code>0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 7.5, 10\n</code></pre> Range Processing Type 5-50ms Fast processing (simple operations) 50-250ms Normal processing 250ms-1s Slow processing 1-10s Very slow processing (possible issues)"},{"location":"reference/observability.html#example-promql-queries_3","title":"Example PromQL Queries","text":"<p>Queue depth trend:</p> <pre><code>messaging_queue_depth\n</code></pre> <p>Processing throughput (messages/second):</p> <pre><code>sum(rate(messaging_messages_processed_total{status=\"success\"}[5m]))\n</code></pre> <p>Error rate (percentage):</p> <pre><code>sum(rate(messaging_messages_failed_total[5m]))\n/\nsum(rate(messaging_messages_processed_total[5m])) * 100\n</code></pre> <p>P95 processing latency:</p> <pre><code>histogram_quantile(0.95, sum(rate(messaging_processing_duration_seconds_bucket[5m])) by (le))\n</code></pre> <p>P95 wait time (queue delay):</p> <pre><code>histogram_quantile(0.95, sum(rate(messaging_wait_time_seconds_bucket[5m])) by (le))\n</code></pre> <p>P95 end-to-end latency:</p> <pre><code>histogram_quantile(0.95, sum(rate(messaging_end_to_end_latency_seconds_bucket[5m])) by (le))\n</code></pre> <p>Consumer saturation check:</p> <pre><code>messaging_consumer_saturation\n# Alert if &gt; 0.85 (85% utilization)\n</code></pre> <p>Time to drain queue (at current rate):</p> <pre><code>messaging_queue_depth / sum(rate(messaging_messages_processed_total{status=\"success\"}[5m]))\n</code></pre> <p>Retry rate (retries per message):</p> <pre><code>sum(rate(messaging_retries_total[5m])) / sum(rate(messaging_messages_processed_total[5m]))\n</code></pre>"},{"location":"reference/observability.html#queue-metrics-dashboard","title":"Queue Metrics Dashboard","text":"<p>Access: <code>http://localhost:3002/d/queue-metrics</code> (dev)</p> <p>A 12-panel Grafana dashboard provides comprehensive queue monitoring:</p> <p>Overview Row (4 panels):</p> Panel Metric Thresholds Queue Depth <code>messaging_queue_depth</code> green=0, yellow=10, red=50 Consumer Saturation <code>messaging_consumer_saturation</code> green=0, yellow=0.5, red=0.85 Consumer Lag <code>messaging_consumer_lag_seconds</code> green=0, yellow=30s, red=300s Active Consumers <code>messaging_active_consumers</code> green=1+, yellow=1, red=0 <p>Time Series Rows:</p> <ul> <li>Queue Depth Over Time - Trend analysis</li> <li>Processing Latency (P50/P95/P99) - Percentile analysis</li> <li>Wait Time (P50/P95) - Queue delay analysis</li> <li>End-to-End Latency (P50/P95) - Full journey latency</li> <li>Throughput (Success/Failure Rate) - Ops/second</li> <li>Error Rate (%) - Gauge panel</li> <li>Failures by Error Type - Pie chart</li> <li>Retry Rate - Retries/second trend</li> </ul>"},{"location":"reference/observability.html#troubleshooting-queue-issues","title":"Troubleshooting Queue Issues","text":""},{"location":"reference/observability.html#growing-queue-backlog","title":"Growing Queue Backlog","text":"<p>Symptoms: - <code>messaging_queue_depth</code> increasing over time - <code>messaging_consumer_lag_seconds</code> increasing - <code>messaging_consumer_saturation</code> near 1.0</p> <p>Diagnosis:</p> <pre><code># Check if production rate exceeds consumption rate\nsum(rate(messaging_messages_processed_total[5m])) &lt; sum(rate(messages_enqueued[5m]))\n\n# Check processing latency trend\nhistogram_quantile(0.95, sum(rate(messaging_processing_duration_seconds_bucket[5m])) by (le))\n</code></pre> <p>Solutions: 1. Scale consumers (increase <code>messaging_active_consumers</code>) 2. Optimize processing (reduce latency) 3. Implement priority queueing 4. Add rate limiting at enqueue</p>"},{"location":"reference/observability.html#high-consumer-lag","title":"High Consumer Lag","text":"<p>Symptoms: - <code>messaging_consumer_lag_seconds</code> &gt; 300 (5 minutes) - Queue depth stable but lag increasing</p> <p>Diagnosis:</p> <pre><code># Time to catch up at current rate\nmessaging_queue_depth / sum(rate(messaging_messages_processed_total{status=\"success\"}[5m]))\n</code></pre> <p>Solutions: 1. Increase consumer count 2. Reduce processing time per message 3. Implement batch processing 4. Scale horizontally (multiple queue instances)</p>"},{"location":"reference/observability.html#consumer-saturation","title":"Consumer Saturation","text":"<p>Symptoms: - <code>messaging_consumer_saturation</code> &gt; 0.85 - Wait times increasing</p> <p>Diagnosis:</p> <pre><code># Check wait time trend\nhistogram_quantile(0.95, sum(rate(messaging_wait_time_seconds_bucket[5m])) by (le))\n</code></pre> <p>Solutions: 1. Add more consumers 2. Increase consumer parallelism 3. Implement async processing</p>"},{"location":"reference/observability.html#high-error-rate","title":"High Error Rate","text":"<p>Symptoms: - <code>messaging_messages_failed_total</code> increasing - Error rate gauge &gt; 5%</p> <p>Diagnosis:</p> <pre><code># Error breakdown by type\nsum by (error_type) (messaging_messages_failed_total)\n</code></pre> <p>Solutions: 1. Check error types in failures panel 2. Fix common error patterns 3. Implement circuit breaker for failing services 4. Add retry with exponential backoff</p>"},{"location":"reference/observability.html#high-retry-rate","title":"High Retry Rate","text":"<p>Symptoms: - <code>messaging_retries_total</code> increasing rapidly - Retry rate &gt; 0.1 retries/message</p> <p>Diagnosis:</p> <pre><code># Retries per successful message\nsum(rate(messaging_retries_total[5m])) / sum(rate(messaging_messages_processed_total{status=\"success\"}[5m]))\n</code></pre> <p>Solutions: 1. Identify root cause of failures 2. Implement dead letter queue 3. Add backoff strategy 4. Limit max retry attempts</p>"},{"location":"reference/observability.html#implementation","title":"Implementation","text":"<p>The queue metrics are implemented in <code>docker/patches/metrics_exporter.py</code>:</p> <pre><code>class QueueMetricsExporter:\n    \"\"\"Manages queue processing metrics.\"\"\"\n\n    def record_enqueue(queue_name, priority)\n    def record_dequeue(queue_name)\n    def record_processing_complete(queue_name, duration, success, error_type)\n    def record_retry(queue_name)\n    def update_queue_depth(queue_name, depth, priority)\n    def update_consumer_metrics(queue_name, active, saturation, lag_seconds)\n</code></pre> <p>Thread safety: All state modifications use locks.</p> <p>Graceful degradation: Methods do nothing if metrics are disabled.</p>"},{"location":"reference/observability.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Reference - All environment variables</li> <li>Developer Notes - Internal architecture details</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"reference/releases.html","title":"Release Process","text":""},{"location":"reference/releases.html#release-process","title":"Release Process","text":""},{"location":"reference/releases.html#overview","title":"Overview","text":"<p>This project uses GitHub Actions to automate Docker image builds and releases. The custom <code>madeinoz-knowledge-system:fixed</code> image is published to GitHub Container Registry (GHCR) and optionally Docker Hub.</p>"},{"location":"reference/releases.html#release-workflows","title":"Release Workflows","text":""},{"location":"reference/releases.html#1-docker-build-workflow","title":"1. Docker Build Workflow","text":"<p>File: <code>.github/workflows/docker-build.yml</code></p> <p>Triggers: - Push to <code>main</code> branch (builds and pushes <code>latest</code> + <code>fixed</code> tags) - Push tags matching <code>v*.*.*</code> (builds and pushes version tags) - Pull requests (builds only, no push) - Manual dispatch via GitHub UI</p> <p>What it does: 1. Builds Docker image for <code>linux/amd64</code> and <code>linux/arm64</code> platforms 2. Tests the image (verifies entrypoint execution) 3. Pushes to GitHub Container Registry (<code>ghcr.io/madeinoz67/madeinoz-knowledge-system</code>) 4. Optionally pushes to Docker Hub (if <code>DOCKERHUB_USERNAME</code> and <code>DOCKERHUB_TOKEN</code> secrets are configured) 5. Tags images with multiple formats:    - <code>latest</code> - most recent main branch build    - <code>fixed</code> - alias for latest (used in code references)    - <code>v1.0.1</code> - semantic version from git tag    - <code>1.0</code> - major.minor from version    - <code>1</code> - major version only    - <code>sha-abc123</code> - git commit SHA</p> <p>Image locations: </p><pre><code># GitHub Container Registry (always available)\nghcr.io/madeinoz67/madeinoz-knowledge-system:latest\nghcr.io/madeinoz67/madeinoz-knowledge-system:fixed\nghcr.io/madeinoz67/madeinoz-knowledge-system:v1.0.1\n\n# Docker Hub (if configured)\nmadeinoz-knowledge-system:latest\nmadeinoz-knowledge-system:fixed\nmadeinoz-knowledge-system:v1.0.1\n</code></pre><p></p>"},{"location":"reference/releases.html#2-release-workflow","title":"2. Release Workflow","text":"<p>File: <code>.github/workflows/release.yml</code></p> <p>Trigger: Manual dispatch only (via GitHub UI)</p> <p>Required Input: - <code>version</code> - Semantic version (e.g., <code>1.0.2</code>) - <code>prerelease</code> - Mark as pre-release (optional, default: false)</p> <p>What it does: 1. Validates version format (<code>X.Y.Z</code>) 2. Checks if tag already exists 3. Updates <code>LABEL version</code> in Dockerfile 4. Generates changelog from git commits 5. Creates git tag (<code>v1.0.2</code>) 6. Pushes tag to GitHub 7. Creates GitHub Release with auto-generated notes 8. Triggers Docker build workflow (via tag push) 9. Updates documentation with release info</p>"},{"location":"reference/releases.html#how-to-create-a-release","title":"How to Create a Release","text":""},{"location":"reference/releases.html#prerequisites","title":"Prerequisites","text":"<ol> <li>Permissions: You need write access to the repository</li> <li>Clean state: Ensure all changes are committed and pushed to <code>main</code></li> <li>Changelog: Recent commits should have clear messages</li> </ol>"},{"location":"reference/releases.html#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"reference/releases.html#1-navigate-to-actions","title":"1. Navigate to Actions","text":"<p>Go to: <code>https://github.com/madeinoz67/madeinoz-knowledge-system/actions</code></p>"},{"location":"reference/releases.html#2-select-release-workflow","title":"2. Select Release Workflow","text":"<ul> <li>Click on \"Release\" in the left sidebar</li> <li>Click \"Run workflow\" button (top right)</li> </ul>"},{"location":"reference/releases.html#3-fill-in-release-details","title":"3. Fill in Release Details","text":"<ul> <li>Branch: Select <code>main</code></li> <li>Version: Enter semantic version (e.g., <code>1.0.2</code>)</li> <li>Must be in format <code>X.Y.Z</code></li> <li>Must not already exist as a tag</li> <li>Pre-release: Check if this is a pre-release (beta, rc, etc.)</li> </ul>"},{"location":"reference/releases.html#4-click-run-workflow","title":"4. Click \"Run workflow\"","text":"<p>The workflow will: - \u2705 Validate version format - \u2705 Update Dockerfile with new version - \u2705 Create git tag <code>v1.0.2</code> - \u2705 Generate release notes from commits - \u2705 Create GitHub Release - \u2705 Trigger Docker build (automatically via tag push) - \u2705 Push images to registries</p>"},{"location":"reference/releases.html#5-monitor-progress","title":"5. Monitor Progress","text":"<ul> <li>Release workflow: <code>https://github.com/madeinoz67/madeinoz-knowledge-system/actions/workflows/release.yml</code></li> <li>Docker build: <code>https://github.com/madeinoz67/madeinoz-knowledge-system/actions/workflows/docker-build.yml</code></li> </ul>"},{"location":"reference/releases.html#6-verify-release","title":"6. Verify Release","text":"<p>Check GitHub Release: </p><pre><code>https://github.com/madeinoz67/madeinoz-knowledge-system/releases/tag/v1.0.2\n</code></pre><p></p> <p>Pull the image: </p><pre><code># From GHCR\ndocker pull ghcr.io/madeinoz67/madeinoz-knowledge-system:1.0.2\ndocker pull ghcr.io/madeinoz67/madeinoz-knowledge-system:latest\n\n# Verify version\ndocker run --rm ghcr.io/madeinoz67/madeinoz-knowledge-system:1.0.2 \\\n  sh -c 'grep \"LABEL version\" /Dockerfile || echo \"Version: 1.0.2\"'\n</code></pre><p></p>"},{"location":"reference/releases.html#version-numbering","title":"Version Numbering","text":"<p>We follow Semantic Versioning (SemVer):</p> <pre><code>MAJOR.MINOR.PATCH\n1.0.2\n</code></pre> <ul> <li>MAJOR - Incompatible API changes or major features</li> <li>MINOR - Backward-compatible functionality additions</li> <li>PATCH - Backward-compatible bug fixes</li> </ul>"},{"location":"reference/releases.html#when-to-bump-each-number","title":"When to Bump Each Number","text":"<p>MAJOR (1.x.x \u2192 2.0.0): - Breaking changes to environment variables - Incompatible Docker compose file changes - Migration to official upstream images (when patches are no longer needed) - Major refactoring requiring user action</p> <p>MINOR (1.0.x \u2192 1.1.0): - New features (new patches, new backend support) - New MCP tools or capabilities - New configuration options (backward-compatible)</p> <p>PATCH (1.0.1 \u2192 1.0.2): - Bug fixes (password typo, network alias, volume mount issues) - Documentation improvements - Dependency updates - Performance improvements</p>"},{"location":"reference/releases.html#docker-hub-configuration-optional","title":"Docker Hub Configuration (Optional)","text":"<p>To enable Docker Hub publishing:</p>"},{"location":"reference/releases.html#1-create-docker-hub-account","title":"1. Create Docker Hub Account","text":"<ul> <li>Go to: https://hub.docker.com</li> <li>Create account or sign in</li> </ul>"},{"location":"reference/releases.html#2-create-access-token","title":"2. Create Access Token","text":"<ul> <li>Settings \u2192 Security \u2192 New Access Token</li> <li>Name: <code>madeinoz-knowledge-system-github</code></li> <li>Permissions: Read, Write, Delete</li> </ul>"},{"location":"reference/releases.html#3-add-github-secrets","title":"3. Add GitHub Secrets","text":"<ul> <li>Go to: <code>https://github.com/madeinoz67/madeinoz-knowledge-system/settings/secrets/actions</code></li> <li>Add two secrets:</li> <li><code>DOCKERHUB_USERNAME</code> - Your Docker Hub username</li> <li><code>DOCKERHUB_TOKEN</code> - The access token from step 2</li> </ul>"},{"location":"reference/releases.html#4-verify-in-next-build","title":"4. Verify in Next Build","text":"<p>The workflow will automatically detect the secrets and push to Docker Hub.</p>"},{"location":"reference/releases.html#rollback-a-release","title":"Rollback a Release","text":"<p>If a release has critical issues:</p>"},{"location":"reference/releases.html#1-mark-as-pre-release-quick-fix","title":"1. Mark as Pre-release (Quick Fix)","text":"<pre><code># Via GitHub UI\n1. Go to Releases\n2. Click \"Edit\" on problematic release\n3. Check \"This is a pre-release\"\n4. Save\n</code></pre>"},{"location":"reference/releases.html#2-create-hotfix-release","title":"2. Create Hotfix Release","text":"<pre><code># Via GitHub Actions\n1. Fix the issue in a branch\n2. Merge to main\n3. Run release workflow with new version (e.g., 1.0.3)\n4. New version becomes latest\n</code></pre>"},{"location":"reference/releases.html#3-delete-release-nuclear-option","title":"3. Delete Release (Nuclear Option)","text":"<pre><code># Delete GitHub Release (via UI)\n1. Go to Releases\n2. Click \"Delete\" on problematic release\n\n# Delete git tag\ngit push --delete origin v1.0.2\ngit tag -d v1.0.2\n\n# Note: Docker images cannot be deleted from GHCR/Docker Hub easily\n# Instead, create a new release that supersedes the bad one\n</code></pre>"},{"location":"reference/releases.html#changelog-generation","title":"Changelog Generation","text":"<p>The release workflow automatically generates changelogs from git commit messages.</p>"},{"location":"reference/releases.html#writing-good-commit-messages","title":"Writing Good Commit Messages","text":"<p>Use conventional commit format for better changelogs:</p> <pre><code># Features\ngit commit -m \"feat: add Neo4j health check retry logic\"\n\n# Bug fixes\ngit commit -m \"fix: correct password typo in docker-compose-neo4j.yml\"\n\n# Documentation\ngit commit -m \"docs: add developer notes for custom image\"\n\n# Chores\ngit commit -m \"chore: bump version to 1.0.2\"\n\n# Breaking changes\ngit commit -m \"feat!: migrate to official upstream image\n\nBREAKING CHANGE: Custom MADEINOZ_KNOWLEDGE_* prefixes no longer supported\"\n</code></pre> <p>Prefixes: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation - <code>chore:</code> - Maintenance - <code>refactor:</code> - Code refactoring - <code>test:</code> - Tests - <code>perf:</code> - Performance improvement - <code>ci:</code> - CI/CD changes</p>"},{"location":"reference/releases.html#monitoring-releases","title":"Monitoring Releases","text":""},{"location":"reference/releases.html#check-latest-version","title":"Check Latest Version","text":"<pre><code># Via GitHub API\ncurl -s https://api.github.com/repos/madeinoz67/madeinoz-knowledge-system/releases/latest | jq -r .tag_name\n\n# Via Docker\ndocker pull ghcr.io/madeinoz67/madeinoz-knowledge-system:latest\ndocker inspect ghcr.io/madeinoz67/madeinoz-knowledge-system:latest | jq -r '.[0].Config.Labels.version'\n</code></pre>"},{"location":"reference/releases.html#subscribe-to-releases","title":"Subscribe to Releases","text":"<ol> <li>Go to: <code>https://github.com/madeinoz67/madeinoz-knowledge-system</code></li> <li>Click \"Watch\" \u2192 \"Custom\" \u2192 \"Releases\"</li> <li>You'll receive notifications for new releases</li> </ol>"},{"location":"reference/releases.html#rss-feed","title":"RSS Feed","text":"<p>Subscribe to releases RSS feed: </p><pre><code>https://github.com/madeinoz67/madeinoz-knowledge-system/releases.atom\n</code></pre><p></p>"},{"location":"reference/releases.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/releases.html#build-fails-with-permission-denied","title":"Build Fails with \"Permission Denied\"","text":"<p>Problem: GitHub Actions can't push images or create releases</p> <p>Solution: Check repository settings: </p><pre><code>Settings \u2192 Actions \u2192 General \u2192 Workflow permissions\n\u2713 Read and write permissions\n</code></pre><p></p>"},{"location":"reference/releases.html#docker-hub-push-fails","title":"Docker Hub Push Fails","text":"<p>Problem: Missing or invalid Docker Hub credentials</p> <p>Solution: 1. Verify secrets exist: <code>Settings \u2192 Secrets \u2192 Actions</code> 2. Ensure <code>DOCKERHUB_TOKEN</code> is an access token (not password) 3. Regenerate token if needed</p>"},{"location":"reference/releases.html#tag-already-exists","title":"Tag Already Exists","text":"<p>Problem: Version tag already exists in git</p> <p>Solution: </p><pre><code># Delete remote tag\ngit push --delete origin v1.0.2\n\n# Delete local tag\ngit tag -d v1.0.2\n\n# Try release again\n</code></pre><p></p>"},{"location":"reference/releases.html#image-not-found-after-release","title":"Image Not Found After Release","text":"<p>Problem: Docker pull fails after successful release</p> <p>Solution: </p><pre><code># Check if tag exists on GHCR\ncurl -s https://ghcr.io/v2/madeinoz67/madeinoz-knowledge-system/tags/list | jq .\n\n# Verify workflow completed\n# Go to: Actions \u2192 Docker Build and Publish\n# Check for green checkmark\n\n# Try full image path\ndocker pull ghcr.io/madeinoz67/madeinoz-knowledge-system:1.0.2\n</code></pre><p></p> <p>See Also: - Developer Notes - Custom image rationale - Configuration Reference - Environment variables - Installation Guide - Setup instructions</p>"},{"location":"reference/releases.html#release-notes","title":"Release Notes","text":""},{"location":"reference/releases.html#feature-017-queue-processing-metrics","title":"Feature 017: Queue Processing Metrics","text":"<p>Summary: Queue observability for monitoring depth, latency, consumer health, and failure patterns.</p> <p>Description: Adds comprehensive metrics for message queue processing operations, enabling monitoring of queue depth, processing latency (wait time, processing duration, end-to-end latency), consumer health (active count, saturation, lag), throughput, and error categorization.</p> <p>Key Metrics (10 total):</p> Metric Type Description <code>messaging_queue_depth</code> Gauge Current number of messages waiting <code>messaging_active_consumers</code> Gauge Number of active consumers <code>messaging_consumer_saturation</code> Gauge Consumer utilization (0-1) <code>messaging_consumer_lag_seconds</code> Gauge Time to catch up (seconds) <code>messaging_messages_processed_total</code> Counter Messages processed by status <code>messaging_messages_failed_total</code> Counter Failures by error category <code>messaging_retries_total</code> Counter Retry attempts <code>messaging_processing_duration_seconds</code> Histogram Processing time distribution <code>messaging_wait_time_seconds</code> Histogram Queue wait time distribution <code>messaging_end_to_end_latency_seconds</code> Histogram Total latency from enqueue to complete <p>Dashboard: 12-panel Grafana dashboard (<code>queue-metrics</code>) providing: - Overview row: Queue depth, consumer saturation, consumer lag, active consumers - Time series: Queue depth trend, processing latency (P50/P95/P99), wait time, end-to-end latency - Throughput and errors: Success/failure rate, error percentage, failures by type, retry rate</p> <p>Error Categorization: Coarse-grained error categories prevent high cardinality: - <code>ConnectionError</code> - Database/network connection issues - <code>ValidationError</code> - Data validation failures - <code>TimeoutError</code> - Request timeouts - <code>RateLimitError</code> - API rate limit exceeded - <code>UnknownError</code> - Uncategorized errors</p> <p>Implementation: - File: <code>docker/patches/metrics_exporter.py</code> - Class: <code>QueueMetricsExporter</code> - Thread-safe: All state modifications use locks - Graceful degradation: No-ops when metrics disabled</p> <p>Testing: Comprehensive unit tests covering: - Metric initialization and recording - Error categorization - Thread safety (concurrent operations) - Performance (&lt; 1ms overhead per recording) - Graceful degradation without OpenTelemetry</p> <p>Documentation: - Updated: <code>docs/reference/observability.md</code> with Queue Metrics section - Includes: PromQL query examples, troubleshooting guide</p> <p>GitHub Issues: - Issue: #61</p> <p>Pull Requests: - PR: #62</p> <p>Feature Flag: None - always available when metrics are enabled</p> <p>Dependencies: - Requires: OpenTelemetry SDK (same as existing metrics) - Compatible: Feature 009 (memory decay metrics), Feature 006 (cache metrics)</p> <p>Migration: No migration required - metrics are additive only</p> <p>Performance Impact: Negligible (&lt; 1ms per metric recording operation)</p>"},{"location":"troubleshooting/common-issues.html","title":"Troubleshooting Guide","text":""},{"location":"troubleshooting/common-issues.html#troubleshooting-guide","title":"Troubleshooting Guide","text":"<p>This guide helps you fix common issues with the Madeinoz Knowledge System. Problems are organized by symptom with step-by-step solutions.</p>"},{"location":"troubleshooting/common-issues.html#quick-diagnostics","title":"Quick Diagnostics","text":"<p>Before diving into specific problems, run these checks:</p>"},{"location":"troubleshooting/common-issues.html#1-check-if-services-are-running","title":"1. Check if Services are Running","text":"<pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nbun run server-cli status\n</code></pre> <p>Expected output:</p> <pre><code>Madeinoz Knowledge System Status:\n\nContainers:\n  madeinoz-knowledge-graph-mcp: running\n  madeinoz-knowledge-neo4j: running\n\nMCP Server: http://localhost:8000/sse\n  Status: healthy\n</code></pre>"},{"location":"troubleshooting/common-issues.html#2-check-logs","title":"2. Check Logs","text":"<pre><code>bun run server-cli logs\n</code></pre> <p>Look for errors (lines with ERROR or WARN).</p>"},{"location":"troubleshooting/common-issues.html#3-test-connectivity","title":"3. Test Connectivity","text":"<pre><code>curl http://localhost:8000/sse -H \"Accept: text/event-stream\"\n</code></pre> <p>Should see some response about the endpoint.</p>"},{"location":"troubleshooting/common-issues.html#common-problems","title":"Common Problems","text":""},{"location":"troubleshooting/common-issues.html#cannot-connect-to-server-or-connection-refused","title":"\"Cannot connect to server\" or \"Connection refused\"","text":"<p>Symptom: Commands fail with connection errors</p> <p>Possible Causes:</p> <ol> <li>Server not running</li> <li>Wrong port</li> <li>Firewall blocking connection</li> </ol> <p>Solutions:</p> <p>Check if server is running:</p> <pre><code>podman ps | grep madeinoz-knowledge\n</code></pre> <p>If nothing shows up, the server isn't running.</p> <p>Start the server:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nbun run server-cli start\n</code></pre> <p>Check if port 8000 is in use:</p> <pre><code>lsof -i :8000\n</code></pre> <p>If another service is using port 8000, you need to either stop that service or change the knowledge system port.</p> <p>To change the port: Edit the Docker Compose files (<code>docker-compose-neo4j.yml</code> or <code>docker-compose-falkordb.yml</code>) and change the port number, then restart.</p>"},{"location":"troubleshooting/common-issues.html#api-key-not-configured-or-invalid-api-key","title":"\"API key not configured\" or \"Invalid API key\"","text":"<p>Symptom: Error messages about API keys</p> <p>Check your configuration:</p> <pre><code>cat \"${PAI_DIR:-$HOME/.claude}/.env\" | grep MADEINOZ_KNOWLEDGE_OPENAI_API_KEY\n</code></pre> <p>If the key is missing or wrong:</p> <ol> <li>Edit the config file:</li> </ol> <pre><code>nano \"${PAI_DIR:-$HOME/.claude}/.env\"\n</code></pre> <ol> <li>Add or fix your API key:</li> </ol> <pre><code>MADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-your-actual-key-here\n</code></pre> <ol> <li> <p>Save (Ctrl+O, Enter, Ctrl+X)</p> </li> <li> <p>Restart the server:</p> </li> </ol> <pre><code>bun run server-cli restart\n</code></pre> <p>Verify your key has credits: Visit https://platform.openai.com/usage to check your API usage and credits.</p>"},{"location":"troubleshooting/common-issues.html#no-entities-extracted-or-poor-extraction-quality","title":"\"No entities extracted\" or Poor Extraction Quality","text":"<p>Symptom: System captures knowledge but extracts no or few entities</p> <p>Causes:</p> <ol> <li>Content too short or vague</li> <li>Model not powerful enough</li> <li>Content lacks clear concepts</li> </ol> <p>Solutions:</p> <p>Add more detail:</p> <p>Instead of:</p> <pre><code>Remember Docker\n</code></pre> <p>Try:</p> <pre><code>Remember that Docker is a container runtime that requires a daemon\nprocess running as root, which manages container lifecycles and images.\n</code></pre> <p>Use a better model:</p> <p>Edit your PAI config (<code>$PAI_DIR/.env</code> or <code>~/.claude/.env</code>):</p> <pre><code>MADEINOZ_KNOWLEDGE_MODEL_NAME=gpt-4o\n</code></pre> <p>Note: gpt-4o costs more but extracts entities better than gpt-4o-mini.</p> <p>Restart the server after changing.</p> <p>Be explicit about relationships:</p> <p>Instead of:</p> <pre><code>Remember Podman and Docker\n</code></pre> <p>Try:</p> <pre><code>Remember that Podman is an alternative to Docker, designed to be\ndaemonless and rootless for better security.\n</code></pre>"},{"location":"troubleshooting/common-issues.html#container-wont-start","title":"Container Won't Start","text":"<p>Symptom: Server fails to start, containers exit immediately</p> <p>Check Docker/Podman is running:</p> <pre><code>podman ps\n# or\ndocker ps\n</code></pre> <p>If \"Cannot connect to Podman socket\":</p> <p>On macOS:</p> <pre><code>podman machine start\n</code></pre> <p>Wait 30 seconds, then try starting the server again.</p> <p>Check logs for specific errors:</p> <pre><code>bun run server-cli logs\n</code></pre> <p>Common specific issues:</p> <p>Error: \"port already in use\" Another service is using port 8000 or 7687 (Neo4j) or 6379 (FalkorDB).</p> <p>Find what's using the port:</p> <pre><code>lsof -i :8000\nlsof -i :7687  # Neo4j Bolt\nlsof -i :6379  # FalkorDB/Redis\n</code></pre> <p>Kill the process or change the knowledge system ports.</p> <p>Error: \"image not found\" The container image needs to be pulled:</p> <pre><code>podman pull falkordb/graphiti-knowledge-graph-mcp:latest\n</code></pre> <p>Error: \"network not found\" Recreate the network:</p> <pre><code>podman network rm madeinoz-knowledge-net\n</code></pre> <p>Then start the server again (it will recreate the network).</p>"},{"location":"troubleshooting/common-issues.html#search-returns-no-results","title":"Search Returns No Results","text":"<p>Symptom: Searches return empty or \"No knowledge found\"</p> <p>Check if knowledge has been captured:</p> <pre><code># In your AI assistant\nShow me recent knowledge additions\n</code></pre> <p>If nothing recent, you need to capture knowledge first.</p> <p>Try a broader search:</p> <p>Instead of:</p> <pre><code>What do I know about Podman volume mounting syntax?\n</code></pre> <p>Try:</p> <pre><code>What do I know about Podman?\n</code></pre> <p>Check you're searching the right group:</p> <p>If you've set a custom group ID, make sure searches use the same group.</p> <p>Verify your group setting:</p> <pre><code>grep MADEINOZ_KNOWLEDGE_GROUP_ID \"${PAI_DIR:-$HOME/.claude}/.env\"\n</code></pre> <p>Verify entities were extracted:</p> <p>Look at a recent capture - did it show \"Entities extracted: 0\"? If so, see the \"No entities extracted\" section above.</p>"},{"location":"troubleshooting/common-issues.html#vector-dimension-mismatch-error","title":"Vector Dimension Mismatch Error","text":"<p>Symptom: Search queries fail with error: <code>Invalid input for 'vector.similarity.cosine()': The supplied vectors do not have the same number of dimensions</code></p> <p>Cause: Data was indexed with one embedding model, but searches use a different model with incompatible vector dimensions.</p> <p>Common scenarios that cause this:</p> <ol> <li>Changed <code>EMBEDDER_MODEL</code> in config after data was already indexed</li> <li>Tested multiple embedding models without clearing data between tests</li> <li>Migrated from one embedding provider to another</li> </ol> <p>Embedding model dimensions:</p> Model Provider Dimensions mxbai-embed-large Ollama 1024 nomic-embed-text Ollama 768 text-embedding-3-small OpenAI 1536 text-embedding-3-large OpenAI 3072 text-embedding-ada-002 OpenAI 1536 <p>The fix: Clear mismatched data</p> <p>Neo4j requires all vectors in an index to have the same dimensions. You must clear data indexed with the old model.</p> <p>Option 1: Clear specific groups (preserves other data)</p> <p>If you know which groups have mismatched embeddings:</p> <pre><code># Via Claude Code / MCP\nclear_graph with group_ids: [\"group1\", \"group2\"]\n</code></pre> <p>Or identify test groups by checking episodes:</p> <pre><code># Via Claude Code / MCP\nget_episodes with max_episodes: 50\n</code></pre> <p>Look for groups with different <code>group_id</code> values, then clear those specific groups.</p> <p>Option 2: Clear entire graph (nuclear option)</p> <p>If unsure which data is affected:</p> <pre><code># Via Claude Code / MCP\nclear_graph\n</code></pre> <p>This deletes ALL data. You'll need to re-add any knowledge you want to keep.</p> <p>Verify the fix:</p> <p>After clearing, test that searches work:</p> <pre><code># Via Claude Code / MCP\nsearch_nodes with query: \"test\"\n</code></pre> <p>Should return <code>\"No relevant nodes found\"</code> (empty but no error), not a dimension mismatch error.</p> <p>Prevention:</p> <ol> <li>Choose an embedding model and stick with it - Changing models requires re-indexing all data</li> <li>Use separate group_ids for testing - e.g., <code>test-llama</code>, <code>test-openai</code>, then clear test groups after</li> <li>Document your embedding config - Note which model was used to index production data</li> <li>Keep <code>EMBEDDER_DIMENSIONS</code> in sync - Must match your model:</li> </ol> <pre><code># Example for mxbai-embed-large\nEMBEDDER_MODEL=mxbai-embed-large\nEMBEDDER_DIMENSIONS=1024\n</code></pre> <p>If you MUST change embedding models:</p> <ol> <li>Export important knowledge (manually note key facts)</li> <li>Clear the entire graph</li> <li>Update <code>EMBEDDER_MODEL</code> and <code>EMBEDDER_DIMENSIONS</code> in config</li> <li>Restart the server</li> <li>Re-add your knowledge</li> </ol> <p>There's no way to \"migrate\" vectors - the embeddings are fundamentally different representations.</p>"},{"location":"troubleshooting/common-issues.html#rate-limit-exceeded-or-api-errors","title":"\"Rate limit exceeded\" or API Errors","text":"<p>Symptom: Errors about too many requests or rate limits</p> <p>Immediate fix:</p> <p>Reduce concurrent requests in your PAI config (<code>$PAI_DIR/.env</code> or <code>~/.claude/.env</code>):</p> <pre><code>MADEINOZ_KNOWLEDGE_SEMAPHORE_LIMIT=3\n</code></pre> <p>Lower number = fewer parallel requests.</p> <p>Restart the server after changing.</p> <p>Permanent solution:</p> <p>Check your OpenAI tier at https://platform.openai.com/account/rate-limits</p> <p>Adjust SEMAPHORE_LIMIT based on your tier:</p> <ul> <li>Free tier: 1-2</li> <li>Tier 1: 3-5</li> <li>Tier 2: 8</li> <li>Tier 3+: 10-15</li> </ul> <p>If you're hitting rate limits constantly: Consider upgrading your OpenAI tier or capturing knowledge less frequently.</p>"},{"location":"troubleshooting/common-issues.html#sse-endpoint-not-responding","title":"\"SSE endpoint not responding\"","text":"<p>Symptom: MCP connection fails, mentions SSE</p> <p>This is the MCP transport layer having issues.</p> <p>Quick fix:</p> <ol> <li>Stop the server: <code>bun run server-cli stop</code></li> <li>Wait 10 seconds</li> <li>Start again: <code>bun run server-cli start</code></li> <li>Restart your AI assistant (Claude Code, etc.)</li> </ol> <p>If that doesn't work:</p> <p>Check if the SSE endpoint responds at all:</p> <pre><code>curl -N -H \"Accept: text/event-stream\" http://localhost:8000/sse\n</code></pre> <p>Should see event-stream data.</p> <p>If curl fails: The MCP server isn't running properly. Check logs:</p> <pre><code>bun run server-cli logs\n</code></pre> <p>Look for startup errors.</p>"},{"location":"troubleshooting/common-issues.html#knowledge-not-syncing-from-memory","title":"Knowledge Not Syncing from Memory","text":"<p>Symptom: Memory captures aren't appearing in knowledge graph</p> <p>Check if the hook is installed:</p> <pre><code>cat ~/.claude/settings.json | grep sync-memory-to-knowledge\n</code></pre> <p>Should see a hook definition.</p> <p>If nothing shows: The hook isn't installed. Install it:</p> <pre><code>cd ~/.claude/skills/Knowledge\nbun run tools/install.ts\n</code></pre> <p>Manually trigger sync:</p> <pre><code>bun run ~/.claude/hooks/sync-memory-to-knowledge.ts --verbose\n</code></pre> <p>This shows what's being synced (or why not).</p> <p>Check sync state:</p> <pre><code>cat ~/.claude/MEMORY/STATE/knowledge-sync/sync-state.json\n</code></pre> <p>Shows what's already been synced.</p> <p>Force re-sync everything:</p> <pre><code>rm ~/.claude/MEMORY/STATE/knowledge-sync/sync-state.json\nbun run ~/.claude/hooks/sync-memory-to-knowledge.ts --all --verbose\n</code></pre>"},{"location":"troubleshooting/common-issues.html#high-api-costs","title":"High API Costs","text":"<p>Symptom: Your OpenAI bill is higher than expected</p> <p>Check usage: https://platform.openai.com/usage</p> <p>Reduce costs:</p> <p>1. Use cheaper model:</p> <p>In your PAI config (<code>$PAI_DIR/.env</code> or <code>~/.claude/.env</code>):</p> <pre><code>MADEINOZ_KNOWLEDGE_MODEL_NAME=gpt-4o-mini\n</code></pre> <p>(gpt-4o-mini is 10x cheaper than gpt-4o)</p> <p>2. Capture less: Only capture truly valuable knowledge, not every conversation.</p> <p>3. Reduce concurrency:</p> <pre><code>MADEINOZ_KNOWLEDGE_SEMAPHORE_LIMIT=3\n</code></pre> <p>4. Monitor usage: Check your API usage weekly to catch cost spikes early.</p> <p>Typical costs:</p> <ul> <li>Light use: $0.50-1.00/month</li> <li>Moderate use: $1.00-3.00/month</li> <li>Heavy use: $3.00-10.00/month</li> </ul> <p>Using gpt-4o-mini, not gpt-4o.</p>"},{"location":"troubleshooting/common-issues.html#database-web-ui-not-accessible","title":"Database Web UI Not Accessible","text":"<p>Symptom: Can't access database UI (Neo4j: http://localhost:7474, FalkorDB: http://localhost:3000)</p> <p>Check if database container is running:</p> <pre><code># For Neo4j (default)\npodman ps | grep neo4j\n\n# For FalkorDB\npodman ps | grep falkordb\n</code></pre> <p>If not running:</p> <pre><code>bun run server-cli start\n</code></pre> <p>Check ports aren't blocked:</p> <pre><code>lsof -i :7474  # Neo4j Browser\nlsof -i :3000  # FalkorDB UI\n</code></pre> <p>Try accessing the graph directly:</p> <pre><code># For Neo4j (default)\npodman exec madeinoz-knowledge-neo4j cypher-shell -u neo4j -p password \"RETURN 1\"\n\n# For FalkorDB\npodman exec madeinoz-knowledge-falkordb redis-cli PING\n</code></pre> <p>Should respond with <code>1</code> (Neo4j) or \"PONG\" (FalkorDB).</p>"},{"location":"troubleshooting/common-issues.html#memory-or-performance-issues","title":"Memory or Performance Issues","text":"<p>Symptom: System is slow or running out of memory</p> <p>Check system resources:</p> <pre><code>podman stats\n</code></pre> <p>Shows CPU and memory usage of containers.</p> <p>If memory usage is high:</p> <p>Option 1: Restart containers</p> <pre><code>bun run server-cli restart\n</code></pre> <p>Option 2: Clear old data If you have a lot of episodes you no longer need:</p> <pre><code># In your AI assistant\nClear my knowledge graph\n</code></pre> <p>(Warning: This deletes everything!)</p> <p>Option 3: Add memory limits Edit container configuration to limit memory usage.</p>"},{"location":"troubleshooting/common-issues.html#edgeduplicate-or-extractedentities-pydantic-validation-errors","title":"EdgeDuplicate or ExtractedEntities Pydantic Validation Errors","text":"<p>Symptom: Container logs show errors like:</p> <pre><code>3 validation errors for EdgeDuplicate\nduplicate_facts: Field required [type=missing, input_value={'properties': {'duplicat...}]\ncontradicted_facts: Field required [type=missing]\nfact_type: Field required [type=missing]\n</code></pre> <p>Or similar errors for <code>ExtractedEntities</code>, <code>EntitySummary</code>, or other Pydantic models.</p> <p>Root Cause:</p> <p>The LLM returns a JSON schema definition instead of actual field values. This happens because:</p> <ol> <li>Some LLM providers don't fully support structured output/parse API</li> <li>The <code>OpenAIGenericClient</code> uses basic <code>json_object</code> mode which is less strict</li> <li>Complex multi-entity content triggers edge deduplication which is more prone to this error</li> </ol> <p>Which models are affected:</p> Model Status Notes gpt-4o-mini \u2705 Works Occasional errors, retry recovers gpt-4o \u2705 Works Occasional errors, retry recovers gemini-2.0-flash \u2705 Works Occasional errors, retry recovers claude-3.5-haiku \u26a0\ufe0f Auth issues Requires different API routing llama/mistral variants \u274c Fails Consistent Pydantic errors <p>Solution: Use OpenAIClient for Cloud Providers</p> <p>The Madeinoz Knowledge System v1.2.4+ includes a patch that uses <code>OpenAIClient</code> (with parse API) for cloud providers instead of <code>OpenAIGenericClient</code> (basic json_object mode).</p> <p>Verify the patch is active:</p> <p>Check container logs after startup:</p> <pre><code>podman logs madeinoz-knowledge-graph-mcp 2&gt;&amp;1 | grep \"Patch v3\"\n</code></pre> <p>Should show:</p> <pre><code>Madeinoz Patch v3: Using OpenAIClient for cloud openrouter\n</code></pre> <p>If errors persist:</p> <ol> <li>Check your model is supported - Llama and Mistral models consistently fail</li> <li>Errors are expected occasionally - The built-in retry logic (2 attempts) usually recovers</li> <li>Simpler content works better - Break very complex episodes into smaller chunks</li> </ol> <p>Tracking:</p> <p>This issue is tracked at: https://github.com/getzep/graphiti/issues/912</p> <p>Technical Details:</p> <p>The fix differentiates between:</p> <ul> <li>Cloud providers (OpenRouter, Together, etc.) \u2192 Use <code>OpenAIClient</code> with parse API</li> <li>Local endpoints (Ollama) \u2192 Use <code>OpenAIGenericClient</code> with json_object mode</li> </ul> <p>The parse API provides stricter schema enforcement, which prevents the LLM from returning JSON schema definitions instead of actual values.</p> <p>Note: This fix is applied at Docker image build time as part of the container configuration.</p>"},{"location":"troubleshooting/common-issues.html#initialization-not-complete-warning","title":"\"Initialization not complete\" Warning","text":"<p>Symptom: MCP logs show \"Received request before initialization was complete\"</p> <p>Cause: This is a known issue with the Graphiti MCP server - Claude Code sometimes sends requests before the SSE session fully initializes.</p> <p>Workaround: Restart your AI assistant (Claude Code). This resets the MCP connection.</p> <p>Tracking: This issue is tracked at: https://github.com/getzep/graphiti/issues/840</p> <p>Not a critical issue: This warning usually doesn't break functionality, but restarting helps if you see repeated failures.</p>"},{"location":"troubleshooting/common-issues.html#query-syntax-errors-with-special-characters-falkordb-backend","title":"Query Syntax Errors with Special Characters (FalkorDB Backend)","text":"<p>Note: This issue is specific to the FalkorDB backend. Neo4j (the default) handles special characters more gracefully.</p> <p>Symptom: Search queries fail with syntax errors, especially when searching for content containing hyphens, at-signs, or other special characters. Error messages may include \"QuerySyntaxError\" or mention unexpected tokens.</p> <p>Root Cause:</p> <p>FalkorDB uses RediSearch for fulltext indexing, which interprets certain characters as Lucene query operators:</p> Character Lucene Interpretation <code>-</code> Negation (NOT operator) <code>+</code> Required term (AND) <code>@</code> Field prefix <code>#</code> Tag field <code>*</code> <code>?</code> Wildcards <code>\"</code> Phrase query <code>( )</code> Grouping <code>{ }</code> <code>[ ]</code> Range queries <code>~</code> Fuzzy/proximity <code>:</code> Field specifier <code>\\|</code> OR operator <code>&amp;</code> AND operator <code>!</code> NOT operator <code>%</code> Fuzzy threshold <code>&lt; &gt; =</code> Comparison operators <code>$</code> Variable reference <code>/</code> Regex delimiter <p>Example of the bug:</p> <p>When you search for <code>madeinoz-threat-intel</code>:</p> <ul> <li>RediSearch interprets this as: <code>pai AND NOT threat AND NOT intel</code></li> <li>This returns wrong results or a syntax error</li> </ul> <p>The Graphiti Bug:</p> <p>Graphiti's FalkorDB driver has a <code>sanitize()</code> method that replaces special characters with whitespace. However, this sanitization is not applied to group_ids in search queries. When you use a group_id like <code>my-knowledge-base</code>, the hyphen is passed directly to RediSearch and interpreted as negation.</p> <p>Related Issues:</p> <ul> <li>RediSearch #2628 - Can't search text with hyphens</li> <li>RediSearch #4092 - Escaping filter values</li> <li>Graphiti #815 - FalkorDB query syntax errors</li> <li>Graphiti #1118 - Fix forward slash handling</li> </ul> <p>Our Solution:</p> <p>The Madeinoz Knowledge System Docker container handles sanitization automatically at runtime:</p> <ol> <li>For group_ids: Special characters are properly escaped in queries</li> <li><code>madeinoz-threat-intel</code> \u2192 <code>pai_threat_intel</code></li> <li> <p>This avoids the Graphiti bug where group_ids aren't escaped</p> </li> <li> <p>For search queries: Special characters are handled by the container's query processor</p> </li> </ol> <p>Note: The sanitization is built into the Docker container and applied automatically.</p> <p>If you encounter syntax errors:</p> <ol> <li>Check if your group_id contains special characters:</li> </ol> <pre><code>grep MADEINOZ_KNOWLEDGE_GROUP_ID \"${PAI_DIR:-$HOME/.claude}/.env\"\n</code></pre> <ol> <li>Use underscores instead of hyphens in group_ids:</li> <li>Bad: <code>my-knowledge-base</code></li> <li> <p>Good: <code>my_knowledge_base</code></p> </li> <li> <p>The sanitization is automatic for MCP tool calls, but if you're calling Graphiti directly, ensure you sanitize inputs.</p> </li> </ol> <p>Recommendation: For the best experience with special characters, consider using the Neo4j backend instead of FalkorDB, as Neo4j handles special characters natively.</p>"},{"location":"troubleshooting/common-issues.html#diagnostic-commands-summary","title":"Diagnostic Commands Summary","text":"<p>Quick reference for troubleshooting:</p> <pre><code># Check status\nbun run server-cli status\n\n# View logs\nbun run server-cli logs\n\n# Restart everything\nbun run server-cli restart\n\n# Check configuration\ncat \"${PAI_DIR:-$HOME/.claude}/.env\" | grep MADEINOZ_KNOWLEDGE\n\n# Test MCP endpoint\ncurl http://localhost:8000/sse -H \"Accept: text/event-stream\"\n\n# Check containers\npodman ps | grep madeinoz-knowledge\n\n# Check ports\nlsof -i :8000    # MCP Server\nlsof -i :7687    # Neo4j Bolt (default)\nlsof -i :7474    # Neo4j Browser (default)\nlsof -i :6379    # FalkorDB/Redis\nlsof -i :3000    # FalkorDB UI\n\n# Manual sync test\nbun run ~/.claude/hooks/sync-memory-to-knowledge.ts --dry-run --verbose\n\n# View container logs directly\npodman logs madeinoz-knowledge-graph-mcp\npodman logs madeinoz-knowledge-neo4j       # Neo4j (default)\npodman logs madeinoz-knowledge-falkordb    # FalkorDB backend\n</code></pre>"},{"location":"troubleshooting/common-issues.html#getting-more-help","title":"Getting More Help","text":"<p>If these solutions don't work:</p> <ol> <li> <p>Check the main README: <code>/Users/seaton/.config/pai/Packs/madeinoz-knowledge-system/README.md</code></p> </li> <li> <p>Check installation guide: <code>docs/installation.md</code></p> </li> <li> <p>Review verification: <code>/Users/seaton/.config/pai/Packs/madeinoz-knowledge-system/VERIFY.md</code></p> </li> <li> <p>Check Graphiti documentation: https://help.getzep.com/graphiti</p> </li> <li> <p>Check FalkorDB documentation: https://docs.falkordb.com/</p> </li> </ol>"},{"location":"troubleshooting/common-issues.html#still-stuck","title":"Still Stuck?","text":"<p>Create a diagnostic report:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\necho \"=== System Status ===\" &gt; diagnostic.txt\nbun run server-cli status &gt;&gt; diagnostic.txt\n\necho \"\\n=== Configuration ===\" &gt;&gt; diagnostic.txt\ncat \"${PAI_DIR:-$HOME/.claude}/.env\" | grep MADEINOZ_KNOWLEDGE | grep -v API_KEY &gt;&gt; diagnostic.txt\n\necho \"\\n=== Recent Logs ===\" &gt;&gt; diagnostic.txt\nbun run server-cli logs | tail -100 &gt;&gt; diagnostic.txt\n\necho \"\\n=== Container Info ===\" &gt;&gt; diagnostic.txt\npodman ps --all | grep madeinoz-knowledge &gt;&gt; diagnostic.txt\n\necho \"\\n=== Port Status ===\" &gt;&gt; diagnostic.txt\nlsof -i :8000 &gt;&gt; diagnostic.txt\nlsof -i :6379 &gt;&gt; diagnostic.txt\n\necho \"Diagnostic report saved to diagnostic.txt\"\n</code></pre> <p>Share <code>diagnostic.txt</code> when asking for help (remove any sensitive info first!).</p>"},{"location":"troubleshooting/known-issues.html","title":"Known Issues","text":""},{"location":"troubleshooting/known-issues.html#known-issues","title":"Known Issues","text":"<p>This page documents known issues with the Madeinoz Knowledge System, including their root causes and implemented workarounds. These are issues we're aware of that have solutions in place but may affect your usage or require understanding.</p>"},{"location":"troubleshooting/known-issues.html#pydantic-validation-errors-with-openai-compatible-apis","title":"Pydantic Validation Errors with OpenAI-Compatible APIs","text":"<p>When using OpenAI-compatible API providers (OpenRouter, Together AI, Fireworks, etc.), users may encounter intermittent Pydantic validation errors during entity extraction and relationship mapping.</p>"},{"location":"troubleshooting/known-issues.html#the-problem","title":"The Problem","text":"<p>Graphiti uses structured output (JSON schema validation) to ensure LLM responses conform to expected data types. Different LLM clients have different levels of structured output support:</p> Client Type Structured Output Method Reliability <code>OpenAIClient</code> Parse API with strict schema enforcement High <code>OpenAIGenericClient</code> Basic <code>json_object</code> mode Variable <p>EdgeDuplicate Validation Errors</p> <p>When using the wrong client type for cloud providers, you may see errors like:</p> <pre><code>pydantic.ValidationError: EdgeDuplicate validation failed\n</code></pre> <p>These occur because the basic <code>json_object</code> mode doesn't enforce the strict schema that Graphiti expects, leading to malformed responses that fail Pydantic validation.</p>"},{"location":"troubleshooting/known-issues.html#the-solution","title":"The Solution","text":"<p>The Docker container includes a patched factory module applied at image build time that automatically selects the appropriate client based on the endpoint type.</p>"},{"location":"troubleshooting/known-issues.html#client-selection-logic","title":"Client Selection Logic","text":"Endpoint Type Client Used Reason OpenAI Direct (<code>api.openai.com</code>) <code>OpenAIClient</code> Native support Cloud Providers (OpenRouter, Together, etc.) <code>OpenAIClient</code> Parse API support via proxy Local Endpoints (Ollama, localhost) <code>OpenAIGenericClient</code> No parse API available"},{"location":"troubleshooting/known-issues.html#root-cause-analysis","title":"Root Cause Analysis","text":"Before PatchAfter Patch (v3) <pre><code>All custom endpoints \u2192 OpenAIGenericClient\nCloud providers like OpenRouter use basic json_object mode\nResult: Intermittent Pydantic validation failures\n</code></pre> <pre><code>Cloud providers \u2192 OpenAIClient (strict schema via parse API)\nLocal endpoints \u2192 OpenAIGenericClient (json_object mode)\nResult: Reliable structured output for cloud providers\n</code></pre>"},{"location":"troubleshooting/known-issues.html#impact-on-users","title":"Impact on Users","text":"<p>Automatic Handling</p> <p>This client selection happens automatically in the patched factory module. If you're using OpenRouter, Together AI, or other cloud providers, the system automatically uses the appropriate client with strict schema enforcement.</p> <p>If you're experiencing validation errors:</p> <ol> <li>Ensure you're using the patched <code>factories.py</code> from this knowledge system</li> <li>Check that your endpoint URL is correctly classified (cloud vs local)</li> <li>For local LLMs (Ollama), validation errors may still occur as they don't support the parse API</li> </ol> <p>Recommended Cloud Providers</p> <p>The following providers have been tested and work reliably with the patched client selection:</p> <ul> <li>OpenRouter (<code>openrouter.ai</code>) - Excellent structured output support</li> <li>Together AI (<code>api.together.xyz</code>) - Good structured output support</li> <li>Fireworks AI (<code>api.fireworks.ai</code>) - Good structured output support</li> </ul> <p>Upstream Tracking</p> <p>This issue is tracked in the Graphiti repository:</p> <ul> <li>Issue #912 - EdgeDuplicate Pydantic validation errors</li> <li>Issue #1116 - MCP server ignores api_base/base_url configuration</li> </ul>"},{"location":"troubleshooting/known-issues.html#reporting-new-issues","title":"Reporting New Issues","text":"<p>If you encounter an issue not documented here, please:</p> <ol> <li>Check the Common Issues page for troubleshooting steps</li> <li>Search existing GitHub Issues</li> <li>If it's a new issue, open a GitHub issue with:<ul> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>System information (OS, container runtime, versions)</li> </ul> </li> </ol>"},{"location":"usage/advanced.html","title":"Advanced Usage","text":""},{"location":"usage/advanced.html#advanced-usage-guide","title":"Advanced Usage Guide","text":"<p>This guide covers advanced patterns, backup and restore procedures, and expert-level techniques for the Madeinoz Knowledge System.</p>"},{"location":"usage/advanced.html#advanced-usage-patterns","title":"Advanced Usage Patterns","text":""},{"location":"usage/advanced.html#capturing-code-snippets","title":"Capturing Code Snippets","text":"<pre><code>You: Remember this bash script for starting services:\n\n```bash\n#!/bin/bash\npodman start madeinoz-knowledge-graph-mcp\npodman start madeinoz-knowledge-falkordb\necho \"Services started\"\n```\n\nThis starts both knowledge system containers.\n</code></pre> <p>The system captures the code and its purpose.</p>"},{"location":"usage/advanced.html#capturing-conversations","title":"Capturing Conversations","text":"<pre><code>You: Store this conversation we just had about API design patterns.\n[paste or summarize the conversation]\n</code></pre> <p>Good for preserving important discussions.</p>"},{"location":"usage/advanced.html#capturing-research","title":"Capturing Research","text":"<pre><code>You: Remember this research finding: Vector embeddings using text-embedding-3-small\nare 99.8% as accurate as large embeddings but 5x cheaper and 3x faster to compute.\n</code></pre> <p>Perfect for building a research knowledge base.</p>"},{"location":"usage/advanced.html#capturing-meeting-notes","title":"Capturing Meeting Notes","text":"<pre><code>You: Store these meeting notes from the architecture review:\n- Decided on microservices architecture\n- Will use gRPC for service communication\n- PostgreSQL for primary database\n- Redis for caching layer\nAction items: Complete service design by Friday\n</code></pre>"},{"location":"usage/advanced.html#creating-knowledge-chains","title":"Creating Knowledge Chains","text":"<p>Build knowledge over time by connecting related episodes:</p> <p>Day 1: </p><pre><code>Remember: Exploring knowledge graph options. Considering Neo4j and FalkorDB.\n</code></pre><p></p> <p>Day 2: </p><pre><code>Remember: FalkorDB is lighter than Neo4j because it's a Redis module,\nnot a standalone database.\n</code></pre><p></p> <p>Day 3: </p><pre><code>Remember: Decision made - using FalkorDB for Madeinoz Knowledge System.\n</code></pre><p></p> <p>The system automatically links these episodes through their shared entities.</p>"},{"location":"usage/advanced.html#backup-restore","title":"Backup &amp; Restore","text":"<p>For complete backup and restore procedures, see the dedicated Backup &amp; Restore Guide.</p>"},{"location":"usage/advanced.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Return to basic usage for fundamental operations</li> <li>Learn more about how the system works</li> <li>Troubleshoot issues in the troubleshooting guide</li> </ul>"},{"location":"usage/backup-restore.html","title":"Backup & Restore","text":""},{"location":"usage/backup-restore.html#backup-restore-guide","title":"Backup &amp; Restore Guide","text":"<p>Your knowledge graph is stored in Neo4j (the default) or FalkorDB. Here's how to protect and migrate your data.</p> <p>Note: The instructions below show both Neo4j (default) and FalkorDB commands. Use the section that matches your configured backend.</p>"},{"location":"usage/backup-restore.html#quick-backup","title":"Quick Backup","text":"<p>Create a backup of your entire knowledge graph:</p> <p>Neo4j (Default) - Podman:</p> <pre><code># Navigate to pack directory\ncd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# Create backup directory\nmkdir -p backups\n\n# Backup Neo4j data directory\npodman exec madeinoz-knowledge-neo4j neo4j-admin database dump neo4j --to-stdout &gt; ./backups/knowledge-$(date +%Y%m%d-%H%M%S).dump\n\necho \"\u2713 Backup created\"\n</code></pre> <p>Neo4j (Default) - Docker:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nmkdir -p backups\n\ndocker exec madeinoz-knowledge-neo4j neo4j-admin database dump neo4j --to-stdout &gt; ./backups/knowledge-$(date +%Y%m%d-%H%M%S).dump\n\necho \"\u2713 Backup created\"\n</code></pre> <p>FalkorDB Backend - Podman:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nmkdir -p backups\n\n# Backup the FalkorDB data (RDB snapshot)\npodman exec madeinoz-knowledge-falkordb redis-cli BGSAVE\nsleep 2  # Wait for save to complete\npodman cp madeinoz-knowledge-falkordb:/data/dump.rdb ./backups/knowledge-$(date +%Y%m%d-%H%M%S).rdb\n\necho \"\u2713 Backup created\"\n</code></pre> <p>FalkorDB Backend - Docker:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\nmkdir -p backups\n\ndocker exec madeinoz-knowledge-falkordb redis-cli BGSAVE\nsleep 2\ndocker cp madeinoz-knowledge-falkordb:/data/dump.rdb ./backups/knowledge-$(date +%Y%m%d-%H%M%S).rdb\n\necho \"\u2713 Backup created\"\n</code></pre>"},{"location":"usage/backup-restore.html#scheduled-backups","title":"Scheduled Backups","text":"<p>Create a cron job for automatic daily backups:</p> <p>Neo4j (Default) - Podman:</p> <pre><code># Edit crontab\ncrontab -e\n\n# Add this line for daily backup at 2 AM\n0 2 * * * cd ~/.config/pai/Packs/madeinoz-knowledge-system &amp;&amp; podman exec madeinoz-knowledge-neo4j neo4j-admin database dump neo4j --to-stdout &gt; ./backups/knowledge-$(date +\\%Y\\%m\\%d).dump\n</code></pre> <p>Neo4j (Default) - Docker:</p> <pre><code>crontab -e\n\n# Add this line for daily backup at 2 AM\n0 2 * * * cd ~/.config/pai/Packs/madeinoz-knowledge-system &amp;&amp; docker exec madeinoz-knowledge-neo4j neo4j-admin database dump neo4j --to-stdout &gt; ./backups/knowledge-$(date +\\%Y\\%m\\%d).dump\n</code></pre> <p>FalkorDB Backend - Podman:</p> <pre><code>crontab -e\n\n# Add this line for daily backup at 2 AM\n0 2 * * * cd ~/.config/pai/Packs/madeinoz-knowledge-system &amp;&amp; podman exec madeinoz-knowledge-falkordb redis-cli BGSAVE &amp;&amp; sleep 2 &amp;&amp; podman cp madeinoz-knowledge-falkordb:/data/dump.rdb ./backups/knowledge-$(date +\\%Y\\%m\\%d).rdb\n</code></pre> <p>FalkorDB Backend - Docker:</p> <pre><code>crontab -e\n\n# Add this line for daily backup at 2 AM\n0 2 * * * cd ~/.config/pai/Packs/madeinoz-knowledge-system &amp;&amp; docker exec madeinoz-knowledge-falkordb redis-cli BGSAVE &amp;&amp; sleep 2 &amp;&amp; docker cp madeinoz-knowledge-falkordb:/data/dump.rdb ./backups/knowledge-$(date +\\%Y\\%m\\%d).rdb\n</code></pre>"},{"location":"usage/backup-restore.html#restore-from-backup","title":"Restore from Backup","text":"<p>To restore your knowledge graph from a backup:</p> <p>Neo4j (Default) - Podman:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# 1. Stop the running containers\nbun run server-cli stop\n\n# 2. Find your backup file\nls -la backups/\n\n# 3. Restore using neo4j-admin\npodman run --rm -v ./backups:/backups:ro -v madeinoz-knowledge-neo4j-data:/data neo4j:2025.12.1 \\\n    neo4j-admin database load neo4j --from-stdin &lt; ./backups/knowledge-YYYYMMDD-HHMMSS.dump --overwrite-destination\n\n# 4. Restart the knowledge system\nbun run server-cli start\n\n# 5. Verify restoration\nbun run server-cli status\n</code></pre> <p>Neo4j (Default) - Docker:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# 1. Stop the running containers\nbun run server-cli stop\n\n# 2. Find your backup file\nls -la backups/\n\n# 3. Restore using neo4j-admin\ndocker run --rm -v ./backups:/backups:ro -v madeinoz-knowledge-neo4j-data:/data neo4j:2025.12.1 \\\n    neo4j-admin database load neo4j --from-stdin &lt; ./backups/knowledge-YYYYMMDD-HHMMSS.dump --overwrite-destination\n\n# 4. Restart the knowledge system\nbun run server-cli start\n\n# 5. Verify restoration\nbun run server-cli status\n</code></pre> <p>Replace <code>knowledge-YYYYMMDD-HHMMSS.dump</code> with your actual backup filename.</p> <p>FalkorDB Backend - Podman:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# 1. Stop the running containers\nbun run server-cli stop\n\n# 2. Find your backup file\nls -la backups/\n\n# 3. Start a temporary container to restore data\npodman run --rm -v ./backups:/backups:ro -v madeinoz-knowledge-data:/data falkordb/falkordb:latest \\\n    sh -c \"cp /backups/knowledge-YYYYMMDD-HHMMSS.rdb /data/dump.rdb\"\n\n# 4. Restart the knowledge system\nbun run server-cli start\n\n# 5. Verify restoration\nbun run server-cli status\n</code></pre> <p>FalkorDB Backend - Docker:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# 1. Stop the running containers\nbun run server-cli stop\n\n# 2. Find your backup file\nls -la backups/\n\n# 3. Start a temporary container to restore data\ndocker run --rm -v ./backups:/backups:ro -v madeinoz-knowledge-data:/data falkordb/falkordb:latest \\\n    sh -c \"cp /backups/knowledge-YYYYMMDD-HHMMSS.rdb /data/dump.rdb\"\n\n# 4. Restart the knowledge system\nbun run server-cli start\n\n# 5. Verify restoration\nbun run server-cli status\n</code></pre> <p>Replace <code>knowledge-YYYYMMDD-HHMMSS.rdb</code> with your actual backup filename.</p>"},{"location":"usage/backup-restore.html#export-to-json-portable-backup","title":"Export to JSON (Portable Backup)","text":"<p>For a human-readable backup or migration to another system:</p> <p>Neo4j (Default) - Podman/Docker:</p> <pre><code># Connect to Neo4j and export graph data using cypher-shell\npodman exec madeinoz-knowledge-neo4j cypher-shell -u neo4j -p password \\\n    \"MATCH (n)-[r]-&gt;(m) RETURN n, r, m\" &gt; backups/knowledge-export.txt\n\n# Export all nodes\npodman exec madeinoz-knowledge-neo4j cypher-shell -u neo4j -p password \\\n    \"MATCH (n) RETURN n\" &gt; backups/nodes-export.txt\n\n# Export all relationships\npodman exec madeinoz-knowledge-neo4j cypher-shell -u neo4j -p password \\\n    \"MATCH ()-[r]-&gt;() RETURN r\" &gt; backups/relationships-export.txt\n</code></pre> <p>FalkorDB Backend - Podman:</p> <pre><code># Connect to FalkorDB and export graph data\npodman exec madeinoz-knowledge-falkordb redis-cli GRAPH.QUERY graphiti \\\n    \"MATCH (n)-[r]-&gt;(m) RETURN n, r, m\" &gt; backups/knowledge-export.txt\n\n# Export all nodes\npodman exec madeinoz-knowledge-falkordb redis-cli GRAPH.QUERY graphiti \\\n    \"MATCH (n) RETURN n\" &gt; backups/nodes-export.txt\n\n# Export all relationships\npodman exec madeinoz-knowledge-falkordb redis-cli GRAPH.QUERY graphiti \\\n    \"MATCH ()-[r]-&gt;() RETURN r\" &gt; backups/relationships-export.txt\n</code></pre> <p>FalkorDB Backend - Docker:</p> <pre><code># Connect to FalkorDB and export graph data\ndocker exec madeinoz-knowledge-falkordb redis-cli GRAPH.QUERY graphiti \\\n    \"MATCH (n)-[r]-&gt;(m) RETURN n, r, m\" &gt; backups/knowledge-export.txt\n\n# Export all nodes\ndocker exec madeinoz-knowledge-falkordb redis-cli GRAPH.QUERY graphiti \\\n    \"MATCH (n) RETURN n\" &gt; backups/nodes-export.txt\n\n# Export all relationships\ndocker exec madeinoz-knowledge-falkordb redis-cli GRAPH.QUERY graphiti \\\n    \"MATCH ()-[r]-&gt;() RETURN r\" &gt; backups/relationships-export.txt\n</code></pre>"},{"location":"usage/backup-restore.html#full-volume-backup","title":"Full Volume Backup","text":"<p>For a complete backup including all container data:</p> <p>Podman:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# 1. Stop containers\nbun run server-cli stop\n\n# 2. Export the entire volume\npodman volume export madeinoz-knowledge-data &gt; backups/volume-$(date +%Y%m%d-%H%M%S).tar\n\n# 3. Restart containers\nbun run server-cli start\n\necho \"\u2713 Full volume backup created\"\n</code></pre> <p>Docker:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# 1. Stop containers\nbun run server-cli stop\n\n# 2. Export the entire volume (Docker requires a helper container)\ndocker run --rm -v madeinoz-knowledge-data:/data -v $(pwd)/backups:/backup alpine \\\n    tar cvf /backup/volume-$(date +%Y%m%d-%H%M%S).tar -C /data .\n\n# 3. Restart containers\nbun run server-cli start\n\necho \"\u2713 Full volume backup created\"\n</code></pre>"},{"location":"usage/backup-restore.html#restore-from-volume-backup","title":"Restore from Volume Backup","text":"<p>Podman:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# 1. Stop containers\nbun run server-cli stop\n\n# 2. Remove existing volume (WARNING: destroys current data)\npodman volume rm madeinoz-knowledge-data\n\n# 3. Create new volume and restore\npodman volume create madeinoz-knowledge-data\npodman volume import madeinoz-knowledge-data &lt; backups/volume-YYYYMMDD-HHMMSS.tar\n\n# 4. Restart containers\nbun run server-cli start\n\n# 5. Verify\nbun run server-cli status\n</code></pre> <p>Docker:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# 1. Stop containers\nbun run server-cli stop\n\n# 2. Remove existing volume (WARNING: destroys current data)\ndocker volume rm madeinoz-knowledge-data\n\n# 3. Create new volume and restore\ndocker volume create madeinoz-knowledge-data\ndocker run --rm -v madeinoz-knowledge-data:/data -v $(pwd)/backups:/backup alpine \\\n    sh -c \"cd /data &amp;&amp; tar xvf /backup/volume-YYYYMMDD-HHMMSS.tar\"\n\n# 4. Restart containers\nbun run server-cli start\n\n# 5. Verify\nbun run server-cli status\n</code></pre>"},{"location":"usage/backup-restore.html#migration-to-new-machine","title":"Migration to New Machine","text":"<p>To move your knowledge graph to a new computer:</p> <p>Podman - On the old machine:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# Create portable backup\nbun run server-cli stop\npodman volume export madeinoz-knowledge-data &gt; knowledge-migration.tar\nbun run server-cli start\n\n# Transfer the file\nscp knowledge-migration.tar user@newmachine:~/\n</code></pre> <p>Podman - On the new machine:</p> <pre><code># After installing madeinoz-knowledge-system\ncd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# Import the volume\npodman volume create madeinoz-knowledge-data\npodman volume import madeinoz-knowledge-data &lt; ~/knowledge-migration.tar\n\n# Start the system\nbun run server-cli start\nbun run server-cli status\n</code></pre> <p>Docker - On the old machine:</p> <pre><code>cd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# Create portable backup\nbun run server-cli stop\ndocker run --rm -v madeinoz-knowledge-data:/data -v $(pwd):/backup alpine \\\n    tar cvf /backup/knowledge-migration.tar -C /data .\nbun run server-cli start\n\n# Transfer the file\nscp knowledge-migration.tar user@newmachine:~/\n</code></pre> <p>Docker - On the new machine:</p> <pre><code># After installing madeinoz-knowledge-system\ncd ~/.config/pai/Packs/madeinoz-knowledge-system\n\n# Import the volume\ndocker volume create madeinoz-knowledge-data\ndocker run --rm -v madeinoz-knowledge-data:/data -v ~/:/backup alpine \\\n    sh -c \"cd /data &amp;&amp; tar xvf /backup/knowledge-migration.tar\"\n\n# Start the system\nbun run server-cli start\nbun run server-cli status\n</code></pre>"},{"location":"usage/backup-restore.html#backup-best-practices","title":"Backup Best Practices","text":"Practice Recommendation Frequency Daily for active use, weekly for light use Retention Keep at least 7 daily + 4 weekly backups Location Store backups outside the container (local disk, cloud) Verification Test restore from backup periodically Before upgrades Always backup before upgrading the system"},{"location":"usage/backup-restore.html#quick-reference","title":"Quick Reference","text":"<p>Neo4j (Default) - Podman:</p> <pre><code># Backup commands\npodman exec madeinoz-knowledge-neo4j neo4j-admin database dump neo4j --to-stdout &gt; backup.dump\npodman volume export madeinoz-knowledge-neo4j-data &gt; volume-backup.tar\n\n# Restore commands\npodman volume import madeinoz-knowledge-neo4j-data &lt; volume-backup.tar\n\n# Verification\npodman exec madeinoz-knowledge-neo4j cypher-shell -u neo4j -p password \"MATCH (n) RETURN count(n)\"\n</code></pre> <p>Neo4j (Default) - Docker:</p> <pre><code># Backup commands\ndocker exec madeinoz-knowledge-neo4j neo4j-admin database dump neo4j --to-stdout &gt; backup.dump\n\n# Restore commands\ndocker run --rm -v madeinoz-knowledge-neo4j-data:/data -v $(pwd):/backup alpine \\\n    sh -c \"cd /data &amp;&amp; tar xvf /backup/volume-backup.tar\"\n\n# Verification\ndocker exec madeinoz-knowledge-neo4j cypher-shell -u neo4j -p password \"MATCH (n) RETURN count(n)\"\n</code></pre> <p>FalkorDB Backend - Podman:</p> <pre><code># Backup commands\npodman exec madeinoz-knowledge-falkordb redis-cli BGSAVE              # Trigger save\npodman cp madeinoz-knowledge-falkordb:/data/dump.rdb ./backup.rdb     # Copy backup\npodman volume export madeinoz-knowledge-data &gt; volume-backup.tar       # Full volume\n\n# Restore commands\npodman volume import madeinoz-knowledge-data &lt; volume-backup.tar       # Restore volume\npodman cp ./backup.rdb madeinoz-knowledge-falkordb:/data/dump.rdb     # Restore RDB\n\n# Verification\npodman exec madeinoz-knowledge-falkordb redis-cli DBSIZE              # Check DB size\npodman exec madeinoz-knowledge-falkordb redis-cli GRAPH.LIST          # List graphs\n</code></pre> <p>FalkorDB Backend - Docker:</p> <pre><code># Backup commands\ndocker exec madeinoz-knowledge-falkordb redis-cli BGSAVE              # Trigger save\ndocker cp madeinoz-knowledge-falkordb:/data/dump.rdb ./backup.rdb     # Copy backup\ndocker run --rm -v madeinoz-knowledge-data:/data -v $(pwd):/backup alpine \\\n    tar cvf /backup/volume-backup.tar -C /data .                 # Full volume\n\n# Restore commands\ndocker run --rm -v madeinoz-knowledge-data:/data -v $(pwd):/backup alpine \\\n    sh -c \"cd /data &amp;&amp; tar xvf /backup/volume-backup.tar\"        # Restore volume\ndocker cp ./backup.rdb madeinoz-knowledge-falkordb:/data/dump.rdb     # Restore RDB\n\n# Verification\ndocker exec madeinoz-knowledge-falkordb redis-cli DBSIZE              # Check DB size\ndocker exec madeinoz-knowledge-falkordb redis-cli GRAPH.LIST          # List graphs\n</code></pre>"},{"location":"usage/backup-restore.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Return to advanced usage for other expert-level patterns</li> <li>Review basic usage for fundamental operations</li> <li>Learn more about how the system works</li> <li>Troubleshoot issues in the troubleshooting guide</li> </ul>"},{"location":"usage/basic-usage.html","title":"Basic Usage","text":""},{"location":"usage/basic-usage.html#basic-usage-guide","title":"Basic Usage Guide","text":"<p>This guide shows you how to use the Madeinoz Knowledge System in everyday situations. All commands are shown as natural language - just talk to your AI assistant normally.</p>"},{"location":"usage/basic-usage.html#capturing-knowledge","title":"Capturing Knowledge","text":""},{"location":"usage/basic-usage.html#basic-capture","title":"Basic Capture","text":"<p>The simplest way to add knowledge is to just say \"remember this\":</p> <pre><code>You: Remember that Podman is a daemonless container engine.\n</code></pre> <pre><code>AI: Knowledge Captured\n\nStored episode: Podman Architecture\n\nEntities extracted:\n- Podman (Tool)\n- container engine (Concept)\n- daemonless (Property)\n\nRelationships identified:\n- Podman -&gt; is a -&gt; container engine\n- Podman -&gt; has property -&gt; daemonless\n</code></pre>"},{"location":"usage/basic-usage.html#alternative-phrases","title":"Alternative Phrases","text":"<p>All of these work the same way:</p> <ul> <li>\"Remember this\"</li> <li>\"Store this\"</li> <li>\"Add to my knowledge\"</li> <li>\"Save this information\"</li> <li>\"Log this\"</li> <li>\"Keep track of this\"</li> </ul>"},{"location":"usage/basic-usage.html#capturing-different-types-of-information","title":"Capturing Different Types of Information","text":""},{"location":"usage/basic-usage.html#technical-decisions","title":"Technical Decisions","text":"<pre><code>You: Remember that we chose FalkorDB over Neo4j because it's lighter weight and uses Redis as the backend.\n</code></pre> <p>The system uses an LLM (GPT-4) to automatically extract:</p> <ul> <li>The decision (FalkorDB vs Neo4j)</li> <li>The reasoning (lighter weight, Redis backend)</li> <li>Temporal context (when you made this decision)</li> <li>Relationships between concepts</li> </ul>"},{"location":"usage/basic-usage.html#personal-preferences","title":"Personal Preferences","text":"<pre><code>You: Store this: I prefer writing documentation in Markdown rather than Word documents because it's easier to version control.\n</code></pre> <p>The system extracts:</p> <ul> <li>Preference: Markdown over Word</li> <li>Reason: version control compatibility</li> <li>Type: personal preference</li> </ul>"},{"location":"usage/basic-usage.html#procedures-and-how-tos","title":"Procedures and How-Tos","text":"<pre><code>You: Remember this procedure: To reset the knowledge graph, use the clear graph command, then restart the MCP server to rebuild the indices.\n</code></pre> <p>The system creates:</p> <ul> <li>A procedure entity</li> <li>Steps in sequence</li> <li>Related tools and commands</li> </ul>"},{"location":"usage/basic-usage.html#bug-fixes-and-solutions","title":"Bug Fixes and Solutions","text":"<pre><code>You: Remember this fix: When Podman containers can't access the internet, run 'podman network reload' to refresh the network stack.\n</code></pre> <p>Perfect for capturing solutions to problems you've solved.</p>"},{"location":"usage/basic-usage.html#configuration-snippets","title":"Configuration Snippets","text":"<pre><code>You: Save my tmux configuration: I use Ctrl+A as prefix, enable mouse mode, and set the history limit to 50000 lines.\n</code></pre> <p>Great for keeping track of your preferred settings.</p>"},{"location":"usage/basic-usage.html#searching-knowledge","title":"Searching Knowledge","text":""},{"location":"usage/basic-usage.html#basic-search","title":"Basic Search","text":"<p>Ask questions in natural language:</p> <pre><code>You: What do I know about Podman?\n</code></pre> <pre><code>AI: Knowledge Found: Podman\n\nBased on your knowledge graph:\n\nKey Entities:\n1. Podman (Tool)\n   - Daemonless container engine\n   - Alternative to Docker\n   - CLI-compatible with Docker commands\n   - Uses libpod library\n\nRelationships:\n- Podman -&gt; alternative to -&gt; Docker\n- Podman -&gt; is a -&gt; container engine\n- Podman -&gt; has property -&gt; daemonless\n\nEpisodes:\n- \"Podman Architecture\" (2025-01-08)\n- \"Container Networking Fix\" (2025-01-07)\n</code></pre>"},{"location":"usage/basic-usage.html#alternative-search-phrases","title":"Alternative Search Phrases","text":"<p>These all trigger searches:</p> <ul> <li>\"What do I know about X?\"</li> <li>\"Search my knowledge for X\"</li> <li>\"Find information about X\"</li> <li>\"What have I learned about X?\"</li> <li>\"Recall what I know about X\"</li> </ul>"},{"location":"usage/basic-usage.html#searching-by-topic","title":"Searching by Topic","text":"<pre><code>You: What do I know about knowledge graphs?\n</code></pre> <p>Returns everything related to knowledge graphs - Graphiti, Neo4j, entities, relationships, etc.</p>"},{"location":"usage/basic-usage.html#searching-for-procedures","title":"Searching for Procedures","text":"<pre><code>You: What do I know about resetting the knowledge graph?\n</code></pre> <p>Returns procedural knowledge about graph operations.</p>"},{"location":"usage/basic-usage.html#searching-for-preferences","title":"Searching for Preferences","text":"<pre><code>You: What are my development environment preferences?\n</code></pre> <p>Returns your stored preferences about editors, tools, settings, etc.</p>"},{"location":"usage/basic-usage.html#searching-for-decisions","title":"Searching for Decisions","text":"<pre><code>You: What architectural decisions have I made?\n</code></pre> <p>Returns episodes about technical choices and their reasoning.</p>"},{"location":"usage/basic-usage.html#filtering-by-entity-type","title":"Filtering by Entity Type","text":"<p>You can narrow searches to specific types of knowledge:</p> <pre><code>You: Search my knowledge for procedures about container setup\n</code></pre> <p>Available Entity Types:</p> Type What It Captures Example Query Preference Your choices and settings \"Find my preferences about editors\" Procedure How-to guides and processes \"Search for procedures about deployment\" Learning Knowledge from learning sessions \"What learnings do I have about APIs?\" Research Findings from research \"Show research about database options\" Decision Architectural choices \"Find decisions about architecture\" Feature Feature implementations \"Search for features we've built\" Person Individual people \"What do I know about team members?\" Organization Companies and teams \"Find information about vendors\" Concept Ideas and technologies \"Search concepts about microservices\" <p>Examples:</p> <pre><code>You: Find only my procedures about Podman\n</code></pre> <pre><code>You: What research have I done about knowledge graphs?\n</code></pre> <pre><code>You: Show me my learning about TypeScript\n</code></pre> <p>This is especially useful when you have lots of knowledge and want to focus on a specific type.</p>"},{"location":"usage/basic-usage.html#finding-connections","title":"Finding Connections","text":""},{"location":"usage/basic-usage.html#discover-relationships","title":"Discover Relationships","text":"<pre><code>You: How are Graphiti and FalkorDB related?\n</code></pre> <pre><code>AI: Relationships between Graphiti and Neo4j:\n\nDirect Relationships:\n- Graphiti -&gt; uses as backend -&gt; Neo4j\n- Neo4j -&gt; provides storage for -&gt; Graphiti\n\nContext:\n\"Graphiti is a knowledge graph framework that uses Neo4j as its\ngraph database backend. Neo4j provides native graph storage\nwith Cypher query support.\" (captured 2025-01-08)\n</code></pre>"},{"location":"usage/basic-usage.html#alternative-phrases_1","title":"Alternative Phrases","text":"<ul> <li>\"How are X and Y connected?\"</li> <li>\"What's the relationship between X and Y?\"</li> <li>\"Show me connections between X and Y\"</li> </ul>"},{"location":"usage/basic-usage.html#finding-related-concepts","title":"Finding Related Concepts","text":"<pre><code>You: What's related to container orchestration in my knowledge?\n</code></pre> <p>Shows all entities connected to that topic - Docker, Podman, Kubernetes, Docker Compose, etc.</p>"},{"location":"usage/basic-usage.html#reviewing-your-knowledge","title":"Reviewing Your Knowledge","text":""},{"location":"usage/basic-usage.html#recent-additions","title":"Recent Additions","text":"<pre><code>You: What did I learn recently?\n</code></pre> <pre><code>AI: Recent Knowledge Additions:\n\nPast 7 Days:\n1. \"Madeinoz Knowledge System Setup\" (2025-01-08)\n   - Installation and configuration\n   - MCP server setup\n\n2. \"Podman Networking Fix\" (2025-01-07)\n   - Network reload command\n   - Troubleshooting connectivity\n\n3. \"Bun vs npm Performance\" (2025-01-06)\n   - Package installation speed\n   - Runtime comparison\n</code></pre>"},{"location":"usage/basic-usage.html#filtered-recent-additions","title":"Filtered Recent Additions","text":"<pre><code>You: What did I learn about containers this week?\n</code></pre> <p>Shows only recent knowledge about containers.</p>"},{"location":"usage/basic-usage.html#temporal-search-date-filtering","title":"Temporal Search (Date Filtering)","text":"<p>Search your knowledge by date using the <code>--since</code> and <code>--until</code> flags:</p> <p>Today's knowledge:</p> <pre><code>You: What did I learn today?\n</code></pre> <p>Last 7 days:</p> <pre><code>You: What did I learn this week about APIs?\n</code></pre> <p>Using CLI for precise date filtering:</p> <pre><code># Today's knowledge\nbun run tools/knowledge-cli.ts search_nodes \"AI\" --since today\n\n# Last 7 days\nbun run tools/knowledge-cli.ts search_facts \"decisions\" --since 7d\n\n# Specific date range\nbun run tools/knowledge-cli.ts search_nodes \"project\" --since 2026-01-01 --until 2026-01-15\n\n# Yesterday only\nbun run tools/knowledge-cli.ts search_nodes \"learning\" --since yesterday --until today\n</code></pre> <p>Supported date formats:</p> Format Example Description <code>today</code> <code>--since today</code> Start of current day <code>yesterday</code> <code>--since yesterday</code> Start of previous day <code>Nd</code> <code>--since 7d</code> N days ago <code>Nw</code> <code>--since 2w</code> N weeks ago <code>Nm</code> <code>--since 1m</code> N months ago ISO 8601 <code>--since 2026-01-15</code> Specific date <p>Temporal search helps you:</p> <ul> <li>Review what you learned on a specific day</li> <li>Find decisions made during a particular period</li> <li>Track knowledge accumulation over time</li> </ul>"},{"location":"usage/basic-usage.html#getting-system-status","title":"Getting System Status","text":"<pre><code>You: Show me knowledge graph status\n</code></pre> <pre><code>AI: Knowledge Graph Status:\n\nDatabase:\n  Total Entities: 156\n  Total Facts: 342\n  Total Episodes: 89\n\nServer:\n  Status: Running\n  Uptime: 3 days, 4 hours\n  MCP Endpoint: http://localhost:8000/sse\n\nRecent Activity:\n  Last capture: 2 hours ago\n  Last search: 5 minutes ago\n</code></pre>"},{"location":"usage/basic-usage.html#managing-your-knowledge","title":"Managing Your Knowledge","text":""},{"location":"usage/basic-usage.html#bulk-import","title":"Bulk Import","text":"<p>Import multiple documents at once:</p> <pre><code>You: Import these documents to my knowledge:\n[list of files or paste content]\n</code></pre> <p>The system processes each document and extracts entities from all of them.</p>"},{"location":"usage/basic-usage.html#clearing-the-graph","title":"Clearing the Graph","text":"<p>Warning: This deletes everything!</p> <pre><code>You: Clear my knowledge graph\n</code></pre> <pre><code>AI: \u26a0\ufe0f  Warning: This will delete all your knowledge\n\nCurrent State:\n- Entities: 156\n- Facts: 342\n- Episodes: 89\n\nAre you sure? (Type 'yes' to confirm)\n</code></pre> <p>Use this when you want to start completely fresh.</p>"},{"location":"usage/basic-usage.html#how-the-llm-processes-your-knowledge","title":"How the LLM Processes Your Knowledge","text":"<p>When you capture knowledge, the system uses LLMs in two ways:</p> <ol> <li>Entity Extraction (GPT-4) - Identifies people, tools, concepts, procedures, and their relationships</li> <li>Embedding Generation (text-embedding-3-small) - Creates searchable vectors for semantic search</li> </ol> <p>This means:</p> <ul> <li>You write naturally, the AI structures it for you</li> <li>Search works by meaning, not just keywords</li> <li>Relationships are automatically discovered</li> </ul> <p>Pro tip: More detailed input = better extraction. \"Podman is fast\" extracts less than \"Podman starts containers faster than Docker because it doesn't need a daemon.\"</p> <p>See concepts/knowledge-graph.md for a deep dive on LLM roles and cost optimization.</p>"},{"location":"usage/basic-usage.html#tips-for-effective-knowledge-management","title":"Tips for Effective Knowledge Management","text":""},{"location":"usage/basic-usage.html#1-be-specific","title":"1. Be Specific","text":"<p>Instead of:</p> <pre><code>Remember Docker.\n</code></pre> <p>Try:</p> <pre><code>Remember that Docker requires a daemon process running as root,\nwhich is why Podman is often preferred for rootless containers.\n</code></pre> <p>More detail = better entity extraction.</p>"},{"location":"usage/basic-usage.html#2-include-context","title":"2. Include Context","text":"<p>Instead of:</p> <pre><code>Remember that config file.\n</code></pre> <p>Try:</p> <pre><code>Remember my VS Code settings: 2-space tabs, auto-save enabled,\nDracula theme, and JetBrains Mono font.\n</code></pre> <p>Context helps with future searches.</p>"},{"location":"usage/basic-usage.html#3-explain-relationships","title":"3. Explain Relationships","text":"<p>Instead of:</p> <pre><code>Remember Graphiti and FalkorDB.\n</code></pre> <p>Try:</p> <pre><code>Remember that Graphiti uses FalkorDB as its graph database backend\nfor storing entities and relationships.\n</code></pre> <p>Explicit relationships make connections clearer.</p>"},{"location":"usage/basic-usage.html#4-add-temporal-context-when-relevant","title":"4. Add Temporal Context When Relevant","text":"<p>Instead of:</p> <pre><code>Remember we had a bug.\n</code></pre> <p>Try:</p> <pre><code>Remember that on January 8th we fixed the container networking bug\nby adding a network reload command to the startup script.\n</code></pre> <p>Temporal context helps track how your knowledge evolves.</p>"},{"location":"usage/basic-usage.html#5-review-regularly","title":"5. Review Regularly","text":"<p>Once a week, run:</p> <pre><code>What did I learn this week?\n</code></pre> <p>This helps reinforce knowledge and spot gaps.</p>"},{"location":"usage/basic-usage.html#working-with-multiple-knowledge-graphs","title":"Working with Multiple Knowledge Graphs","text":"<p>You can maintain separate graphs for different purposes:</p>"},{"location":"usage/basic-usage.html#setting-up-groups","title":"Setting Up Groups","text":"<p>In your PAI config (<code>$PAI_DIR/.env</code> or <code>~/.claude/.env</code>):</p> <pre><code>MADEINOZ_KNOWLEDGE_GROUP_ID=work\n</code></pre> <p>Or specify in commands:</p> <pre><code>Remember this in my work knowledge: [information]\n</code></pre>"},{"location":"usage/basic-usage.html#use-cases-for-multiple-groups","title":"Use Cases for Multiple Groups","text":"<ul> <li>work - Professional knowledge, project decisions</li> <li>personal - Personal preferences, life organization</li> <li>research - Academic or exploratory learning</li> <li>code - Programming patterns and solutions</li> </ul>"},{"location":"usage/basic-usage.html#switching-between-groups","title":"Switching Between Groups","text":"<p>Change the GROUP_ID in your config and restart the server, or use group-specific commands if your AI assistant supports them.</p>"},{"location":"usage/basic-usage.html#integration-with-other-pai-systems","title":"Integration with Other PAI Systems","text":""},{"location":"usage/basic-usage.html#memory-system-integration","title":"Memory System Integration","text":"<p>The PAI Memory System (~/.claude/MEMORY/) automatically syncs with the sync hook:</p> <p>What Gets Synced:</p> <ul> <li>Learning captures from LEARNING/ALGORITHM/ (task execution insights)</li> <li>Learning captures from LEARNING/SYSTEM/ (PAI/tooling insights)</li> <li>Research findings from RESEARCH/</li> </ul> <p>Automatic Sync: The hook runs at session start and syncs new captures automatically.</p> <p>Manual Sync:</p> <pre><code>bun run ~/.claude/hooks/sync-memory-to-knowledge.ts --verbose\n</code></pre>"},{"location":"usage/basic-usage.html#checking-sync-status","title":"Checking Sync Status","text":"<pre><code># Dry run - see what would be synced\nbun run ~/.claude/hooks/sync-memory-to-knowledge.ts --dry-run\n\n# View sync history\ncat ~/.claude/MEMORY/STATE/knowledge-sync/sync-state.json\n</code></pre>"},{"location":"usage/basic-usage.html#search-caching","title":"Search Caching","text":"<p>The knowledge system caches search results to make repeated queries faster.</p>"},{"location":"usage/basic-usage.html#how-it-works","title":"How It Works","text":"<p>When you search for something, the result is cached for 5 minutes. If you ask the same question again within that time, you get an instant response without waiting for the database.</p> <p>Example:</p> <pre><code>You: What do I know about Podman?\nAI: [fetches from database - takes ~500ms]\n\nYou: What do I know about Podman?\nAI: [returns cached result - instant]\n</code></pre>"},{"location":"usage/basic-usage.html#when-cache-refreshes","title":"When Cache Refreshes","text":"<ul> <li>Automatically: After 5 minutes, the next search fetches fresh data</li> <li>After adding knowledge: New knowledge becomes searchable after cache expires</li> </ul>"},{"location":"usage/basic-usage.html#if-you-need-fresh-results","title":"If You Need Fresh Results","text":"<p>After adding new knowledge and wanting to search for it immediately:</p> <ol> <li>Wait 5 minutes for automatic refresh, or</li> <li>Ask a slightly different question (different queries aren't cached together):</li> </ol> <pre><code>You: Tell me about Podman containers\n</code></pre> <p>instead of</p> <pre><code>You: What do I know about Podman?\n</code></pre>"},{"location":"usage/basic-usage.html#whats-cached-vs-not-cached","title":"What's Cached vs Not Cached","text":"Action Cached? Why Searching knowledge \u2705 Yes Speeds up repeated queries Searching relationships \u2705 Yes Speeds up repeated queries Adding knowledge \u274c No Must always save to database Getting recent episodes \u274c No Needs real-time data Checking system status \u274c No Needs real-time data <p>Tip: Caching is transparent - you don't need to think about it. It just makes things faster.</p>"},{"location":"usage/basic-usage.html#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>Capture immediately - Don't wait to remember details later</li> <li>Be descriptive - More detail is better than less</li> <li>Use natural language - Write as you'd explain to a friend</li> <li>Review weekly - See what you've learned</li> <li>Search first - Before researching externally, check your knowledge</li> <li>Connect concepts - Explicitly mention relationships when you know them</li> <li>Don't worry about organization - The system handles that automatically</li> </ol>"},{"location":"usage/basic-usage.html#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about how the system works</li> <li>Explore advanced usage patterns</li> <li>Troubleshoot issues in the troubleshooting guide</li> </ul>"},{"location":"usage/memory-decay.html","title":"Memory Decay & Lifecycle Management","text":""},{"location":"usage/memory-decay.html#memory-decay-lifecycle-management","title":"Memory Decay &amp; Lifecycle Management","text":"<p>The Madeinoz Knowledge System includes an intelligent memory decay and lifecycle management system (Feature 009) that automatically prioritizes important memories, allows stale information to fade, and maintains sustainable graph growth over time.</p>"},{"location":"usage/memory-decay.html#overview","title":"Overview","text":"<p>Why Memory Decay Matters</p> <p>Without decay management, your knowledge graph would accumulate every piece of information indefinitely, leading to:</p> <ul> <li>Bloated search results - Trivial temporary information crowds out important knowledge</li> <li>Degraded performance - More memories = slower queries and higher costs</li> <li>Stale data prominence - Old, irrelevant memories appear alongside current information</li> </ul> <p>Memory decay solves this by automatically:</p> <ul> <li>Prioritizing important memories in search results</li> <li>Fading unused, unimportant information over time</li> <li>Archiving or deleting stale memories to maintain graph health</li> </ul>"},{"location":"usage/memory-decay.html#key-concepts","title":"Key Concepts","text":""},{"location":"usage/memory-decay.html#importance-classification-1-5","title":"Importance Classification (1-5)","text":"<p>Every memory is assigned an importance score at ingestion time:</p> Level Name Description Examples 5 CORE Fundamental to your identity or work \"I am allergic to shellfish\", \"My name is Stephen\" 4 HIGH Important to your current projects \"Working on payment feature this sprint\", \"Team uses TypeScript\" 3 MODERATE General knowledge, default \"Prefers dark mode\", \"Uses VS Code\" 2 LOW Useful but replaceable \"Read an article about Rust\", \"Tried a new library\" 1 TRIVIAL Ephemeral, can forget quickly \"Weather was nice today\", \"Had coffee at 9am\" <p></p>"},{"location":"usage/memory-decay.html#how-it-works","title":"How it works","text":"<ul> <li>The LLM analyzes the content and context during ingestion</li> <li>Default fallback: <code>3</code> (MODERATE) if LLM unavailable</li> <li>Higher importance = slower decay, prioritized in search</li> </ul> <p>LLM Central Tendency Bias</p> <p>Problem: All LLMs exhibit a well-documented \"central tendency bias\" or \"neutrality bias\" - they systematically avoid extreme ratings and cluster toward middle options (like rating everything as importance=3/MODERATE).</p> <p>Research findings: - This bias affects all LLM models regardless of provider - RLHF (Reinforcement Learning from Human Feedback) can exacerbate the problem by pushing models toward over-neutralization - Without intervention, most content gets rated as 3/3 regardless of actual importance</p> <p>Our mitigation: The classification prompt uses forced-choice techniques to overcome this bias: - Explicit instructions to avoid neutral ratings - Decision framework forcing 2/4 choices over default 3 - Specific examples for each importance level with test questions - \"When in doubt, choose 2 or 4\" reinforcement</p> <p>Results: With forced-choice prompting: - SSN \u2192 5/5 (correctly identified as CORE) - \"Wonder about weather\" \u2192 1/1 (correctly identified as TRIVIAL) - \"Paris is capital of France\" \u2192 2/5 (correctly identified as LOW but permanent)</p> <p>References: - Prompt Perturbations Reveal Human-Like Biases in LLMs - FadeMem: Biologically-Inspired Forgetting - Quantifying and Mitigating Label Bias in LLMs</p>"},{"location":"usage/memory-decay.html#stability-classification-1-5","title":"Stability Classification (1-5)","text":"<p>Every memory is assigned a stability score predicting how likely it is to change:</p> Level Name Description Examples 5 PERMANENT Never changes \"Birth date\", \"Education history\" 4 HIGH Rarely changes (months/years) \"Home address\", \"Job title\" 3 MODERATE Changes occasionally (weeks/months) \"Current project\", \"Team structure\" 2 LOW Changes regularly (days/weeks) \"Current sprint goals\", \"Reading list\" 1 VOLATILE Changes frequently (hours/days) \"Today's meeting notes\", \"Current task\""},{"location":"usage/memory-decay.html#how-it-works_1","title":"How it works","text":"<ul> <li>The LLM predicts volatility based on content type</li> <li>Default fallback: <code>3</code> (MODERATE) if LLM unavailable</li> <li>Higher stability = longer half-life, slower decay</li> </ul>"},{"location":"usage/memory-decay.html#decay-score-00-10","title":"Decay Score (0.0-1.0)","text":"<p>The decay score represents how \"stale\" a memory has become:</p> <ul> <li>0.0 = Fresh (recently accessed or created)</li> <li>0.5 = Somewhat stale</li> <li>1.0 = Fully decayed (should be archived or deleted)</li> </ul>"},{"location":"usage/memory-decay.html#calculation","title":"Calculation","text":"<p>The decay score uses exponential decay (not linear), matching how human memory and real-world phenomena work:</p> <pre><code>decay_score = 1 - exp(-\u03bb \u00d7 days_since_access)\nwhere \u03bb = ln(2) / half_life_days\n</code></pre> <p>Why exponential?</p> <ul> <li>Linear decay would lose value at constant rate (e.g., 0.56% per day), reaching 100% after 180 days</li> <li>Exponential decay follows natural patterns:</li> <li>Radioactive decay</li> <li>Drug elimination from the body</li> <li>Human memory forgetting (Ebbinghaus curve)</li> <li>Information relevance over time</li> </ul> <p>Exponential vs Linear Decay (180-day half-life)</p> <p>Linear decay (constant 0.56% per day): - Day 0: 0% decay - Day 90: 50% decay - Day 180: 100% decay (completely gone)</p> <p>Exponential decay (our implementation): - Day 0: 0% decay - Day 30: 11% decay - Day 90: 29% decay - Day 180: 50% decay (half-life) - Day 360: 75% decay - Day 540: 87.5% decay - Never truly reaches 100%</p> <p></p> <p>Decay progression by half-life:</p> Half-Lives Decay Score Remaining Value 0 0.00 100% (fresh) 1 0.50 50% 2 0.75 25% 3 0.875 12.5% 4 0.9375 6.25% 5 0.96875 3.125% 6 0.9844 1.56% <p>Impact of stability on half-life:</p> <p>The stability level adjusts the half-life:</p> Stability Half-Life Days to 75% Decay Days to 87.5% Decay 1 (VOLATILE) 60 days 120 180 2 (LOW) 120 days 240 360 3 (MODERATE) 180 days 360 540 4 (HIGH) 240 days 480 720 5 (PERMANENT) \u221e Never Never (\u03bb = 0) <p>Key Insight</p> <p>Exponential decay means memories lose value quickly at first, then decay slows down over time. This preserves older memories that have proven valuable (high stability) while allowing trivial information to fade rapidly.</p>"},{"location":"usage/memory-decay.html#lifecycle-states","title":"Lifecycle States","text":"<p>Memories transition through 5 lifecycle states based on decay score and time inactive:</p> <p></p> State Description Search Behavior Recovery ACTIVE Recently accessed, full relevance Ranked normally N/A DORMANT Not accessed for 90+ days Lower priority Auto-reactivates on access ARCHIVED Not accessed for 180+ days Much lower priority Auto-reactivates on access EXPIRED Not accessed for 360+ days Excluded from search Manual recovery only SOFT_DELETED Deleted, 90-day recovery window Hidden from search Admin recovery within 90 days <p>Permanent memories exempt: Importance \u22654 AND Stability \u22654 = PERMANENT (never decays)</p>"},{"location":"usage/memory-decay.html#weighted-search-results","title":"Weighted Search Results","text":"<p>Primary Benefit</p> <p>The main user-facing benefit of memory decay is better search results.</p> <p>Without decay: Results ranked purely by semantic similarity With decay: Results ranked by semantic relevance (60%) + recency (25%) + importance (15%)</p> <p></p>"},{"location":"usage/memory-decay.html#how-search-ranking-works","title":"How Search Ranking Works","text":"<p>When you search for knowledge, results are scored using:</p> <pre><code>weighted_score = (semantic_similarity \u00d7 0.60) + (recency_boost \u00d7 0.25) + (importance \u00d7 0.15)\n</code></pre> <p>Example:</p> Memory Semantic Recency Importance Weighted Score Rank \"Project architecture\" 0.85 0.90 0.80 (HIGH) 0.8475 1st \"Random blog post\" 0.90 0.30 0.20 (LOW) 0.6200 2nd \"Today's weather\" 0.95 0.95 0.20 (TRIVIAL) 0.7150 3rd <p>Even though \"Today's weather\" has the highest semantic similarity (0.95), the \"Project architecture\" memory ranks first because it combines good semantic match with high importance and reasonable recency.</p>"},{"location":"usage/memory-decay.html#configuring-search-weights","title":"Configuring Search Weights","text":"<p>Edit <code>config/decay-config.yaml</code>:</p> <pre><code>decay:\n  weights:\n    semantic: 0.60    # Vector similarity weight\n    recency: 0.25     # Temporal freshness weight\n    importance: 0.15  # Importance score weight\n</code></pre> <p>Note: Weights must sum to 1.0. Adjust based on your priorities:</p> <ul> <li>Want recent stuff more? Increase <code>recency</code></li> <li>Only care about accuracy? Increase <code>semantic</code></li> <li>Always show important stuff? Increase <code>importance</code></li> </ul>"},{"location":"usage/memory-decay.html#lifecycle-management","title":"Lifecycle Management","text":""},{"location":"usage/memory-decay.html#state-transitions","title":"State Transitions","text":"<p>Memories automatically transition states based on:</p> <ol> <li>Time inactive (days since last access)</li> <li>Decay score (0.0-1.0 scale)</li> <li>Importance threshold (some states exempt important memories)</li> </ol> <p>Default thresholds:</p> Transition Criteria ACTIVE \u2192 DORMANT 90 days inactive AND decay_score \u2265 0.3 DORMANT \u2192 ARCHIVED 180 days inactive AND decay_score \u2265 0.6 ARCHIVED \u2192 EXPIRED 360 days inactive AND decay_score \u2265 0.9 AND importance \u2264 3 EXPIRED \u2192 SOFT_DELETED Maintenance runs (automatic) <p>Reactivation: Any memory access (search result, explicit retrieval) immediately transitions DORMANT or ARCHIVED memories back to ACTIVE.</p>"},{"location":"usage/memory-decay.html#permanent-memory-protection","title":"Permanent Memory Protection","text":"<p>Memories with importance \u22654 AND stability \u22654 are classified as PERMANENT:</p> <ul> <li>Never accumulate decay (decay_score always 0.0)</li> <li>Never transition lifecycle states (always ACTIVE)</li> <li>Exempt from archival and deletion</li> <li>Prioritized in search results</li> </ul>"},{"location":"usage/memory-decay.html#use-cases","title":"Use cases","text":"<ul> <li>Core identity information (name, birthdate)</li> <li>Critical facts (allergies, medical conditions)</li> <li>Permanent career details (profession, degree)</li> </ul>"},{"location":"usage/memory-decay.html#maintenance-operations","title":"Maintenance Operations","text":""},{"location":"usage/memory-decay.html#automatic-maintenance","title":"Automatic Maintenance","text":"<p>The system runs automatic maintenance every 24 hours (configurable):</p>"},{"location":"usage/memory-decay.html#what-it-does","title":"What it does","text":"<ol> <li>Recalculates decay scores for all memories</li> <li>Transitions memories between lifecycle states</li> <li>Soft-deletes expired memories (with 90-day retention)</li> <li>Generates health metrics for Grafana</li> </ol>"},{"location":"usage/memory-decay.html#how-to-verify","title":"How to verify","text":"<pre><code># Check last maintenance run\ncurl http://localhost:8000/health | jq '.maintenance.last_run_at'\n\n# View maintenance metrics\ncurl http://localhost:9091/metrics | grep knowledge_decay_maintenance\n</code></pre>"},{"location":"usage/memory-decay.html#manual-maintenance","title":"Manual Maintenance","text":"<p>Trigger maintenance on demand:</p> <pre><code># Via MCP tool (if exposed)\n{\n  \"name\": \"run_maintenance\",\n  \"arguments\": {\n    \"max_duration_minutes\": 10\n  }\n}\n</code></pre>"},{"location":"usage/memory-decay.html#what-to-expect","title":"What to expect","text":"<ul> <li>Processes memories in batches of 500</li> <li>Maximum runtime: 10 minutes (configurable)</li> <li>Updates Prometheus metrics throughout</li> </ul>"},{"location":"usage/memory-decay.html#configuration","title":"Configuration","text":""},{"location":"usage/memory-decay.html#configuration-file","title":"Configuration File","text":"<p>Location: <code>config/decay-config.yaml</code></p> <p>This file is copied into the Docker container at build time. Rebuild after changes:</p> <pre><code># 1. Edit configuration\nnano config/decay-config.yaml\n\n# 2. Rebuild Docker image\ndocker build -f docker/Dockerfile -t madeinoz-knowledge-system:local .\n\n# 3. Restart containers\nbun run server-cli stop\nbun run server-cli start --dev\n</code></pre>"},{"location":"usage/memory-decay.html#key-configuration-sections","title":"Key Configuration Sections","text":""},{"location":"usage/memory-decay.html#decay-thresholds","title":"Decay Thresholds","text":"<pre><code>decay:\n  thresholds:\n    dormant:\n      days: 90           # Days before ACTIVE \u2192 DORMANT\n      decay_score: 0.3   # Decay score threshold\n    archived:\n      days: 180          # Days before DORMANT \u2192 ARCHIVED\n      decay_score: 0.6   # Decay score threshold\n    expired:\n      days: 360          # Days before ARCHIVED \u2192 EXPIRED\n      decay_score: 0.9   # Decay score threshold\n      max_importance: 3  # Only expire if importance \u2264 3\n</code></pre> <p>How thresholds work with half-life:</p> <p>Transitions require BOTH conditions to be met: minimum days inactive AND decay_score threshold.</p> <p>With the default 180-day half-life for MODERATE memories:</p> Transition Config (minimum) Actual timing Decay at minimum days ACTIVE \u2192 DORMANT 90 days + decay \u2265 0.3 ~93 days Day 90: decay \u2248 0.29 DORMANT \u2192 ARCHIVED 180 days + decay \u2265 0.6 ~238 days Day 180: decay \u2248 0.50 ARCHIVED \u2192 EXPIRED 360 days + decay \u2265 0.9 ~598 days Day 360: decay \u2248 0.75 <p>The days threshold is a minimum\u2014actual transitions occur when decay_score reaches the threshold.</p>"},{"location":"usage/memory-decay.html#maintenance-schedule","title":"Maintenance Schedule","text":"<pre><code>decay:\n  maintenance:\n    batch_size: 500             # Memories per batch\n    max_duration_minutes: 10    # Maximum runtime\n    schedule_interval_hours: 24 # Hours between automatic runs (0 = disabled)\n</code></pre> <p>Set to <code>0</code> to disable automatic maintenance (run manually only).</p>"},{"location":"usage/memory-decay.html#search-weights","title":"Search Weights","text":"<pre><code>decay:\n  weights:\n    semantic: 0.60    # Vector similarity\n    recency: 0.25     # Temporal freshness\n    importance: 0.15  # Importance score\n</code></pre>"},{"location":"usage/memory-decay.html#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"usage/memory-decay.html#grafana-dashboard","title":"Grafana Dashboard","text":"<p>The Memory Decay dashboard provides real-time visibility:</p> <p>Key Panels:</p> <ol> <li>Total Memories - Current memory count (excluding soft-deleted)</li> <li>Avg Decay Score - Average decay across all memories (0.0 = healthy)</li> <li>Maintenance - Shows \"Completed\" if maintenance ran successfully</li> <li>Total Purged - Count of soft-deleted memories permanently removed</li> <li>By State - Pie chart showing distribution across lifecycle states</li> <li>Avg Scores - Average importance and stability scores</li> <li>By Importance - Distribution across importance levels (TRIVIAL \u2192 CORE)</li> </ol> <p>Access: <code>http://localhost:3002/d/memory-decay-dashboard</code> (dev)</p>"},{"location":"usage/memory-decay.html#health-endpoint","title":"Health Endpoint","text":"<pre><code>curl http://localhost:8000/health | jq\n</code></pre> <p>Response:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"maintenance\": {\n    \"last_run_at\": \"2026-01-29T14:00:00Z\",\n    \"last_duration_seconds\": 45.2,\n    \"last_run_status\": \"success\"\n  },\n  \"memory_counts\": {\n    \"total\": 65,\n    \"by_state\": {\n      \"ACTIVE\": 65,\n      \"DORMANT\": 0,\n      \"ARCHIVED\": 0,\n      \"EXPIRED\": 0\n    }\n  },\n  \"decay_metrics\": {\n    \"avg_decay_score\": 0.0,\n    \"avg_importance\": 3.0,\n    \"avg_stability\": 3.0\n  }\n}\n</code></pre>"},{"location":"usage/memory-decay.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"usage/memory-decay.html#memories-decaying-too-fast","title":"Memories Decaying Too Fast","text":"<p>Symptom: Important memories becoming DORMANT or ARCHIVED quickly</p> <p>Solutions:</p> <ol> <li> <p>Check importance classification: </p><pre><code># View memory attributes via Neo4j Browser\n# http://localhost:7474\nMATCH (n:Entity)\nWHERE n.name CONTAINS \"important-thing\"\nRETURN n.name, n.attributes.importance, n.attributes.stability\n</code></pre><p></p> </li> <li> <p>Adjust decay thresholds: </p><pre><code># config/decay-config.yaml\ndecay:\n  base_half_life_days: 60  # Increase for slower decay\n</code></pre><p></p> </li> <li> <p>Mark important memories as permanent:</p> </li> <li>Edit memory and set importance=4, stability=4</li> <li>Or rebuild graph with corrected importance</li> </ol>"},{"location":"usage/memory-decay.html#maintenance-not-running","title":"Maintenance Not Running","text":"<p>Symptom: Grafana shows \"Never\" for Maintenance status</p> <p>Solutions:</p> <ol> <li> <p>Check schedule configuration: </p><pre><code># config/decay-config.yaml\ndecay:\n  maintenance:\n    schedule_interval_hours: 24  # Ensure not 0\n</code></pre><p></p> </li> <li> <p>Check MCP server logs: </p><pre><code>bun run server-cli logs --mcp | grep -i maintenance\n</code></pre><p></p> </li> <li> <p>Verify maintenance code loaded: </p><pre><code>docker exec madeinoz-knowledge-graph-mcp-dev ls -la /app/mcp/src/utils/maintenance_service.py\n</code></pre><p></p> </li> </ol>"},{"location":"usage/memory-decay.html#poor-search-results","title":"Poor Search Results","text":"<p>Symptom: Trivial recent results outranking important older ones</p> <p>Solutions:</p> <ol> <li> <p>Adjust search weights: </p><pre><code># config/decay-config.yaml\ndecay:\n  weights:\n    semantic: 0.50    # Decrease semantic weight\n    recency: 0.20     # Decrease recency weight\n    importance: 0.30  # Increase importance weight\n</code></pre><p></p> </li> <li> <p>Ensure importance classification is working: </p><pre><code># Check classification metrics\ncurl http://localhost:9091/metrics | grep knowledge_classification\n</code></pre><p></p> </li> <li> <p>Verify decay scores are being calculated: </p><pre><code>curl http://localhost:9091/metrics | grep knowledge_decay_score_avg\n</code></pre><p></p> </li> </ol>"},{"location":"usage/memory-decay.html#advanced-topics","title":"Advanced Topics","text":""},{"location":"usage/memory-decay.html#soft-delete-recovery","title":"Soft-Delete Recovery","text":"<p>Memories in <code>SOFT_DELETED</code> state can be recovered within 90 days:</p> <pre><code># Via Neo4j Browser\nMATCH (n:Entity)\nWHERE n.attributes.soft_deleted_at IS NOT NULL\n  AND datetime() &gt; datetime(n.attributes.soft_deleted_at) + duration('P90D')\nRETURN n.name, n.attributes.soft_deleted_at\n</code></pre>"},{"location":"usage/memory-decay.html#recovery-process","title":"Recovery process","text":"<ol> <li>Identify memory to recover via query above</li> <li>Clear soft_deleted_at attribute</li> <li>Set lifecycle_state to \"ARCHIVED\"</li> <li>Memory will be re-evaluated on next maintenance</li> </ol>"},{"location":"usage/memory-decay.html#bulk-import-considerations","title":"Bulk Import Considerations","text":"<p>When importing large amounts of data:</p>"},{"location":"usage/memory-decay.html#classification-behavior","title":"Classification behavior","text":"<ul> <li>All memories start with default importance=3, stability=3</li> <li>Background LLM classification refines scores asynchronously</li> <li>First maintenance run after import will properly classify everything</li> </ul>"},{"location":"usage/memory-decay.html#recommendations","title":"Recommendations","text":"<ol> <li>Import in batches of 1000-5000 memories</li> <li>Wait for maintenance to run between batches</li> <li>Monitor classification metrics for success rate</li> <li>Adjust defaults if LLM unavailable:    <pre><code># config/decay-config.yaml\nclassification:\n  default_importance: 3  # Set based on your data\n  default_stability: 3\n</code></pre></li> </ol>"},{"location":"usage/memory-decay.html#custom-decay-curves","title":"Custom Decay Curves","text":"<p>For specialized use cases, you can implement custom decay logic:</p>"},{"location":"usage/memory-decay.html#half-life-adjustment","title":"Half-life adjustment","text":"<ul> <li>Base half-life: 180 days (configurable)</li> <li>Stability factor: Multiplier based on stability level</li> </ul> <p>Stability multiplier examples:</p> <p>The stability level adjusts the base half-life:</p> <ul> <li>Stability 1 (VOLATILE): 0.33\u00d7 half-life (60 days)</li> <li>Stability 3 (MODERATE): 1.0\u00d7 half-life (180 days)</li> <li>Stability 5 (PERMANENT): \u221e half-life (never decays)</li> </ul> <p>See implementation: <code>docker/patches/memory_decay.py</code> - <code>calculate_half_life()</code></p>"},{"location":"usage/memory-decay.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Observability &amp; Metrics - Prometheus metrics reference</li> <li>Configuration Reference - Full configuration guide</li> <li>Monitoring Guide - Grafana dashboard setup</li> </ul>"},{"location":"usage/memory-decay.html#acknowledgments","title":"Acknowledgments","text":"<p>The memory decay scoring and lifecycle management concepts in this feature were inspired by discussions in the Personal AI_Infrastructure community, specifically around PAI Discussion #527: Knowledge System Long-term Memory Strategy.</p> <p>The implementation adapts these concepts for a Graphiti-based knowledge graph, with automated importance/stability classification, weighted search scoring, and lifecycle state transitions.</p>"},{"location":"usage/monitoring.html","title":"Monitoring with Prometheus and Grafana","text":""},{"location":"usage/monitoring.html#monitoring-with-prometheus-and-grafana","title":"Monitoring with Prometheus and Grafana","text":"<p>The Madeinoz Knowledge System includes a built-in monitoring stack to help you track token usage, costs, cache performance, and errors in real-time.</p> <p></p>"},{"location":"usage/monitoring.html#what-youll-see","title":"What You'll See","text":"<p>The monitoring stack gives you visibility into:</p> <ul> <li>Token usage - How many tokens you're using across models</li> <li>Costs - Real-time cost tracking by model and operation</li> <li>Performance - Request durations and latency percentiles</li> <li>Cache efficiency - Hit rates and cached token savings</li> <li>Errors - Any failures that occur during operations</li> </ul>"},{"location":"usage/monitoring.html#getting-started","title":"Getting Started","text":""},{"location":"usage/monitoring.html#development-environment","title":"Development Environment","text":"<p>If you're running the development environment, monitoring is already enabled:</p> <pre><code>bun run server start --dev\n</code></pre> <p>Access the dashboards:</p> <ul> <li>Grafana: http://localhost:3002 (FalkorDB dev) or http://localhost:3003 (Neo4j dev)</li> <li>Prometheus: http://localhost:9092</li> </ul>"},{"location":"usage/monitoring.html#production-environment","title":"Production Environment","text":"<p>In production, monitoring is optional. Enable it with the <code>--profile</code> flag:</p> <pre><code># Start with monitoring enabled\ndocker compose -f src/skills/server/docker-compose-falkordb.yml --profile monitoring up -d\n\n# Or for Neo4j backend\ndocker compose -f src/skills/server/docker-compose-neo4j.yml --profile monitoring up -d\n</code></pre> <p>Access points:</p> <ul> <li>Grafana: http://localhost:3001 (Neo4j) or http://localhost:3002 (FalkorDB)</li> <li>Prometheus: http://localhost:9092</li> </ul>"},{"location":"usage/monitoring.html#logging-into-grafana","title":"Logging into Grafana","text":"<p>When you first access Grafana, you'll see the login screen:</p> <p></p> <p>Default credentials:</p> Field Value Username <code>admin</code> Password <code>admin</code> <p>You'll be prompted to change the password on first login - you can skip this for local development.</p>"},{"location":"usage/monitoring.html#the-dashboard","title":"The Dashboard","text":"<p>After logging in, navigate to Dashboards in the left sidebar. You'll find the Madeinoz Knowledge System dashboard pre-configured.</p> <p>The dashboard has six sections:</p>"},{"location":"usage/monitoring.html#overview-row","title":"Overview Row","text":"<p>Quick stats at a glance:</p> Panel What It Shows Total API Cost Cumulative cost in dollars Total Tokens Used Combined prompt + completion tokens Cache Status Whether caching is enabled Cache Hit Rate Percentage of cache hits Total Errors Count of failed operations"},{"location":"usage/monitoring.html#token-usage-row","title":"Token Usage Row","text":"<p>Detailed token metrics:</p> <ul> <li>Token Usage Rate - Tokens per second by model</li> <li>Prompt vs Completion - Breakdown of input vs output tokens</li> </ul>"},{"location":"usage/monitoring.html#cost-tracking-row","title":"Cost Tracking Row","text":"<p>Financial metrics:</p> <ul> <li>Cost Rate - Dollars per hour by model</li> <li>Cost by Model - Pie chart of spending distribution</li> <li>Input vs Output Cost - Cost breakdown by token type</li> </ul>"},{"location":"usage/monitoring.html#request-duration-row","title":"Request Duration Row","text":"<p>Performance metrics:</p> <ul> <li>Request Duration - Response time histogram</li> <li>Duration Percentiles - P50, P90, P99 latencies</li> <li>Requests per Second - Throughput over time</li> </ul>"},{"location":"usage/monitoring.html#cache-performance-row","title":"Cache Performance Row","text":"<p>Caching effectiveness:</p> <ul> <li>Cache Hits vs Misses - Hit rate visualization</li> <li>Cached Tokens - Tokens served from cache</li> <li>Cache Efficiency - Cost savings from caching</li> </ul>"},{"location":"usage/monitoring.html#errors-row","title":"Errors Row","text":"<p>Error tracking:</p> <ul> <li>Errors by Type - Breakdown by operation</li> <li>Error Rate - Errors per minute trend</li> </ul>"},{"location":"usage/monitoring.html#verifying-prometheus-is-working","title":"Verifying Prometheus is Working","text":"<p>You can check that Prometheus is scraping metrics correctly:</p> <p></p> <p>Navigate to http://localhost:9092/targets and verify:</p> <ul> <li>The graphiti-mcp target shows UP status</li> <li>Last scrape was recent (within 15 seconds)</li> <li>Scrape duration is low (typically &lt;100ms)</li> </ul>"},{"location":"usage/monitoring.html#checking-raw-metrics","title":"Checking Raw Metrics","text":"<p>To see the raw metrics being collected:</p> <pre><code># From the host (dev environment)\ncurl http://localhost:9091/metrics\n\n# Example output\ngraphiti_prompt_tokens_total{model=\"google/gemini-2.5-flash\"} 12456\ngraphiti_completion_tokens_total{model=\"google/gemini-2.5-flash\"} 3721\ngraphiti_cache_hit_total 847\ngraphiti_cache_miss_total 153\ngraphiti_request_duration_seconds_bucket{le=\"0.1\"} 234\n</code></pre>"},{"location":"usage/monitoring.html#port-reference","title":"Port Reference","text":"Service Dev Port Prod Port Notes Grafana 3002 (FalkorDB), 3003 (Neo4j) 3001 (Neo4j), 3002 (FalkorDB) Dashboard UI Prometheus 9092 9092 Query interface MCP Metrics 9091 9090 (internal) Scraped by Prometheus"},{"location":"usage/monitoring.html#customizing-the-dashboard","title":"Customizing the Dashboard","text":"<p>The dashboard is auto-provisioned but you can customize it:</p> <ol> <li>Click Edit in the top-right corner</li> <li>Modify panels, add queries, adjust time ranges</li> <li>Click Save dashboard to persist changes</li> </ol> <p>Note: Provisioned dashboards reset on container restart. To keep changes, export the dashboard JSON and save it to <code>config/monitoring/grafana/provisioning/dashboards/</code>.</p>"},{"location":"usage/monitoring.html#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>GRAFANA_ADMIN_PASSWORD</code> <code>admin</code> Grafana admin password <p>Set this in your <code>.env</code> file to change the default password:</p> <pre><code># In $PAI_DIR/.env or ~/.claude/.env\nGRAFANA_ADMIN_PASSWORD=your-secure-password\n</code></pre>"},{"location":"usage/monitoring.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"usage/monitoring.html#grafana-shows-no-data","title":"Grafana shows \"No Data\"","text":"<ol> <li>Check Prometheus is scraping successfully at http://localhost:9092/targets</li> <li>Verify the MCP server is running: <code>docker ps | grep mcp</code></li> <li>Make some knowledge operations to generate metrics</li> </ol>"},{"location":"usage/monitoring.html#cant-connect-to-grafana","title":"Can't connect to Grafana","text":"<ol> <li>Verify the container is running: <code>docker ps | grep grafana</code></li> <li>Check the correct port for your environment (dev vs prod, FalkorDB vs Neo4j)</li> <li>Check container logs: <code>docker logs madeinoz-knowledge-grafana-dev</code></li> </ol>"},{"location":"usage/monitoring.html#prometheus-target-is-down","title":"Prometheus target is DOWN","text":"<ol> <li>Check the MCP server is healthy: <code>curl http://localhost:8001/health</code></li> <li>Verify network connectivity between containers</li> <li>Check Prometheus logs: <code>docker logs madeinoz-knowledge-prometheus-dev</code></li> </ol>"},{"location":"usage/monitoring.html#quick-reference","title":"Quick Reference","text":"Task Command Start dev with monitoring <code>bun run server start --dev</code> Start prod with monitoring <code>docker compose -f &lt;compose-file&gt; --profile monitoring up -d</code> Stop monitoring only <code>docker compose -f &lt;compose-file&gt; --profile monitoring down</code> View Grafana http://localhost:3002 (check port table above) View Prometheus http://localhost:9092 Check targets http://localhost:9092/targets View raw metrics <code>curl http://localhost:9091/metrics</code>"},{"location":"usage/monitoring.html#next-steps","title":"Next Steps","text":"<ul> <li>Explore the observability reference for detailed metric descriptions</li> <li>Learn about advanced usage patterns for optimization tips</li> <li>Explore detailed metrics reference for all available metrics</li> </ul>"}]}