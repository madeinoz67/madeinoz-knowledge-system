# Development Environment Configuration
# Used by: bun run src/skills/tools/server-cli.ts start --dev
#
# This file provides defaults for Qdrant (LKAP) and Vision LLM (Feature 024).
# Values here override PAI .env when running in dev mode.
#
# Usage:
#   source .env.dev && bun run src/skills/tools/server-cli.ts start --dev

# =============================================================================
# Qdrant Configuration (LKAP - Document Memory, Feature 022/023)
# =============================================================================
# For MCP container: use Docker service name (qdrant)
# For CLI (rag-cli.ts): use localhost:6334 (dev port mapping)
MADEINOZ_KNOWLEDGE_QDRANT_URL=http://qdrant:6333
MADEINOZ_KNOWLEDGE_QDRANT_URL_CLI=http://localhost:6334
MADEINOZ_KNOWLEDGE_MCP_URL_CLI=http://localhost:8001
MADEINOZ_KNOWLEDGE_QDRANT_COLLECTION=lkap_documents
MADEINOZ_KNOWLEDGE_QDRANT_EMBEDDING_DIMENSION=1024
MADEINOZ_KNOWLEDGE_QDRANT_CONFIDENCE_THRESHOLD=0.70
MADEINOZ_KNOWLEDGE_QDRANT_CHUNK_SIZE_MIN=512
MADEINOZ_KNOWLEDGE_QDRANT_CHUNK_SIZE_MAX=768
MADEINOZ_KNOWLEDGE_QDRANT_CHUNK_OVERLAP=100
MADEINOZ_KNOWLEDGE_QDRANT_LOG_LEVEL=INFO

# =============================================================================
# Ollama Configuration (for Qdrant RAG embeddings)
# =============================================================================
# Qdrant RAG embedding model (used by ollama_embedder.py and qdrant_client.py)
# Options: bge-m3 (best, needs OLLAMA_FLASH_ATTENTION=false on server), mxbai-embed-large (stable)
MADEINOZ_KNOWLEDGE_QDRANT_EMBEDDING_MODEL=mxbai-embed-large
MADEINOZ_KNOWLEDGE_QDRANT_OLLAMA_URL=http://10.0.0.150:11434

# Container env vars (passed to MCP container by server-cli.ts)
OLLAMA_BASE_URL=http://10.0.0.150:11434
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large

# Fix for bge-m3 NaN bug in Ollama (GitHub issue #13572)
# NOTE: This must be set on the Ollama SERVER, not here. Set in ollama.service or ollama start command.
# OLLAMA_FLASH_ATTENTION=false

# =============================================================================
# Graphiti Knowledge Graph Embeddings (DO NOT CHANGE)
# =============================================================================
# Used by Graphiti for knowledge graph - separate from Qdrant RAG
MADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=bge-m3

# =============================================================================
# Vision LLM Configuration (Feature 024 - Multimodal Image Extraction)
# =============================================================================
# Provider: openrouter, zai, or ollama
MADEINOZ_KNOWLEDGE_VISION_LLM_PROVIDER=ollama
# Options: llama3.2-vision (general), deepseek-ocr (text extraction)
MADEINOZ_KNOWLEDGE_VISION_LLM_MODEL=llama3.2-vision
MADEINOZ_KNOWLEDGE_VISION_LLM_FALLBACK=false

# Ollama base URL is shared with embedding config above
# Vision uses same server: http://10.0.0.150:11434/v1
