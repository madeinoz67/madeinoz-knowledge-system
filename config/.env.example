# Madeinoz Knowledge System Configuration
# Copy this file to ~/.claude/.env and configure your settings
#
# ============================================================================
# VARIABLE MAPPING REFERENCE
# ============================================================================
#
# All variables use the MADEINOZ_KNOWLEDGE_* prefix and map to container env vars:
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Pack Variable (set these)                   â”‚ Container Variable (mapped) â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ API KEYS                                    â”‚                             â”‚
# â”‚ MADEINOZ_KNOWLEDGE_OPENAI_API_KEY           â”‚ OPENAI_API_KEY              â”‚
# â”‚ MADEINOZ_KNOWLEDGE_ANTHROPIC_API_KEY        â”‚ ANTHROPIC_API_KEY           â”‚
# â”‚ MADEINOZ_KNOWLEDGE_GOOGLE_API_KEY           â”‚ GOOGLE_API_KEY              â”‚
# â”‚ MADEINOZ_KNOWLEDGE_GROQ_API_KEY             â”‚ GROQ_API_KEY                â”‚
# â”‚ MADEINOZ_KNOWLEDGE_VOYAGE_API_KEY           â”‚ VOYAGE_API_KEY              â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ LLM CONFIGURATION                           â”‚                             â”‚
# â”‚ MADEINOZ_KNOWLEDGE_LLM_PROVIDER             â”‚ LLM_PROVIDER                â”‚
# â”‚ MADEINOZ_KNOWLEDGE_MODEL_NAME               â”‚ MODEL_NAME                  â”‚
# â”‚ MADEINOZ_KNOWLEDGE_OPENAI_BASE_URL          â”‚ OPENAI_BASE_URL             â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ EMBEDDER CONFIGURATION                      â”‚                             â”‚
# â”‚ MADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER        â”‚ EMBEDDER_PROVIDER           â”‚
# â”‚ MADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER_URL    â”‚ EMBEDDER_PROVIDER_URL       â”‚
# â”‚ MADEINOZ_KNOWLEDGE_EMBEDDER_MODEL           â”‚ EMBEDDER_MODEL              â”‚
# â”‚ MADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS      â”‚ EMBEDDER_DIMENSIONS         â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ DATABASE CONFIGURATION                      â”‚                             â”‚
# â”‚ MADEINOZ_KNOWLEDGE_DATABASE_TYPE            â”‚ DATABASE_TYPE               â”‚
# â”‚ MADEINOZ_KNOWLEDGE_NEO4J_URI                â”‚ NEO4J_URI                   â”‚
# â”‚ MADEINOZ_KNOWLEDGE_NEO4J_USER               â”‚ NEO4J_USER                  â”‚
# â”‚ MADEINOZ_KNOWLEDGE_NEO4J_PASSWORD           â”‚ NEO4J_PASSWORD              â”‚
# â”‚ MADEINOZ_KNOWLEDGE_NEO4J_DATABASE           â”‚ NEO4J_DATABASE              â”‚
# â”‚ MADEINOZ_KNOWLEDGE_FALKORDB_HOST            â”‚ FALKORDB_HOST               â”‚
# â”‚ MADEINOZ_KNOWLEDGE_FALKORDB_PORT            â”‚ FALKORDB_PORT               â”‚
# â”‚ MADEINOZ_KNOWLEDGE_FALKORDB_PASSWORD        â”‚ FALKORDB_PASSWORD           â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ KNOWLEDGE GRAPH                             â”‚                             â”‚
# â”‚ MADEINOZ_KNOWLEDGE_GROUP_ID                 â”‚ GROUP_ID                    â”‚
# â”‚ MADEINOZ_KNOWLEDGE_SEMAPHORE_LIMIT          â”‚ SEMAPHORE_LIMIT             â”‚
# â”‚ MADEINOZ_KNOWLEDGE_GRAPHITI_TELEMETRY_ENABLEDâ”‚ GRAPHITI_TELEMETRY_ENABLED â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# Benefits of prefixed variables:
#   - Clear ownership: Each variable is pack-specific
#   - No conflicts: Different packs can use different settings
#   - Separate billing: Track costs per pack
#   - Independent rate limits: One pack doesn't affect another
#
# ============================================================================

# ============================================================================
# API Keys (pack-specific)
# ============================================================================

# API Key Configuration
# - For Ollama: No API key needed (uses local models)
# - For OpenAI: Get your key from https://platform.openai.com/api-keys
# - For other providers: See their respective documentation

# OpenAI API Key (required for OpenAI provider, or when using OpenAI embeddings)
# MADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-your-openai-api-key-here

# Optional: Alternative LLM providers
# Uncomment if you want to use these instead of Ollama/OpenAI
# MADEINOZ_KNOWLEDGE_ANTHROPIC_API_KEY=your-anthropic-api-key
# MADEINOZ_KNOWLEDGE_GOOGLE_API_KEY=your-google-api-key
# MADEINOZ_KNOWLEDGE_GROQ_API_KEY=your-groq-api-key
# MADEINOZ_KNOWLEDGE_VOYAGE_API_KEY=your-voyage-api-key

# ============================================================================
# LLM Provider Configuration (pack-specific)
# ============================================================================

# ============================================================================
# LLM Provider Selection (IMPORTANT: Read benchmark results!)
# ============================================================================
#
# âš ï¸ BENCHMARK RESULTS (15 models tested with real MCP extraction):
#   âœ… WORKING: Gemini 2.0 Flash, Qwen 2.5 72B, GPT-4o Mini, Claude 3.5 Haiku, GPT-4o, Grok 3
#   âŒ FAILING: All Llama models, Mistral, DeepSeek, Grok Fast variants
#
# RECOMMENDATION: Use OpenRouter (openai provider with custom base_url)
#   - Best Value: google/gemini-2.0-flash-001 ($0.125/1K ops)
#   - Most Reliable: openai/gpt-4o-mini ($0.129/1K ops)
#   - Fastest: openai/gpt-4o ($2.155/1K ops, 12s extraction)
#
# LLM Provider: openai (with OpenRouter), ollama (âš ï¸ EXPERIMENTAL), anthropic, gemini, groq
# - openai: Use with OpenRouter base_url for access to Gemini, Claude, GPT models
# - ollama: âš ï¸ FREE but Llama/Mistral FAIL Graphiti Pydantic validation
# - anthropic/gemini/groq: Cloud-based alternatives
MADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai

# Embeddings Provider: ollama (recommended - free + fast), openai (for cloud)
# Note: Ollama embeddings work great even when LLM is cloud-based!
MADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=ollama

# Model name (adjust based on your provider)
#
# âœ… TESTED & WORKING with Graphiti:
#   OpenRouter: openai/gpt-4o-mini (MOST RELIABLE), google/gemini-2.0-flash-001 (best value but less stable)
#   OpenRouter: qwen/qwen-2.5-72b-instruct, anthropic/claude-3.5-haiku, x-ai/grok-3, openai/gpt-4o
#
# âŒ FAILS Graphiti Pydantic validation:
#   Ollama: llama3.2, llama3.1:70b, mistral, deepseek-r1:7b
#   OpenRouter: meta-llama/*, mistralai/*, deepseek/*, x-ai/grok-4-fast
#
# NOTE: GPT-4o-mini is the most stable for entity extraction. Gemini 2.0 Flash
# is cheaper but may occasionally fail with Pydantic validation errors.
# If you experience validation errors, switch to openai/gpt-4o-mini.
MADEINOZ_KNOWLEDGE_MODEL_NAME=openai/gpt-4o-mini

# ============================================================================
# OpenRouter Configuration (RECOMMENDED - use with LLM_PROVIDER=openai)
# ============================================================================

# OpenRouter API endpoint - provides access to Gemini, Claude, GPT, and more
# Get your API key at: https://openrouter.ai/keys
MADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1

# ============================================================================
# Prompt Caching Configuration (AUTOMATIC for most providers!)
# ============================================================================
#
# âš¡ AUTOMATIC COST SAVINGS - NO CONFIGURATION NEEDED FOR MOST MODELS
#
# Many LLM providers automatically cache prompts â‰¥1024 tokens, reducing costs
# by 50-90% and latency by 80-85% for repeated prompts. Graphiti's system
# prompts (~1500 tokens) benefit automatically on supported providers.
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ PROVIDER COMPATIBILITY MATRIX                                           â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ âœ… AUTOMATIC CACHING (Zero Configuration)                               â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Provider           â”‚ Min Tokens â”‚ Cost Savings â”‚ Notes                  â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ OpenAI Direct      â”‚ 1024       â”‚ 50%          â”‚ Cached reads: 50% off  â”‚
# â”‚ OpenRouter: OpenAI â”‚ 1024       â”‚ 50%          â”‚ Same as direct         â”‚
# â”‚ OpenRouter: Groq   â”‚ 1024       â”‚ ~50%         â”‚ Free tier has caching  â”‚
# â”‚ OpenRouter: DeepSeekâ”‚ 1024      â”‚ ~50%         â”‚ Backend automatic      â”‚
# â”‚ OpenRouter: Moonshotâ”‚ 1024      â”‚ ~50%         â”‚ Backend automatic      â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ ğŸ”§ MANUAL CACHING (Requires cache_control configuration)                â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Provider             â”‚ Min Tokens â”‚ Cost Savings â”‚ Status              â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Anthropic Direct     â”‚ 1024       â”‚ 90%          â”‚ âš ï¸ Not implemented  â”‚
# â”‚ OpenRouter: Anthropicâ”‚ 1024       â”‚ 90%          â”‚ âš ï¸ Not implemented  â”‚
# â”‚ OpenRouter: Google   â”‚ 1024       â”‚ ~50%         â”‚ âš ï¸ Not implemented  â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ âŒ NO API-LEVEL CACHING                                                 â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Ollama (local)       â”‚ N/A        â”‚ None         â”‚ Use KV cache insteadâ”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# CACHE REPORTING (OpenRouter)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OpenRouter returns cache metrics in every response:
#
#   {
#     "usage": {
#       "prompt_tokens": 2500,
#       "completion_tokens": 150,
#       "cached_tokens": 1800
#     },
#     "cache_discount": 0.45  // 45% cost reduction on this request
#   }
#
# View detailed cache analytics at: https://openrouter.ai/activity
#
# COST ESTIMATES (based on Graphiti's ~1500 token system prompt)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OpenAI gpt-4o-mini via OpenRouter:
#   - Without caching: $0.129 per 1000 operations
#   - With caching:    $0.065 per 1000 operations (50% savings)
#   - Break-even:      Just 2 API calls
#
# Google Gemini 2.0 Flash via OpenRouter:
#   - Without caching: $0.125 per 1000 operations
#   - With caching:    ~$0.063 per 1000 operations (50% savings)
#   - Note: Requires cache_control implementation (not yet available)
#
# CONFIGURATION (Future Enhancement)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# The following variables are planned for future releases to enable
# manual caching for Anthropic and Google models:
#
# # Enable prompt caching for providers that require cache_control
# # MADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED=false
#
# # Cache TTL: 5m (ephemeral) or 1h (extended) - Anthropic only
# # MADEINOZ_KNOWLEDGE_PROMPT_CACHE_TTL=5m
#
# RECOMMENDATIONS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Use openai/gpt-4o-mini via OpenRouter for automatic caching (recommended)
# 2. Check OpenRouter activity page to confirm cache savings
# 3. For Ollama local deployments, caching is not applicable at the API level
# 4. Future: Anthropic models will support 90% cache savings when implemented
#
# ============================================================================
# Embedder Endpoint Configuration
# ============================================================================

# Embedder provider URL (for custom/local embeddings like Ollama)
# - Docker/Podman: http://host.containers.internal:11434 (default)
# - Same machine: http://localhost:11434
# - Remote Ollama: http://10.0.0.150:11434 (use your Ollama server IP)
#
# This is the endpoint for embedding providers that need a custom URL (Ollama, etc.)
MADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER_URL=http://host.containers.internal:11434

# Embedding model for Ollama (BENCHMARK TESTED on RTX 4090)
# - mxbai-embed-large (RECOMMENDED: 73.9% quality, 87ms, 1024 dimensions)
# - nomic-embed-text (63.5% quality, 93ms, 768 dimensions)
# - all-minilm (384 dimensions, fastest but lower quality)
MADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large

# ============================================================================
# Embedding Dimensions (CRITICAL: Must match your EMBEDDER_MODEL!)
# ============================================================================
#
# âš ï¸ DIMENSION MISMATCH = SEARCH FAILURES
#   If dimensions don't match your model, searches will fail with
#   "vector dimension mismatch" errors. Clear graph and re-index if changed.
#
# DIMENSION REFERENCE TABLE:
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Model                       â”‚ Dimensions â”‚ Provider                   â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ mxbai-embed-large           â”‚ 1024       â”‚ Ollama (RECOMMENDED)       â”‚
# â”‚ nomic-embed-text            â”‚ 768        â”‚ Ollama                     â”‚
# â”‚ all-minilm                  â”‚ 384        â”‚ Ollama (fastest)           â”‚
# â”‚ snowflake-arctic-embed      â”‚ 1024       â”‚ Ollama                     â”‚
# â”‚ bge-large                   â”‚ 1024       â”‚ Ollama                     â”‚
# â”‚ bge-m3                      â”‚ 1024       â”‚ Ollama                     â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ text-embedding-3-small      â”‚ 1536       â”‚ OpenAI                     â”‚
# â”‚ text-embedding-3-large      â”‚ 3072       â”‚ OpenAI                     â”‚
# â”‚ text-embedding-ada-002      â”‚ 1536       â”‚ OpenAI (legacy)            â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ BAAI/bge-large-en-v1.5      â”‚ 1024       â”‚ Together AI / HuggingFace  â”‚
# â”‚ BAAI/bge-base-en-v1.5       â”‚ 768        â”‚ Together AI / HuggingFace  â”‚
# â”‚ BAAI/bge-small-en-v1.5      â”‚ 384        â”‚ Together AI / HuggingFace  â”‚
# â”‚ sentence-transformers/*     â”‚ varies     â”‚ HuggingFace (check model)  â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
MADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024

# ============================================================================
# Database Configuration (pack-specific)
# ============================================================================

# Database backend: neo4j (default) or falkordb
#
# Neo4j (default):
#   - Native graph database with Cypher query language
#   - Better special character handling (no escaping needed)
#   - Richer query language and visualization
#   - Neo4j Browser at http://localhost:7474
#
# FalkorDB:
#   - Redis-based graph database with RediSearch
#   - Simpler setup, lower resource usage
#   - Uses Lucene query syntax (requires character escaping)
#   - Web UI at http://localhost:3000
#
MADEINOZ_KNOWLEDGE_DATABASE_TYPE=neo4j

# ============================================================================
# FalkorDB Configuration (used when DATABASE_TYPE=falkordb)
# ============================================================================

# FalkorDB connection (public network - DO NOT MODIFY)
# These are set automatically by the container orchestration
MADEINOZ_KNOWLEDGE_FALKORDB_HOST=madeinoz-knowledge-falkordb
MADEINOZ_KNOWLEDGE_FALKORDB_PORT=6379

# FalkorDB Password (optional - leave empty for no authentication)
MADEINOZ_KNOWLEDGE_FALKORDB_PASSWORD=

# ============================================================================
# Neo4j Configuration (used when DATABASE_TYPE=neo4j)
# ============================================================================

# Neo4j connection URI
# When using containers, the host is the container name on the shared network
MADEINOZ_KNOWLEDGE_NEO4J_URI=bolt://madeinoz-knowledge-neo4j:7687

# Neo4j authentication
MADEINOZ_KNOWLEDGE_NEO4J_USER=neo4j
MADEINOZ_KNOWLEDGE_NEO4J_PASSWORD=madeinozknowledge

# Neo4j database name (usually 'neo4j' for community edition)
MADEINOZ_KNOWLEDGE_NEO4J_DATABASE=neo4j

# ============================================================================
# Performance Configuration (pack-specific)
# ============================================================================

# Concurrency limit for parallel LLM requests
# Adjust based on your API tier:
#   Free tier:   2-5
#   Tier 2:      8
#   Tier 3:      10 (default)
#   Tier 4:      20
MADEINOZ_KNOWLEDGE_SEMAPHORE_LIMIT=10

# ============================================================================
# Knowledge Graph Configuration (pack-specific)
# ============================================================================

# Group ID - allows multiple isolated knowledge graphs
# Use 'main' for your primary graph
MADEINOZ_KNOWLEDGE_GROUP_ID=main

# ============================================================================
# Telemetry (pack-specific)
# ============================================================================

# Disable Graphiti telemetry
MADEINOZ_KNOWLEDGE_GRAPHITI_TELEMETRY_ENABLED=false

# Disable Neo4j usage reporting/telemetry (Neo4j backend only)
MADEINOZ_KNOWLEDGE_NEO4J_TELEMETRY_ENABLED=false

# ============================================================================
# Prompt Caching Configuration (Feature 006 - Gemini via OpenRouter)
# ============================================================================
#
# Enable prompt caching for cost savings with Gemini models via OpenRouter.
# This feature adds cache_control markers to requests and reports cost savings.
#
# Requirements:
#   - LLM_PROVIDER must be 'openai' with OPENAI_BASE_URL=https://openrouter.ai/api/v1
#   - MODEL_NAME must be a Gemini model (e.g., google/gemini-2.0-flash-001)
#
# How it works:
#   1. System prompts get cache_control markers for caching
#   2. OpenRouter manages cache lifecycle automatically
#   3. Responses include cache_metrics (tokens saved, cost saved, hit rate)
#   4. Prometheus metrics expose cache statistics at /metrics endpoint
#
# Example cache_metrics in response:
#   {
#     "cache_hit": true,
#     "cached_tokens": 1523,
#     "cost_saved": 0.00040725,
#     "savings_percent": 27.69
#   }
#

# Enable/disable prompt caching feature (default: true)
# When disabled, requests use standard format without cache_control markers
MADEINOZ_KNOWLEDGE_PROMPT_CACHE_ENABLED=true

# Include cache metrics in MCP response metadata (default: true)
# When disabled, cache_metrics field is omitted from responses
MADEINOZ_KNOWLEDGE_PROMPT_CACHE_METRICS_ENABLED=true

# Cache TTL (Time-To-Live) for prompt caching (default: 1h)
# Options: "5m" (5 minutes), "1h" (1 hour)
# Longer TTL increases cache hit rate but may serve stale context
# OpenRouter/Gemini: Both 5m and 1h supported
MADEINOZ_KNOWLEDGE_PROMPT_CACHE_TTL=1h

# Prometheus metrics endpoint port (default: 9090)
# Exposes cache statistics in Prometheus format at http://localhost:9090/metrics
# Metrics include: cache hits, misses, tokens saved, cost saved, hit rate
MADEINOZ_KNOWLEDGE_METRICS_PORT=9090

# Log detailed cache metrics for each request (default: false)
# Enable for debugging cache behavior; generates verbose logs
# WARNING: May expose prompt content in logs - use only in development
MADEINOZ_KNOWLEDGE_PROMPT_CACHE_LOG_REQUESTS=false

# ============================================================================
# Madeinoz Patch: Search All Groups (Neo4j only)
# ============================================================================

# When true, searches query ALL groups when no group_ids specified
# This ensures knowledge in different groups (osint-profiles, main, etc.) is discoverable
# SECURITY: Default is false for least privilege (data isolation between groups)
# Set to 'true' to enable cross-group search
# Note: Uses GRAPHITI_* prefix for upstream compatibility
GRAPHITI_SEARCH_ALL_GROUPS=${GRAPHITI_SEARCH_ALL_GROUPS:-false}

# ============================================================================
# Monitoring Configuration (Prometheus & Grafana)
# ============================================================================
#
# The system includes optional Prometheus and Grafana for monitoring.
#
# Development: Monitoring is enabled by default
#   docker compose -f src/skills/server/docker-compose-neo4j-dev.yml up -d
#
# Production: Monitoring requires --profile monitoring flag
#   docker compose -f src/skills/server/docker-compose-neo4j.yml --profile monitoring up -d
#
# Access points (when enabled):
#   - Prometheus UI: http://localhost:9092
#   - Grafana (Dev): http://localhost:3002 (Neo4j) or :3003 (FalkorDB)
#   - Grafana (Prod): http://localhost:3001 (Neo4j) or :3002 (FalkorDB)

# Grafana admin password (default: admin)
# SECURITY: Change this in production!
GRAFANA_ADMIN_PASSWORD=admin

# ============================================================================
# Network Architecture Notes
# ============================================================================
#
# This system uses a public bridge network (madeinoz-knowledge-net):
#
# 1. Database and MCP server containers run on the same network
#    - Containers communicate via service names
#    - Network is accessible for inter-container communication
#
# 2. MCP Server connects to database via container service name:
#    - FalkorDB: madeinoz-knowledge-falkordb (port 6379)
#    - Neo4j: madeinoz-knowledge-neo4j (port 7687)
#
# 3. Exposed ports (host access):
#
#    FalkorDB backend:
#    - 8000: MCP server HTTP endpoint
#    - 3000: FalkorDB web UI
#
#    Neo4j backend:
#    - 8000: MCP server HTTP endpoint
#    - 7474: Neo4j Browser (web UI)
#    - 7687: Neo4j Bolt protocol
#
# ============================================================================

# ============================================================================
# LKAP Configuration (Feature 022: Self-Hosted RAG)
# ============================================================================
#
# Local Knowledge Augmentation Platform - adds RAG capabilities to the
# knowledge graph system with automatic document ingestion, semantic search,
# and evidence-based fact promotion.
#
# Services:
#   - RAGFlow: Vector database for document chunks and semantic search
#   - Ollama: Optional local embeddings/LLM (or use OpenRouter API)
#
# Document Storage:
#   - knowledge/inbox/: Drop PDFs, markdown, text files here for ingestion
#   - knowledge/processed/: Canonical storage after successful ingestion
#
# CONSTITUTION REQUIREMENT: All variables use MADEINOZ_KNOWLEDGE_ prefix
# to prevent naming conflicts and ensure clear ownership. Container env vars
# are mapped from these prefixed variables in docker-compose files.

# RAGFlow Configuration
# RAGFlow API endpoint (container-to-container communication)
MADEINOZ_KNOWLEDGE_RAGFLOW_API_URL=http://ragflow:9380

# Optional RAGFlow API key for authentication
# MADEINOZ_KNOWLEDGE_RAGFLOW_API_KEY=your-ragflow-api-key

# Embedding Configuration (reuses existing Graphiti variables above)
# LKAP uses the same embedding provider and dimensions as the Knowledge Graph
# - Configure MADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER (ollama, openai)
# - Configure MADEINOZ_KNOWLEDGE_EMBEDDER_MODEL (mxbai-embed-large, text-embedding-3-large)
# - Configure MADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS (1024+ required for LKAP)
# - Configure MADEINOZ_KNOWLEDGE_OPENROUTER_API_KEY if using OpenAI embeddings via OpenRouter

# Chunking configuration (heading-aware per research decision RT-002)
MADEINOZ_KNOWLEDGE_RAGFLOW_CHUNK_SIZE_MIN=512
MADEINOZ_KNOWLEDGE_RAGFLOW_CHUNK_SIZE_MAX=768
MADEINOZ_KNOWLEDGE_RAGFLOW_CHUNK_OVERLAP=100

# Search confidence threshold (chunks below 0.70 are not returned)
MADEINOZ_KNOWLEDGE_RAGFLOW_CONFIDENCE_THRESHOLD=0.70

# Logging level (FR-036a: basic logging)
MADEINOZ_KNOWLEDGE_RAGFLOW_LOG_LEVEL=INFO
