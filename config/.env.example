# Madeinoz Knowledge System Configuration
# Copy this file to .env and configure your API keys
# Location: src/config/.env

# ============================================================================
# DEDICATED CONFIGURATION FOR THIS PACK
# ============================================================================
# This pack uses MADEINOZ_KNOWLEDGE_* prefixed variables to avoid conflicts with
# other packs that may use the same services. These variables will be
# automatically mapped to standard container environment variables during
# startup (e.g., MADEINOZ_KNOWLEDGE_LLM_PROVIDER → LLM_PROVIDER in container).
#
# Benefits:
#   - Clear ownership: Each variable is pack-specific
#   - No conflicts: Different packs can use different settings
#   - Separate billing: Track costs per pack
#   - Independent rate limits: One pack doesn't affect another
#
# For example:
#   - gpt-image-1 pack uses OPENAI_API_KEY, MODEL_NAME, etc.
#   - madeinoz-knowledge-system pack uses MADEINOZ_KNOWLEDGE_OPENAI_API_KEY,
#     MADEINOZ_KNOWLEDGE_MODEL_NAME, etc.
#
# During installation, the installer will:
#   1. Check if MADEINOZ_KNOWLEDGE_* variables exist in ~/.config/pai/.env
#   2. If found, display values and ask to confirm
#   3. If not found, prompt for new values
#   4. Store in both ~/.config/pai/.env and this .env file
# ============================================================================

# ============================================================================
# API Keys (pack-specific)
# ============================================================================

# API Key Configuration
# - For Ollama: No API key needed (uses local models)
# - For OpenAI: Get your key from https://platform.openai.com/api-keys
# - For other providers: See their respective documentation

# OpenAI API Key (required for OpenAI provider, or when using OpenAI embeddings)
# MADEINOZ_KNOWLEDGE_OPENAI_API_KEY=sk-your-openai-api-key-here

# Optional: Alternative LLM providers
# Uncomment if you want to use these instead of Ollama/OpenAI
# MADEINOZ_KNOWLEDGE_ANTHROPIC_API_KEY=your-anthropic-api-key
# MADEINOZ_KNOWLEDGE_GOOGLE_API_KEY=your-google-api-key
# MADEINOZ_KNOWLEDGE_GROQ_API_KEY=your-groq-api-key
# MADEINOZ_KNOWLEDGE_VOYAGE_API_KEY=your-voyage-api-key

# ============================================================================
# LLM Provider Configuration (pack-specific)
# ============================================================================

# ============================================================================
# LLM Provider Selection (IMPORTANT: Read benchmark results!)
# ============================================================================
#
# ⚠️ BENCHMARK RESULTS (15 models tested with real MCP extraction):
#   ✅ WORKING: Gemini 2.0 Flash, Qwen 2.5 72B, GPT-4o Mini, Claude 3.5 Haiku, GPT-4o, Grok 3
#   ❌ FAILING: All Llama models, Mistral, DeepSeek, Grok Fast variants
#
# RECOMMENDATION: Use OpenRouter (openai provider with custom base_url)
#   - Best Value: google/gemini-2.0-flash-001 ($0.125/1K ops)
#   - Most Reliable: openai/gpt-4o-mini ($0.129/1K ops)
#   - Fastest: openai/gpt-4o ($2.155/1K ops, 12s extraction)
#
# LLM Provider: openai (with OpenRouter), ollama (⚠️ EXPERIMENTAL), anthropic, gemini, groq
# - openai: Use with OpenRouter base_url for access to Gemini, Claude, GPT models
# - ollama: ⚠️ FREE but Llama/Mistral FAIL Graphiti Pydantic validation
# - anthropic/gemini/groq: Cloud-based alternatives
MADEINOZ_KNOWLEDGE_LLM_PROVIDER=openai

# Embeddings Provider: ollama (recommended - free + fast), openai (for cloud)
# Note: Ollama embeddings work great even when LLM is cloud-based!
MADEINOZ_KNOWLEDGE_EMBEDDER_PROVIDER=ollama

# Model name (adjust based on your provider)
#
# ✅ TESTED & WORKING with Graphiti:
#   OpenRouter: google/gemini-2.0-flash-001 (BEST VALUE), openai/gpt-4o-mini, openai/gpt-4o
#   OpenRouter: qwen/qwen-2.5-72b-instruct, anthropic/claude-3.5-haiku, x-ai/grok-3
#
# ❌ FAILS Graphiti Pydantic validation:
#   Ollama: llama3.2, llama3.1:70b, mistral, deepseek-r1:7b
#   OpenRouter: meta-llama/*, mistralai/*, deepseek/*, x-ai/grok-4-fast
#
MADEINOZ_KNOWLEDGE_MODEL_NAME=google/gemini-2.0-flash-001

# ============================================================================
# OpenRouter Configuration (RECOMMENDED - use with LLM_PROVIDER=openai)
# ============================================================================

# OpenRouter API endpoint - provides access to Gemini, Claude, GPT, and more
# Get your API key at: https://openrouter.ai/keys
MADEINOZ_KNOWLEDGE_OPENAI_BASE_URL=https://openrouter.ai/api/v1

# ============================================================================
# Ollama Configuration (for embeddings, or experimental LLM)
# ============================================================================

# Ollama API endpoint (for embeddings - works great with OpenRouter LLM)
# - Docker users: http://host.docker.internal:11434/v1 (default)
# - Native Podman: http://host.containers.internal:11434/v1
# - Same machine: http://localhost:11434/v1
# - Remote Ollama: http://your-ollama-server:11434/v1
MADEINOZ_KNOWLEDGE_EMBEDDER_BASE_URL=http://host.docker.internal:11434/v1

# Embedding model for Ollama (BENCHMARK TESTED on RTX 4090)
# - mxbai-embed-large (RECOMMENDED: 73.9% quality, 87ms, 1024 dimensions)
# - nomic-embed-text (63.5% quality, 93ms, 768 dimensions)
# - all-minilm (384 dimensions, fastest but lower quality)
MADEINOZ_KNOWLEDGE_EMBEDDER_MODEL=mxbai-embed-large

# ============================================================================
# Embedding Dimensions (CRITICAL: Must match your EMBEDDER_MODEL!)
# ============================================================================
#
# ⚠️ DIMENSION MISMATCH = SEARCH FAILURES
#   If dimensions don't match your model, searches will fail with
#   "vector dimension mismatch" errors. Clear graph and re-index if changed.
#
# DIMENSION REFERENCE TABLE:
# ┌─────────────────────────────┬────────────┬────────────────────────────┐
# │ Model                       │ Dimensions │ Provider                   │
# ├─────────────────────────────┼────────────┼────────────────────────────┤
# │ mxbai-embed-large           │ 1024       │ Ollama (RECOMMENDED)       │
# │ nomic-embed-text            │ 768        │ Ollama                     │
# │ all-minilm                  │ 384        │ Ollama (fastest)           │
# │ snowflake-arctic-embed      │ 1024       │ Ollama                     │
# │ bge-large                   │ 1024       │ Ollama                     │
# │ bge-m3                      │ 1024       │ Ollama                     │
# ├─────────────────────────────┼────────────┼────────────────────────────┤
# │ text-embedding-3-small      │ 1536       │ OpenAI                     │
# │ text-embedding-3-large      │ 3072       │ OpenAI                     │
# │ text-embedding-ada-002      │ 1536       │ OpenAI (legacy)            │
# ├─────────────────────────────┼────────────┼────────────────────────────┤
# │ BAAI/bge-large-en-v1.5      │ 1024       │ Together AI / HuggingFace  │
# │ BAAI/bge-base-en-v1.5       │ 768        │ Together AI / HuggingFace  │
# │ BAAI/bge-small-en-v1.5      │ 384        │ Together AI / HuggingFace  │
# │ sentence-transformers/*     │ varies     │ HuggingFace (check model)  │
# └─────────────────────────────┴────────────┴────────────────────────────┘
#
MADEINOZ_KNOWLEDGE_EMBEDDER_DIMENSIONS=1024

# ============================================================================
# Database Configuration (pack-specific)
# ============================================================================

# Database backend: neo4j (default) or falkordb
#
# Neo4j (default):
#   - Native graph database with Cypher query language
#   - Better special character handling (no escaping needed)
#   - Richer query language and visualization
#   - Neo4j Browser at http://localhost:7474
#
# FalkorDB:
#   - Redis-based graph database with RediSearch
#   - Simpler setup, lower resource usage
#   - Uses Lucene query syntax (requires character escaping)
#   - Web UI at http://localhost:3000
#
MADEINOZ_KNOWLEDGE_DATABASE_TYPE=neo4j

# ============================================================================
# FalkorDB Configuration (used when DATABASE_TYPE=falkordb)
# ============================================================================

# FalkorDB connection (public network - DO NOT MODIFY)
# These are set automatically by the container orchestration
MADEINOZ_KNOWLEDGE_FALKORDB_HOST=madeinoz-knowledge-falkordb
MADEINOZ_KNOWLEDGE_FALKORDB_PORT=6379

# FalkorDB Password (optional - leave empty for no authentication)
MADEINOZ_KNOWLEDGE_FALKORDB_PASSWORD=

# ============================================================================
# Neo4j Configuration (used when DATABASE_TYPE=neo4j)
# ============================================================================

# Neo4j connection URI
# When using containers, the host is the container name on the shared network
MADEINOZ_KNOWLEDGE_NEO4J_URI=bolt://madeinoz-knowledge-neo4j:7687

# Neo4j authentication
MADEINOZ_KNOWLEDGE_NEO4J_USER=neo4j
MADEINOZ_KNOWLEDGE_NEO4J_PASSWORD=madeinozknowledge

# Neo4j database name (usually 'neo4j' for community edition)
MADEINOZ_KNOWLEDGE_NEO4J_DATABASE=neo4j

# ============================================================================
# Performance Configuration (pack-specific)
# ============================================================================

# Concurrency limit for parallel LLM requests
# Adjust based on your API tier:
#   Free tier:   2-5
#   Tier 2:      8
#   Tier 3:      10 (default)
#   Tier 4:      20
MADEINOZ_KNOWLEDGE_SEMAPHORE_LIMIT=10

# ============================================================================
# Knowledge Graph Configuration (pack-specific)
# ============================================================================

# Group ID - allows multiple isolated knowledge graphs
# Use 'main' for your primary graph
MADEINOZ_KNOWLEDGE_GROUP_ID=main

# ============================================================================
# Telemetry (pack-specific)
# ============================================================================

# Disable Graphiti telemetry
MADEINOZ_KNOWLEDGE_GRAPHITI_TELEMETRY_ENABLED=false

# Disable Neo4j usage reporting/telemetry (Neo4j backend only)
MADEINOZ_KNOWLEDGE_NEO4J_TELEMETRY_ENABLED=false

# ============================================================================
# Madeinoz Patch: Search All Groups (Neo4j only)
# ============================================================================

# When true, searches query ALL groups when no group_ids specified
# This ensures knowledge in different groups (osint-profiles, main, etc.) is discoverable
# Set to 'false' to use original behavior (only search default group)
MADEINOZ_KNOWLEDGE_SEARCH_ALL_GROUPS=true

# ============================================================================
# Network Architecture Notes
# ============================================================================
#
# This system uses a public bridge network (madeinoz-knowledge-net):
#
# 1. Database and MCP server containers run on the same network
#    - Containers communicate via service names
#    - Network is accessible for inter-container communication
#
# 2. MCP Server connects to database via container service name:
#    - FalkorDB: madeinoz-knowledge-falkordb (port 6379)
#    - Neo4j: madeinoz-knowledge-neo4j (port 7687)
#
# 3. Exposed ports (host access):
#
#    FalkorDB backend:
#    - 8000: MCP server HTTP endpoint
#    - 3000: FalkorDB web UI
#
#    Neo4j backend:
#    - 8000: MCP server HTTP endpoint
#    - 7474: Neo4j Browser (web UI)
#    - 7687: Neo4j Bolt protocol
#
# ============================================================================
