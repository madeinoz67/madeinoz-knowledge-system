"""
Cache Metrics - Gemini Prompt Caching Performance Data

Feature: 006-gemini-prompt-caching
Purpose: Represents caching performance data for tracking token savings and cost reductions
"""

from dataclasses import dataclass
from typing import Dict, Any, Optional


@dataclass
class CacheMetrics:
    """
    Represents caching performance data for a single API request.

    Provides transparency into cache effectiveness and cost savings when using
    Gemini models via OpenRouter with prompt caching enabled.
    """

    cache_hit: bool
    """Whether any tokens were served from cache (true if cached_tokens > 0)"""

    cached_tokens: int
    """Count of tokens retrieved from cache (billed at discounted rate)"""

    prompt_tokens: int
    """Total input tokens in the request (includes cached + uncached)"""

    completion_tokens: int
    """Output tokens generated by the model (not affected by caching)"""

    tokens_saved: int
    """Semantic alias for cached_tokens (tokens saved from re-processing)"""

    cost_without_cache: float
    """Hypothetical cost if caching was not used (USD, 8 decimal places)"""

    actual_cost: float
    """Actual API cost after cache discount (USD, 8 decimal places)"""

    cost_saved: float
    """Monetary savings from caching (USD, always >= 0)"""

    savings_percent: float
    """Percentage reduction in cost (0.0 to 100.0)"""

    model: str
    """Gemini model identifier used for pricing (e.g., 'google/gemini-2.0-flash-001')"""

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to dictionary for JSON serialization in MCP responses.

        Returns:
            Dict containing all cache metrics fields
        """
        return {
            "cache_hit": self.cache_hit,
            "cached_tokens": self.cached_tokens,
            "prompt_tokens": self.prompt_tokens,
            "completion_tokens": self.completion_tokens,
            "tokens_saved": self.tokens_saved,
            "cost_without_cache": round(self.cost_without_cache, 8),
            "actual_cost": round(self.actual_cost, 8),
            "cost_saved": round(self.cost_saved, 8),
            "savings_percent": round(self.savings_percent, 2),
            "model": self.model
        }

    @classmethod
    def from_openrouter_response(
        cls,
        response: Dict[str, Any],
        model: str,
        pricing_tier: "PricingTier"
    ) -> "CacheMetrics":
        """
        Extract cache metrics from OpenRouter API response.

        Args:
            response: OpenRouter API response dictionary
            model: Model identifier (e.g., 'google/gemini-2.0-flash-001')
            pricing_tier: Pricing information for cost calculations

        Returns:
            CacheMetrics instance with calculated savings

        Example OpenRouter response formats:

        Format 1 - Standard OpenAI (usage object):
            {
                "usage": {
                    "prompt_tokens": 2048,
                    "completion_tokens": 342,
                    "cached_tokens": 1523
                }
            }

        Format 2 - OpenRouter metadata (root level):
            {
                "tokens_prompt": 685,
                "tokens_completion": 26,
                "native_tokens_cached": 523,
                "native_tokens_prompt": 704,
                "native_tokens_completion": 30
            }
        """
        # Try OpenRouter format first (root-level fields)
        if "native_tokens_cached" in response:
            prompt_tokens = response.get("tokens_prompt", 0)
            completion_tokens = response.get("tokens_completion", 0)
            cached_tokens = response.get("native_tokens_cached", 0)
        else:
            # Fall back to standard OpenAI format (usage object)
            usage = response.get("usage", {})
            prompt_tokens = usage.get("prompt_tokens", 0)
            completion_tokens = usage.get("completion_tokens", 0)
            cached_tokens = usage.get("cached_tokens", 0)

        cache_discount = response.get("cache_discount")  # Optional from OpenRouter

        # Calculate token costs
        uncached_tokens = prompt_tokens - cached_tokens

        # Cost without caching (all prompt tokens at full rate)
        cost_without_cache = (
            (prompt_tokens * pricing_tier.input_per_million / 1_000_000) +
            (completion_tokens * pricing_tier.output_per_million / 1_000_000)
        )

        # Actual cost (cached tokens at discounted rate)
        actual_cost = (
            (uncached_tokens * pricing_tier.input_per_million / 1_000_000) +
            (cached_tokens * pricing_tier.cached_input_per_million / 1_000_000) +
            (completion_tokens * pricing_tier.output_per_million / 1_000_000)
        )

        # Calculate savings
        cost_saved = max(0.0, cost_without_cache - actual_cost)

        # Calculate savings percentage
        if cost_without_cache > 0:
            savings_percent = (cost_saved / cost_without_cache) * 100
        else:
            savings_percent = 0.0

        return cls(
            cache_hit=(cached_tokens > 0),
            cached_tokens=cached_tokens,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            tokens_saved=cached_tokens,  # Semantic alias
            cost_without_cache=cost_without_cache,
            actual_cost=actual_cost,
            cost_saved=cost_saved,
            savings_percent=savings_percent,
            model=model
        )


@dataclass
class PricingTier:
    """
    Pricing rates for a specific Gemini model via OpenRouter.

    Used for cost calculations when extracting cache metrics from API responses.
    Pricing data from OpenRouter (January 2026).
    """

    model: str
    """Model identifier (e.g., 'google/gemini-2.0-flash-001')"""

    input_per_million: float
    """Standard input token cost (USD per 1M tokens)"""

    cached_input_per_million: float
    """Cached token cost (USD per 1M tokens, typically ~50% of input rate)"""

    output_per_million: float
    """Output token cost (USD per 1M tokens)"""

    min_cache_tokens: int = 1024
    """Minimum tokens for caching eligibility (default: 1024)"""


# OpenRouter Gemini Pricing (January 2026)
# Based on: https://openrouter.ai/models
PRICING_TIERS: Dict[str, PricingTier] = {
    "google/gemini-2.0-flash-001": PricingTier(
        model="google/gemini-2.0-flash-001",
        input_per_million=0.10,
        cached_input_per_million=0.01,  # 90% discount
        output_per_million=0.40,
        min_cache_tokens=1024
    ),
    "google/gemini-2.5-flash": PricingTier(
        model="google/gemini-2.5-flash",
        input_per_million=0.30,
        cached_input_per_million=0.03,  # 90% discount
        output_per_million=2.50,
        min_cache_tokens=1024
    ),
    "google/gemini-2.5-pro": PricingTier(
        model="google/gemini-2.5-pro",
        input_per_million=1.25,
        cached_input_per_million=0.125,  # 90% discount
        output_per_million=10.00,
        min_cache_tokens=1024
    )
}


def get_pricing_tier(model: str) -> Optional[PricingTier]:
    """
    Get pricing tier for a model, with fallback to closest match.

    Args:
        model: Full model identifier (e.g., 'google/gemini-2.0-flash-001')

    Returns:
        PricingTier if found, None otherwise
    """
    # Exact match first
    if model in PRICING_TIERS:
        return PRICING_TIERS[model]

    # Fuzzy match for model variants (e.g., 'google/gemini-2.0-flash-thinking-exp')
    for tier_model, pricing in PRICING_TIERS.items():
        if tier_model in model or model in tier_model:
            return pricing

    return None
